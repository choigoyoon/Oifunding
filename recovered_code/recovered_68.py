[{"text":"API 요청과 응답을 분석한 결과, 다음과 같은 문제점들이 발견되었습니다:\n\n## 심볼 형식 오류\n\n현재 코드의 심볼 형식이 잘못되었습니다. API 문서에 따르면 올바른 형식은 다음과 같습니다:\n```python\nsymbols = [\n    \"BTCUSDT_PERP.A\",  # Binance\n    \"BTCUSD_PERP.O\",   # OKX\n]\n```\n\n## 수정해야 할 사항\n\n**1. 심볼 형식 수정**\n```python\nsymbols = [\n    \"BTCUSDT_PERP.A\",  # Binance\n    \"BTCUSD_PERP.O\",   # OKX\n    \"BTCUSD_PERP.B\",   # Bybit\n    \"BTCF0:USTF0.F\"    # Bitfinex\n]\n```\n\n**2. 타임스탬프 처리**\n```python\nparams = {\n    \"symbols\": ','.join(symbols),\n    \"interval\": interval,\n    \"from\": int(from_time),  # 초 단위 유지\n    \"to\": int(to_time),      # 초 단위 유지\n    \"api_key\": API_KEY\n}\n```\n\n**3. 에러 처리 강화**\n```python\nasync with session.get(url, params=params) as response:\n    if response.status == 200:\n        data = await response.json()\n        if not data:\n            logger.warning(\"응답은 성공했지만 데이터가 비어있습니다\")\n        return data\n    elif response.status == 401:\n        logger.error(\"API 키가 유효하지 않습니다\")\n    elif response.status == 400:\n        logger.error(\"잘못된 요청 파라미터입니다\")\n    else:\n        logger.error(f\"API 오류 (상태 코드: {response.status})\")\n```\n\n## 추가 권장사항\n\n1. API 응답이 비어있는 경우에 대한 처리 추가\n2. 각 거래소별 심볼 형식 검증 로직 구현\n3. 시간 범위가 너무 길 경우 여러 번 나누어 요청하는 로직 구현\n\n이러한 수정사항들을 적용하면 API 호출이 정상적으로 작동할 것으로 예상됩니다[1][2].\n\nCitations:\n[1] https://pplx-res.cloudinary.com/image/upload/v1738767978/user_uploads/TRxSLGYzvKdEHku/image.jpg\n[2] https://api.coinalyze.net/v1/doc/\n[3] https://coinalyze.net/bitcoin/open-interest/\n[4] https://gitbook-docs.coinmetrics.io/market-data/market-data-overview/open_interest/market-open-interest\n[5] https://getblock.io/blog/coinalyze-track-and-monitor-crypto-markets/\n[6] https://goldfishman.tistory.com/71\n[7] https://getblock.io/blog/coinalyze-track-and-monitor-crypto-markets/\n[8] https://www.youtube.com/watch?v=rvLImGHuDXU\n[9] https://stackoverflow.com/questions/67509891/how-to-view-aggregated-liquidations-for-cryptocurrencies-from-binance\n[10] https://coinalyze.net/bitcoin/usdt/binance/open-interest-chart/btcusdt_perp_oi/\n[11] https://coinalyze.net\n[12] https://coinalyze.net/bitcoin/usdt/binance/price-chart-live/\n[13] https://goldfishman.tistory.com\n[14] https://coinalyze.net/bitcoin/usdt/price-live/\n[15] https://coinalyze.net/blog/\n[16] https://coinalyze.net/coinalyze-custom-metrics.pdf\n[17] https://github.com/starton-io/awesome-web3-tools-and-dapps\n[18] https://coinalyze.net/bitcoin/usd/hyperliquid/btc/price-chart-live/\n[19] https://contents.premium.naver.com/yonseident/ysdent/contents/231226212333745yz\n[20] https://coinalyze.net/category-all/open-interest/\n[21] https://coinalyze.net/ethereum/usdt/binance/ethusdt_perp/price-chart-live/\n[22] https://api.coinalyze.net/v1/doc/\n[23] https://www.coinglass.com/pro/futures/LiquidationHeatMap\n[24] https://coinalyze.net/category-defi/open-interest/\n[25] https://learn.bybit.com/trading/tools-for-trading-on-bybit/\n[26] https://coinalyze.net/bitcoin/usd/bybit/open-interest-chart/btcusd_oi/\n[27] https://github.com/mariano54/coinalyze\n[28] https://airbyte.com/how-to-sync/coin-api-to-local-json\n[29] https://arxiv.org/html/2402.03953v4\n[30] https://gitbook-docs.coinmetrics.io/access-our-data/api\n[31] https://ko.coinalyze.net/bitcoin/usd/kraken/price-chart-live/\n[32] https://github.com/OffcierCia/On-Chain-Investigations-Tools-List\n[33] https://info.arkm.com/research/leading-crypto-analysis-tools-for-traders-investors\n[34] https://tr.okx.com/docs-v5/en/\n[35] https://coinalyze.net/bitcoin/usdt/binance/btcusdt_perp/price-chart-live/\n[36] https://coinalyze.net/bitcoin/usdt/okx/btcusdt_perp/price-chart-live/\n[37] https://coinalyze.net/bitcoin/usd/coinbase/price-chart-live/\n[38] https://interactivebrokers.github.io/tws-api/historical_limitations.html\n[39] https://coinalyze.net/bitcoin/usd/bybit/btcusd/price-chart-live/\n[40] https://coinalyze.net/ethereum/open-interest/\n[41] https://www.meetgor.com/bash-crypto-scrapper/\n[42] https://www.youtube.com/watch?v=UaPAis0vtK0\n[43] https://stackoverflow.com/questions/44814910/crypto-comare-api-response-data-type?rq=3\n[44] https://www.linkedin.com/pulse/navigate-cryptocurrency-mania-simple-web-scrape-r-matt-lunkes\n\n체크","commandType":4},{"text":"이게 무슨에러?","commandType":4},{"text":"이쁘게 좀 조정해봐","commandType":4},{"text":"왜 77000일까? 현재가가 98000대인데? ","commandType":4},{"text":"아니 데이터 자체가 문제가 심각한데? 왜 7만대가 나와 현재 비트코인 시세를 체크하고 봐봐","commandType":4},{"text":"또 안되네?","commandType":4},{"text":"안되는데?","commandType":4},{"text":"API 응답과 에러 메시지를 분석한 결과, 다음과 같은 문제점들이 발견되었습니다:\n\n## 주요 문제점\n\n1. 잘못된 엔드포인트 사용:\n```python\nurl = f\"{BASE_URL}/v1/futures/price-history\"  # 잘못된 엔드포인트\n```\n\n2. 잘못된 심볼 형식:\n```python\nsymbol = \"BTCUSDT_PERP.B\"  # 잘못된 형식\n```\n\n## 수정 방안\n\n**1. 올바른 엔드포인트 사용**\n```python\nurl = f\"{BASE_URL}/v1/ohlcv-history\"  # 가격 데이터를 위한 올바른 엔드포인트\n```\n\n**2. 올바른 심볼 형식**\n```python\nsymbol = \"BTCUSDT_PERP.A\"  # Binance의 경우 .A 사용\n```\n\n**3. 파라미터 수정**\n```python\nparams = {\n    \"symbols\": symbol,\n    \"interval\": interval,\n    \"from\": int(from_time),  # 초 단위\n    \"to\": int(to_time),      # 초 단위\n    \"api_key\": API_KEY\n}\n```\n\n## 데이터 처리 로직 수정\n\n```python\nif historical_data:\n    df = pd.DataFrame([{\n        'timestamp': item['t'],\n        'open': item['o'],\n        'high': item['h'],\n        'low': item['l'],\n        'close': item['c'],\n        'volume': item.get('v', 0)\n    } for item in historical_data[0]['history']])\n    \n    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n```\n\n이러한 수정사항들을 적용하면 API가 정상적으로 작동할 것으로 예상됩니다. 특히 심볼 형식과 엔드포인트 URL이 가장 중요한 수정 포인트입니다[4][6].\n\nCitations:\n[1] https://pplx-res.cloudinary.com/image/upload/v1738767978/user_uploads/TRxSLGYzvKdEHku/image.jpg\n[2] https://developers.coinranking.com/api/documentation/coins/coin-price-history\n[3] https://developers.binance.com/docs/derivatives/change-log\n[4] https://api.coinalyze.net/v1/doc/\n[5] https://getblock.io/blog/coinalyze-track-and-monitor-crypto-markets/\n[6] https://goldfishman.tistory.com/71\n[7] https://pro.coinmarketcap.com/api/v1\n[8] https://www.coinapi.io/products/market-data-api\n[9] https://coinalyze.net\n[10] https://api.coinpaprika.com\n[11] https://docs.coincap.io\n[12] https://developers.binance.com/docs/derivatives/usds-margined-futures/market-data/rest-api/24hr-Ticker-Price-Change-Statistics\n[13] https://docs.coingecko.com/reference/coins-id-history\n[14] https://coinalyze.net/futures-data/\n[15] https://goldfishman.tistory.com/71\n[16] https://coinmarketcap.com/api/documentation/v1/\n[17] https://www.coinapi.io/products/market-data-api/pricing\n[18] https://ko.coinalyze.net/bitcoin/usdt/binance/btcusdt_perp/price-chart-live/\n[19] https://ko.coinalyze.net/bitcoin/usdt/binance/open-interest-chart/btcusdt_perp_oi/\n[20] https://developers.moralis.com/web3-wiki/coinalyze/\n[21] https://ko.coinalyze.net/bitcoin/usdt/binance/liquidation-chart/btcusdt_perp_lq/\n[22] https://coinalyze.net/bitcoin/usd/coinbase/price-chart-live/\n[23] https://coinalyze.net/request/open-interest/\n[24] https://stackoverflow.com/questions/67509891/how-to-view-aggregated-liquidations-for-cryptocurrencies-from-binance\n[25] https://getblock.io/blog/coinalyze-track-and-monitor-crypto-markets/\n[26] https://www.kucoin.com/docs/rest/futures-trading/positions/get-positions-history\n[27] https://dev.to/bloger_07/what-is-coinalyze-crypto-analysis-24ef\n[28] https://api.coinalyze.net/v1/doc/\n[29] https://stackoverflow.com/questions/63963411/what-are-the-symbols-used-in-binances-dapi-coin-futures-api/63972391\n[30] https://docs.coinapi.io/market-data/rest-api/metadata/list-all-symbols\n[31] https://www.kucoin.com/docs/rest/futures-trading/market-data/get-symbols-list\n\n체크","commandType":4},{"text":"API 응답과 에러 메시지를 분석한 결과, 다음과 같은 문제점들이 발견되었습니다:\n\n## 주요 문제점\n\n**1. 잘못된 심볼 형식**\n```python\nsymbol = \"BTCUSDT_PERP.B\"  # 잘못된 형식\n```\n바이낸스 선물의 올바른 심볼 형식은 \"BTCUSDT_PERP.A\"입니다[4][5].\n\n**2. 잘못된 엔드포인트**\n```python\nurl = f\"{BASE_URL}/v1/ohlcv-history\"  # 잘못된 엔드포인트\n```\n올바른 엔드포인트는 \"/v1/futures/price-history\"입니다[7].\n\n## 수정된 코드\n\n```python\nasync def fetch_price_history(session, symbols: str, from_time: int, to_time: int, interval: str = \"1hour\"):\n    url = f\"{BASE_URL}/v1/futures/price-history\"  # 올바른 엔드포인트\n    \n    params = {\n        \"symbols\": \"BTCUSDT_PERP.A\",  # 바이낸스 선물 올바른 심볼\n        \"interval\": interval,\n        \"from\": from_time,\n        \"to\": to_time,\n        \"api_key\": API_KEY\n    }\n    \n    async with session.get(url, params=params) as response:\n        if response.status == 200:\n            return await response.json()\n        elif response.status == 429:\n            retry_after = int(response.headers.get('Retry-After', 60))\n            logger.warning(f\"Rate limit reached. Waiting {retry_after} seconds...\")\n            await asyncio.sleep(retry_after)\n            return await fetch_price_history(session, symbols, from_time, to_time, interval)\n        else:\n            error_msg = await response.text()\n            logger.error(f\"API 오류 (상태 코드: {response.status}): {error_msg}\")\n            return None\n```\n\n## 추가 권장사항\n\n1. API 호출 제한 확인: 분당 40회로 제한되어 있습니다[41].\n2. 시간 범위 조정: 24시간 데이터를 한 번에 요청하는 대신 더 작은 단위로 나누어 요청\n3. 에러 처리 강화: 429 (Rate Limit) 에러에 대한 재시도 로직 추가\n4. 데이터 검증: 응답 데이터의 형식과 내용 검증 로직 추가\n\n이러한 수정사항들을 적용하면 API가 정상적으로 작동할 것으로 예상됩니다.\n\nCitations:\n[1] https://pplx-res.cloudinary.com/image/upload/v1738767978/user_uploads/TRxSLGYzvKdEHku/image.jpg\n[2] https://developers.coinranking.com/api/documentation/coins/coin-price\n[3] https://docs.coinapi.io/market-data/how-to-guides/get-historical-ohlcv-data-using-coinapi\n[4] https://api.coinalyze.net/v1/doc/\n[5] https://goldfishman.tistory.com/71\n[6] https://www.binance.com/en/support/announcement/updates-to-api-key-permission-rules-2021-07-26-11e4c2f44e7a47b9b5fc0e479c0b256f\n[7] https://getblock.io/blog/coinalyze-track-and-monitor-crypto-markets/\n[8] https://github.com/rsz44/python-coinmarketcap\n[9] https://finnhub.io/docs/api/crypto-candles\n[10] https://goldfishman.tistory.com/71\n[11] https://coinalyze.net\n[12] https://github.com/crypto-chassis/cryptochassis-data-api-docs\n[13] https://docs.coingecko.com/reference/endpoint-overview\n[14] https://coinmarketcap.com/api/documentation/v1/\n[15] https://docs.api.coinstats.app/reference/get-coin-avg-price\n[16] https://api.coinpaprika.com\n[17] https://coinalyze.net/bitcoin/usdt/binance/btcusdt_perp/price-chart-live/\n[18] https://coinalyze.net/bitcoin/usdt/binance/liquidation-chart/btcusdt_perp_lq/\n[19] https://coinalyze.net/mode/open-interest/\n[20] https://coinalyze.net/bitcoin/usdt/binance/long-short-ratio-chart/btcusdt_perp_glsra/\n[21] https://stackoverflow.com/questions/67509891/how-to-view-aggregated-liquidations-for-cryptocurrencies-from-binance\n[22] https://coinalyze.net/bitcoin/usdt/binance/open-interest-chart/btcusdt_perp_oi/\n[23] https://learn.bybit.com/trading/tools-for-trading-on-bybit/\n[24] https://coinalyze.net/bitcoin/usdt/bybit/btcusdt/price-chart-live/\n[25] https://ko.coinalyze.net/bitcoin/usd/kraken/price-chart-live/\n[26] https://coinalyze.net/bitcoin/usdt/bitmex/open-interest-chart/btcusdt_perp_oi/\n[27] https://dexterlab.com/follow-the-money-how-to-make-the-best-of-this-bull-cycle-by-studying-attention/\n[28] https://info.arkm.com/research/leading-crypto-analysis-tools-for-traders-investors\n[29] https://support.coinapi.io/hc/en-us/articles/360029202272--Invalid-API-Key-error-for-a-newly-purchased-key\n[30] https://support.cointracker.io/hc/en-us/articles/26260019609233-Troubleshooting-General-API-Synced-Error-Messages\n[31] https://stackoverflow.com/questions/30319561/how-long-is-the-google-api-key-valid\n[32] https://coinalyze.net/bitcoin/usd/coinbase/price-chart-live/\n[33] https://docs.coinmetrics.io/api/v4/\n[34] https://support.coingecko.com/hc/en-us/articles/23406416525209-Why-am-I-getting-an-error-or-timeout-when-calling-the-CoinGecko-API-endpoint\n[35] https://www.coinapi.io/legal\n[36] https://support.coinapi.io/hc/en-us/articles/360025657592-FIX-API-Logon-Troubleshooting\n[37] https://trade-docs.coinlist.co\n[38] https://github.com/ccxt/ccxt/issues/52\n[39] https://developers.coinranking.com/api/documentation/coins/coin-price-history\n[40] https://docs.coinapi.io/market-data/rest-api/ohlcv/historical-data\n[41] https://getblock.io/blog/coinalyze-track-and-monitor-crypto-markets/\n[42] https://dev.to/bloger_07/what-is-coinalyze-crypto-analysis-24ef\n[43] https://stackoverflow.com/questions/63963411/what-are-the-symbols-used-in-binances-dapi-coin-futures-api/63972391\n[44] https://coinalyze.net/bitcoin/open-interest/\n[45] https://coinalyze.net/category-all/open-interest/\n[46] https://help.coinpanda.io/en/articles/3839221-how-to-import-transactions-using-api-keys\n[47] https://docs.coinapi.io/general/authentication\n[48] https://sbicrypto.com/api/\n[49] https://docs.coinapi.io/market-data/authentication","commandType":4},{"text":"?????????????????????","commandType":4},{"text":"????????????????????????","commandType":4},{"text":"???????","commandType":4},{"text":"????????????????","commandType":4},{"text":"똑같이 안되 심볼 확인방법없냐","commandType":4},{"text":"????????????","commandType":4},{"text":"?????????????","commandType":4},{"text":"심볼을 확인하는 플을 만들던지 좀해봐","commandType":4},{"text":"맞습니다. 두 코드를 하나로 합치는 작업이 필요합니다. 제가 두 코드를 분석하고 통합 방법을 설명해 드리겠습니다.\n\n### 코드 분석\n\n**1. SymbolAnalyzer (첫 번째 코드)**\n- 개별 암호화폐 심볼의 데이터를 분석하는 클래스\n- 주요 기능:\n  - CSV 데이터 로드 및 전처리\n  - 멀티 타임프레임(5분, 30분, 1시간, 4시간) 데이터 준비\n  - 다양한 패턴 감지 (OI 패턴, 펀딩비 패턴, 복합 패턴)\n  - 분석 결과를 JSON으로 저장\n\n**2. AnalysisWorker (두 번째 코드)**\n- 여러 심볼에 대한 분석을 병렬로 수행하는 관리 클래스\n- 주요 기능:\n  - 분석할 심볼 목록 가져오기\n  - SymbolAnalyzer 인스턴스 생성 및 분석 실행\n  - ThreadPoolExecutor를 사용한 병렬 처리\n  - 분석 진행 상황 로깅\n\n### 분석 방식\n두 코드는 함께 암호화폐 시장의 기술적 패턴을 감지하는 시스템을 구성합니다:\n\n1. **데이터 기반 패턴 탐지**:\n   - OI(오픈 인터레스트) 변화 패턴: 급증 후 하락, 급락 후 반등, 횡보 후 급변\n   - 펀딩비 패턴: 방향 전환, 극단적 값, 예측값과 실제값 차이\n   - 복합 패턴: OI-펀딩비 다이버전스, 거래량+롱숏 극단, 대량 청산\n\n2. **신뢰도 계산**:\n   - 보조 지표(거래량, 롱숏 비율, 청산량, 추세 강도)를 활용하여 패턴의 신뢰도 점수 계산\n\n3. **멀티 타임프레임 분석**:\n   - 5분(기본), 30분(6캔들), 1시간(12캔들), 4시간(48캔들) 타임프레임에서 패턴 감지\n   - 다양한 시간대의 패턴을 복합적으로 분석\n\n4. **병렬 처리**:\n   - 여러 암호화폐를 동시에 분석하여 효율성 향상\n   - 스레드풀을 통한 동시 처리\n\n### 통합 코드 접근법\n\n두 코드를 통합하려면 다음과 같은 방식이 적절합니다:\n\n1. **단일 파일에 두 클래스 유지**:\n   - SymbolAnalyzer와 AnalysisWorker 클래스를 모두 하나의 파일에 포함\n   - 각 클래스의 기능은 그대로 유지하되 불필요한 중복 제거\n\n2. **통합 로깅 및 설정**:\n   - 로깅 설정 공유\n   - 공통 설정 통합\n\n3. **명령줄 인터페이스 추가**:\n   - 다양한 실행 모드와 옵션 제공\n   - 단일 심볼 분석, 전체 분석, 특정 패턴 검색 등의 기능 추가\n\n\n이 내용을 만들거야.\n\n1번 코드는 줄게 필요한거 모듈 설치해","commandType":4},{"text":"ccxt도 깔아야지?","commandType":4},{"text":"모듈부터깔자","commandType":4},{"text":"무슨문제일까요?","commandType":4},{"text":"파이선 버전부터체크하자","commandType":4},{"text":"\"\"\"\n암호화폐 데이터 통합 관리 시스템 (CCXT 데이터 보완 및 장기 보관 기능 추가)\n- 과거 데이터 누락 검사 및 복구 (CCXT로 보완)\n- 실시간 데이터 수집 및 저장\n- 단일 CSV 파일에 시간순 정렬 저장\n- 자동 심볼 추가 및 관리\n- 안전한 종료 메커니즘\n- 데이터 장기 보관\n- 소수점 정확도 유지\n\"\"\"\n\nimport os\nimport logging\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nimport time\nimport base64\nimport signal\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Union\nimport random\nimport threading\nimport queue\nimport ccxt\n\n# 안전한 종료를 위한 전역 플래그\nshutdown_flag = threading.Event()\n\n# 기본 설정\nBASE_URL = \"https://api.coinalyze.net/v1\"\nDATA_DIR = Path(\"data\")\nLIVE_DIR = DATA_DIR / \"live\"\nBACKUP_DIR = DATA_DIR / \"backup\"\nARCHIVE_DIR = DATA_DIR / \"archive\"\nLONG_TERM_DIR = DATA_DIR / \"longterm\"\n\n# API 요청 간격 설정 (초)\nAPI_REQUEST_INTERVAL = 0.5  # API 요청 사이 대기 시간 (초)\nCOLLECTION_INTERVAL = 300   # 데이터 수집 간격 (초) - 5분\n\n# API 엔드포인트\nENDPOINTS = {\n    \"exchanges\": \"/exchanges\",\n    \"future_markets\": \"/future-markets\",\n    \"spot_markets\": \"/spot-markets\",\n    \"open_interest\": \"/open-interest\",\n    \"funding_rate\": \"/funding-rate\",\n    \"predicted_funding_rate\": \"/predicted-funding-rate\",\n    \"open_interest_history\": \"/open-interest-history\",\n    \"funding_rate_history\": \"/funding-rate-history\",\n    \"predicted_funding_rate_history\": \"/predicted-funding-rate-history\",\n    \"liquidation_history\": \"/liquidation-history\",\n    \"long_short_ratio_history\": \"/long-short-ratio-history\",\n    \"ohlcv_history\": \"/ohlcv-history\"\n}\n\n# API 키 설정 (16개 키를 4개 티어로 구분)\nAPI_KEYS = {\n    \"tier1\": [\n        \"0d0ba171-4185-4a58-bc02-8c8627cd1f54\",\n        \"e629efa9-68b0-4b6b-b794-fa2d9e379b79\",\n        \"c9779cfd-a85c-4e58-a378-9474174a075e\",\n        \"58a39d7b-84e5-4852-8375-90de055cba18\"\n    ],\n    \"tier2\": [\n        \"7b80a59d-f0f9-4a83-81f7-1314dbdd9dc7\",\n        \"2b951698-64e7-4a86-9930-c503d4e29e54\",\n        \"10b24c79-ddd1-4046-94b6-0af0d23b241e\",\n        \"fac040b5-5043-4351-b8f3-09c1d8cfd78f\"\n    ],\n    \"tier3\": [\n        \"fe1caf7e-5e27-4f0c-9d14-2a4611db625f\",\n        \"fc9e5080-9607-46e8-b48c-deca57591990\",\n        \"a82fd8a4-873f-4619-9b9f-d8a29373b5b8\",\n        \"6431161c-8815-4d18-846d-55e00863682e\"\n    ],\n    \"tier4\": [\n        \"07cc6de0-0d02-41b8-acdc-dd1284bf5730\",\n        \"f2842bed-d43e-4152-a0b5-68d9d9ed30b0\",\n        \"013d0da7-76ea-4699-97d7-6e0f66857939\",\n        \"7906d7bf-b493-42ad-a578-0cd13c6a070c\"\n    ]\n}\n\n# CCXT 설정 - 공개 데이터만 사용하므로 API 키 불필요\nCCXT_EXCHANGES = [\n    \"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"\n]\n\n# 로깅 설정\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler(\"crypto_data_manager.log\", encoding='utf-8')\n    ]\n)\nlogger = logging.getLogger(__name__)\n\ndef signal_handler(sig, frame):\n    \"\"\"시스템 종료 신호를 처리하는 핸들러\"\"\"\n    logger.warning(\"종료 신호를 받았습니다. 안전하게 종료합니다...\")\n    shutdown_flag.set()\n\n# 시그널 핸들러 등록\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\n\ndef check_stop_file():\n    \"\"\"종료 파일 확인\"\"\"\n    if os.path.exists(\"stop.signal\"):\n        logger.warning(\"종료 파일이 감지되었습니다. 안전하게 종료합니다...\")\n        try:\n            os.remove(\"stop.signal\")  # 종료 파일 삭제\n        except:\n            pass\n        shutdown_flag.set()\n        return True\n    return False\n\nclass APIKeyManager:\n    \"\"\"API 키 관리 클래스\"\"\"\n    def __init__(self):\n        self.tier_keys = API_KEYS\n        self.current_key_index = {tier: 0 for tier in API_KEYS.keys()}\n        self.key_locks = {tier: threading.Lock() for tier in API_KEYS.keys()}\n        self.key_rate_limits = {key: {'last_used': 0, 'count': 0} for tier in API_KEYS.values() for key in tier}\n        self.rate_limit_reset_time = time.time() + 60  # 1분 후 리셋\n        \n    def get_next_key(self, tier: str) -> str:\n        \"\"\"라운드 로빈 방식으로 다음 API 키 반환\"\"\"\n        if tier not in self.tier_keys or not self.tier_keys[tier]:\n            available_tiers = [t for t in self.tier_keys if self.tier_keys[t]]\n            if not available_tiers:\n                raise ValueError(\"사용 가능한 API 키가 없습니다\")\n            tier = random.choice(available_tiers)\n            \n        with self.key_locks[tier]:\n            keys = self.tier_keys[tier]\n            current_index = self.current_key_index[tier]\n            \n            # 현재 시간이 리셋 시간을 초과했는지 확인\n            current_time = time.time()\n            if current_time > self.rate_limit_reset_time:\n                # 사용량 카운터 리셋\n                for key_info in self.key_rate_limits.values():\n                    key_info['count'] = 0\n                self.rate_limit_reset_time = current_time + 60  # 다음 1분 후 리셋\n            \n            # 사용량이 가장 적은 키를 찾아 반환\n            min_count = float('inf')\n            selected_key = None\n            \n            for i in range(len(keys)):\n                idx = (current_index + i) % len(keys)\n                key = keys[idx]\n                \n                if self.key_rate_limits[key]['count'] < min_count:\n                    min_count = self.key_rate_limits[key]['count']\n                    selected_key = key\n                    self.current_key_index[tier] = (idx + 1) % len(keys)\n            \n            # 키 사용량 기록\n            self.key_rate_limits[selected_key]['last_used'] = current_time\n            self.key_rate_limits[selected_key]['count'] += 1\n            \n            # API 요청 간격 조절을 위한 대기\n            last_used = self.key_rate_limits[selected_key]['last_used']\n            if current_time - last_used < API_REQUEST_INTERVAL:\n                time.sleep(API_REQUEST_INTERVAL - (current_time - last_used))\n            \n            return selected_key\n\nclass CcxtManager:\n    \"\"\"CCXT 거래소 관리 및 심볼 매핑 클래스\"\"\"\n    def __init__(self):\n        self.exchanges = {}\n        self.symbol_mappings = {}  # base_symbol -> {exchange_id -> ccxt_symbol}\n        self.exchange_priority = [\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"]\n        self.init_exchanges()\n        self.supported_symbols = set()  # CCXT에서 지원하는 심볼 목록\n        self.timeframe_map = {\n            \"1min\": \"1m\", \"5min\": \"5m\", \"15min\": \"15m\", \"30min\": \"30m\",\n            \"1h\": \"1h\", \"4h\": \"4h\", \"1d\": \"1d\", \"1w\": \"1w\"\n        }\n        \n    def init_exchanges(self):\n        \"\"\"CCXT 거래소 초기화 (API 키 없이)\"\"\"\n        for exchange_id in CCXT_EXCHANGES:\n            try:\n                if exchange_id in ccxt.exchanges:\n                    exchange_class = getattr(ccxt, exchange_id)\n                    self.exchanges[exchange_id] = exchange_class({\n                        'enableRateLimit': True,\n                        'options': {\n                            'defaultType': 'future'  # 선물 거래 기본 설정\n                        }\n                    })\n                    logger.info(f\"CCXT {exchange_id} 거래소 초기화 성공\")\n            except Exception as e:\n                logger.error(f\"CCXT {exchange_id} 거래소 초기화 실패: {str(e)}\")\n    \n    def load_all_markets(self):\n        \"\"\"모든 거래소의 시장 정보 로드\"\"\"\n        for exchange_id, exchange in self.exchanges.items():\n            try:\n                exchange.load_markets()\n                logger.info(f\"{exchange_id} 시장 정보 로드 성공: {len(exchange.symbols)}개 심볼\")\n            except Exception as e:\n                logger.error(f\"{exchange_id} 시장 정보 로드 실패: {str(e)}\")\n    \n    def discover_symbol_mappings(self, base_symbols):\n        \"\"\"여러 기본 심볼에 대한 CCXT 매핑 자동 탐색\"\"\"\n        logger.info(f\"CCXT 심볼 매핑 탐색 시작: {len(base_symbols)}개 심볼\")\n        \n        # 거래소 시장 정보 로드\n        self.load_all_markets()\n        \n        # 각 심볼별 매핑 탐색\n        for base_symbol in base_symbols:\n            self.discover_symbol_mapping(base_symbol)\n        \n        # 매핑 정보 저장\n        self.save_symbol_mappings()\n        \n        logger.info(f\"CCXT 심볼 매핑 탐색 완료: {len(self.symbol_mappings)}개 심볼\")\n    \n    def discover_symbol_mapping(self, base_symbol):\n        \"\"\"단일 기본 심볼에 대한 CCXT 매핑 탐색\"\"\"\n        if base_symbol in self.symbol_mappings:\n            return\n            \n        self.symbol_mappings[base_symbol] = {}\n        \n        # 거래소별 검색 패턴\n        search_patterns = {\n            \"binance\": [\n                f\"{base_symbol}/USDT:USDT\",  # 영구 선물\n                f\"{base_symbol}USDT_PERP\",\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"bybit\": [\n                f\"{base_symbol}USDT\",        # 영구 선물\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"okx\": [\n                f\"{base_symbol}-USDT-SWAP\",  # 영구 선물\n                f\"{base_symbol}/USDT:USDT\",\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"kucoin\": [\n                f\"{base_symbol}USDTM\",       # 영구 선물\n                f\"{base_symbol}-USDT\",       # 영구 선물 (다른 형식)\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"bitget\": [\n                f\"{base_symbol}USDT_UMCBL\",  # 영구 선물\n                f\"{base_symbol}/USDT\"        # 현물\n            ]\n        }\n        \n        # 거래소 우선순위대로 검색\n        for exchange_id in self.exchange_priority:\n            if exchange_id not in self.exchanges:\n                continue\n                \n            exchange = self.exchanges[exchange_id]\n            patterns = search_patterns.get(exchange_id, [f\"{base_symbol}/USDT\"])\n            \n            for pattern in patterns:\n                found = False\n                for symbol in exchange.symbols:\n                    normalized_symbol = symbol.upper()\n                    normalized_pattern = pattern.upper()\n                    \n                    # 정확한 일치 또는 포함 관계 확인\n                    if normalized_symbol == normalized_pattern or normalized_pattern in normalized_symbol:\n                        self.symbol_mappings[base_symbol][exchange_id] = symbol\n                        logger.info(f\"심볼 매핑 발견: {base_symbol} -> {exchange_id}:{symbol}\")\n                        self.supported_symbols.add(base_symbol)\n                        found = True\n                        break\n                \n                if found:\n                    break\n    \n    def save_symbol_mappings(self):\n        \"\"\"심볼 매핑 정보 파일로 저장\"\"\"\n        try:\n            filepath = DATA_DIR / \"symbol_mappings.json\"\n            with open(filepath, 'w') as f:\n                json.dump({\n                    \"mappings\": self.symbol_mappings,\n                    \"supported_symbols\": list(self.supported_symbols),\n                    \"timestamp\": datetime.now().isoformat()\n                }, f, indent=2)\n            logger.info(f\"심볼 매핑 정보 저장 완료: {filepath}\")\n        except Exception as e:\n            logger.error(f\"심볼 매핑 정보 저장 실패: {str(e)}\")\n    \n    def load_symbol_mappings(self):\n        \"\"\"저장된 심볼 매핑 정보 로드\"\"\"\n        try:\n            filepath = DATA_DIR / \"symbol_mappings.json\"\n            if filepath.exists():\n                with open(filepath, 'r') as f:\n                    data = json.load(f)\n                    self.symbol_mappings = data.get(\"mappings\", {})\n                    self.supported_symbols = set(data.get(\"supported_symbols\", []))\n                logger.info(f\"심볼 매핑 정보 로드 완료: {len(self.symbol_mappings)}개 심볼\")\n                return True\n            return False\n        except Exception as e:\n            logger.error(f\"심볼 매핑 정보 로드 실패: {str(e)}\")\n            return False\n    \n    def get_ccxt_symbol(self, base_symbol, exchange_id=None):\n        \"\"\"CCXT 심볼 가져오기 (특정 거래소 또는 우선순위 기반)\"\"\"\n        # 매핑 정보 확인\n        if base_symbol not in self.symbol_mappings:\n            self.discover_symbol_mapping(base_symbol)\n        \n        # 특정 거래소 지정한 경우\n        if exchange_id:\n            return self.symbol_mappings.get(base_symbol, {}).get(exchange_id)\n        \n        # 거래소 우선순위 기준 탐색\n        for ex_id in self.exchange_priority:\n            if ex_id in self.symbol_mappings.get(base_symbol, {}):\n                return self.symbol_mappings[base_symbol][ex_id], ex_id\n        \n        return None, None\n    \n    def fetch_ohlcv(self, base_symbol, timeframe='5m', since=None, limit=1000):\n        \"\"\"CCXT를 통해 OHLCV 데이터 가져오기\"\"\"\n        if shutdown_flag.is_set():\n            return pd.DataFrame()\n            \n        # 심볼 매핑 확인 및 가져오기\n        ccxt_symbol, exchange_id = self.get_ccxt_symbol(base_symbol)\n        \n        if not ccxt_symbol or not exchange_id:\n            logger.warning(f\"CCXT 심볼 매핑을 찾을 수 없음: {base_symbol}\")\n            return pd.DataFrame()\n        \n        # 타임프레임 변환\n        ccxt_timeframe = self.timeframe_map.get(timeframe, timeframe)\n        \n        # since 변환 (timestamp to milliseconds)\n        since_ms = int(since.timestamp() * 1000) if since else None\n        \n        # 데이터 가져오기 시도\n        try:\n            exchange = self.exchanges[exchange_id]\n            \n            # OHLCV 데이터 가져오기\n            ohlcv = exchange.fetch_ohlcv(ccxt_symbol, ccxt_timeframe, since_ms, limit)\n            \n            if not ohlcv:\n                logger.warning(f\"CCXT에서 데이터 없음: {exchange_id}:{ccxt_symbol}\")\n                return pd.DataFrame()\n            \n            # DataFrame 변환\n            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n            df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n            df['symbol'] = base_symbol\n            df['exchange'] = exchange_id\n            \n            logger.info(f\"CCXT 데이터 수집 성공: {exchange_id}:{ccxt_symbol}, {len(df)}행\")\n            return df\n            \n        except Exception as e:\n            logger.error(f\"CCXT 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n            return pd.DataFrame()\n    \n    def fetch_funding_rate(self, base_symbol, timeframe='1h', since=None, limit=500):\n        \"\"\"CCXT를 통해 펀딩비 데이터 가져오기 (지원하는 거래소만)\"\"\"\n        if shutdown_flag.is_set():\n            return pd.DataFrame()\n        \n        # 펀딩비를 지원하는 거래소들\n        funding_supported = [\"binance\", \"bybit\", \"okx\"]\n        funding_data = []\n        \n        # 지원하는 거래소에서 데이터 수집 시도\n        for exchange_id in funding_supported:\n            if exchange_id not in self.exchanges:\n                continue\n                \n            ccxt_symbol = self.get_ccxt_symbol(base_symbol, exchange_id)\n            if not ccxt_symbol:\n                continue\n                \n            try:\n                exchange = self.exchanges[exchange_id]\n                \n                # 1. fetchFundingRateHistory 사용 (최신 데이터)\n                if hasattr(exchange, 'fetchFundingRateHistory') and callable(getattr(exchange, 'fetchFundingRateHistory')):\n                    since_ms = int(since.timestamp() * 1000) if since else None\n                    funding_history = exchange.fetch_funding_rate_history(ccxt_symbol, since_ms, limit)\n                    \n                    if funding_history:\n                        for entry in funding_history:\n                            funding_data.append({\n                                'datetime': pd.to_datetime(entry['timestamp'], unit='ms'),\n                                'symbol': base_symbol,\n                                'exchange': exchange_id,\n                                'funding_rate': entry['fundingRate']\n                            })\n                \n                # 2. fetchFundingRate 사용 (현재 펀딩비)\n                elif hasattr(exchange, 'fetchFundingRate') and callable(getattr(exchange, 'fetchFundingRate')):\n                    funding_info = exchange.fetch_funding_rate(ccxt_symbol)\n                    if funding_info:\n                        funding_data.append({\n                            'datetime': pd.to_datetime(funding_info['timestamp'], unit='ms'),\n                            'symbol': base_symbol,\n                            'exchange': exchange_id,\n                            'funding_rate': funding_info['fundingRate']\n                        })\n            \n            except Exception as e:\n                logger.error(f\"CCXT 펀딩비 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n        \n        # 수집된 데이터를 DataFrame으로 변환\n        if funding_data:\n            df = pd.DataFrame(funding_data)\n            logger.info(f\"CCXT 펀딩비 데이터 수집 성공: {base_symbol}, {len(df)}행\")\n            return df\n        else:\n            return pd.DataFrame()\n\n# API 키 매니저 인스턴스 생성\napi_key_manager = APIKeyManager()\n\n# CCXT 매니저 인스턴스 생성\nccxt_manager = CcxtManager()\n\ndef get_simple_symbol(symbol: str) -> str:\n    \"\"\"심볼명을 단순화 (예: BTCUSDT_PERP.A -> BTC)\"\"\"\n    if 'USDT_PERP.A' in symbol:\n        return symbol.split('USDT_')[0]\n    return symbol.replace('.csv', '')\n\ndef exponential_backoff(attempt, base_delay=2, max_delay=120):\n    \"\"\"지수 백오프 알고리즘\"\"\"\n    delay = min(max_delay, base_delay * (2 ** attempt))\n    jitter = random.uniform(0, 0.1 * delay)  # 10% 지터 추가\n    return delay + jitter\n\ndef make_request(endpoint: str, params: dict = None, tier: str = \"tier1\", max_retries: int = 5) -> Optional[dict]:\n    \"\"\"API 요청 수행 (지수 백오프 적용)\"\"\"\n    url = f\"{BASE_URL}{endpoint}\"\n    \n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(\"종료 중에는 API 요청을 수행하지 않습니다\")\n        return None\n    \n    for attempt in range(max_retries):\n        try:\n            api_key = api_key_manager.get_next_key(tier)\n            headers = {\n                \"Accept\": \"application/json\",\n                \"Api-Key\": api_key\n            }\n            \n            response = requests.get(url, headers=headers, params=params, timeout=60)\n            \n            if response.status_code == 429:  # Rate limit exceeded\n                sleep_time = exponential_backoff(attempt)\n                logger.warning(f\"API 속도 제한 초과. {sleep_time:.2f}초 후 재시도... (시도 {attempt+1}/{max_retries})\")\n                time.sleep(sleep_time)\n                continue\n                \n            if response.status_code == 401:  # 인증 실패\n                logger.error(f\"API 키 인증 실패: {api_key}\")\n                continue\n                \n            response.raise_for_status()\n            return response.json()\n            \n        except requests.exceptions.Timeout:\n            sleep_time = exponential_backoff(attempt)\n            logger.error(f\"API 요청 타임아웃 ({endpoint}) - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n            time.sleep(sleep_time)\n        except requests.exceptions.RequestException as e:\n            sleep_time = exponential_backoff(attempt)\n            logger.error(f\"API 요청 실패 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n            time.sleep(sleep_time)\n        except Exception as e:\n            sleep_time = exponential_backoff(attempt)\n            logger.error(f\"예상치 못한 오류 발생 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n            time.sleep(sleep_time)\n    \n    return None\n\ndef get_all_symbols() -> List[str]:\n    \"\"\"사용 가능한 모든 심볼 목록 조회\"\"\"\n    data = make_request(ENDPOINTS[\"future_markets\"])\n    \n    if not data:\n        logger.error(\"심볼 목록을 가져올 수 없습니다\")\n        return []\n        \n    symbols = []\n    if isinstance(data, dict):\n        markets = data.get('markets', [])\n    else:\n        markets = data\n        \n    for market in markets:\n        symbol = market.get('symbol', '')\n        if symbol.endswith('USDT_PERP.A'):\n            base_symbol = get_simple_symbol(symbol)\n            symbols.append(base_symbol)\n    \n    logger.info(f\"총 {len(symbols)}개의 심볼을 찾았습니다\")\n    \n    # 심볼 목록 파일로 저장\n    with open(DATA_DIR / 'symbols.json', 'w') as f:\n        json.dump({\n            'symbols': symbols,\n            'count': len(symbols),\n            'timestamp': datetime.now().isoformat()\n        }, f, indent=2)\n        \n    return sorted(symbols)\n\ndef extract_data_from_response(data, data_type: str):\n    \"\"\"API 응답에서 데이터 추출 개선\"\"\"\n    history = []\n    \n    try:\n        # 응답 데이터 구조 확인\n        if isinstance(data, list):\n            # 리스트 응답 처리\n            if len(data) > 0:\n                if isinstance(data[0], dict):\n                    if 'history' in data[0]:\n                        # 기존 구조: [{\"history\": [...]}]\n                        history = data[0]['history']\n                    else:\n                        # 새 구조: 리스트가 직접 데이터인 경우\n                        history = data\n                else:\n                    # 직접 데이터 리스트인 경우\n                    history = data\n        elif isinstance(data, dict):\n            # 딕셔너리 응답 처리\n            if 'history' in data:\n                history = data['history']\n            elif 'data' in data:\n                history = data['data']\n            elif 'result' in data:\n                history = data['result']\n            elif 'items' in data:\n                history = data['items']\n            else:\n                # 다른 키가 없으면 응답 전체를 사용\n                potential_data = []\n                for key, value in data.items():\n                    if isinstance(value, list) and len(value) > 0:\n                        potential_data.extend(value)\n                if potential_data:\n                    history = potential_data\n    except Exception as e:\n        logger.error(f\"{data_type} 데이터 파싱 중 오류: {str(e)}\")\n    \n    return history\n\ndef normalize_columns(df, data_type: str) -> pd.DataFrame:\n    \"\"\"데이터 프레임 컬럼 정규화 (소수점 보존)\"\"\"\n    # 타임스탬프 변환\n    if 't' in df.columns:\n        df['datetime'] = pd.to_datetime(df['t'], unit='s')\n    elif 'timestamp' in df.columns:\n        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n    elif 'time' in df.columns:\n        df['datetime'] = pd.to_datetime(df['time'], unit='s')\n    \n    # 데이터 타입별 컬럼 처리\n    if data_type == 'ohlcv':\n        column_mappings = {\n            'o': 'open', 'open': 'open', 'price_open': 'open',\n            'h': 'high', 'high': 'high', 'price_high': 'high',\n            'l': 'low', 'low': 'low', 'price_low': 'low',\n            'c': 'close', 'close': 'close', 'price_close': 'close',\n            'v': 'volume', 'volume': 'volume', 'vol': 'volume'\n        }\n    elif data_type == 'oi':\n        column_mappings = {\n            'c': 'open_interest', 'value': 'open_interest', \n            'openInterest': 'open_interest', 'oi': 'open_interest'\n        }\n    elif data_type == 'funding':\n        column_mappings = {\n            'c': 'funding_rate', 'rate': 'funding_rate', \n            'fundingRate': 'funding_rate', 'funding': 'funding_rate'\n        }\n    elif data_type == 'predicted_funding':\n        column_mappings = {\n            'c': 'predicted_funding_rate', 'rate': 'predicted_funding_rate',\n            'predictedRate': 'predicted_funding_rate', 'predicted': 'predicted_funding_rate'\n        }\n    elif data_type == 'liquidation':\n        column_mappings = {\n            'c': 'liquidation', 'value': 'liquidation',\n            'liq': 'liquidation', 'liquidations': 'liquidation'\n        }\n    elif data_type == 'long_short_ratio':\n        column_mappings = {\n            'c': 'long_short_ratio', 'ratio': 'long_short_ratio',\n            'l': 'long_percentage', 'long': 'long_percentage', 'longPercentage': 'long_percentage',\n            's': 'short_percentage', 'short': 'short_percentage', 'shortPercentage': 'short_percentage'\n        }\n    else:\n        column_mappings = {}\n    \n    # 컬럼 매핑 적용\n    for old_col, new_col in column_mappings.items():\n        if old_col in df.columns and old_col != new_col:\n            df[new_col] = df[old_col]\n    \n    return df\n\ndef validate_data(df, data_type: str) -> pd.DataFrame:\n    \"\"\"데이터 검증 (소수점 보존)\"\"\"\n    if df.empty:\n        return df\n    \n    # 타임스탬프가 없는 행 제거\n    if 'datetime' in df.columns:\n        df = df[df['datetime'].notna()]\n    \n    # 데이터 타입별 검증\n    if data_type == 'ohlcv':\n        # 가격 데이터 검증 (음수 값 제거, 소수점은 유지)\n        for col in ['open', 'high', 'low', 'close']:\n            if col in df.columns:\n                # 0 이하 값 확인 후 경고만 발생 (제거하지 않음)\n                zero_or_neg = df[df[col] <= 0]\n                if not zero_or_neg.empty:\n                    logger.warning(f\"{data_type}: {len(zero_or_neg)}개 행에서 {col} 값이 0 이하입니다. 데이터 유지됨.\")\n        \n        # 볼륨 검증 (음수 제거)\n        if 'volume' in df.columns:\n            neg_volume = df[df['volume'] < 0]\n            if not neg_volume.empty:\n                logger.warning(f\"{data_type}: {len(neg_volume)}개 행에서 volume 값이 음수입니다. 해당 값을 0으로 설정합니다.\")\n                df.loc[df['volume'] < 0, 'volume'] = 0\n    \n    elif data_type == 'funding' or data_type == 'predicted_funding':\n        # 극단적인 펀딩비 값 확인 (소수점은 유지)\n        funding_col = 'funding_rate' if data_type == 'funding' else 'predicted_funding_rate'\n        if funding_col in df.columns:\n            extreme_funding = df[(df[funding_col] < -0.15) | (df[funding_col] > 0.15)]\n            if not extreme_funding.empty:\n                logger.warning(f\"{data_type}: {len(extreme_funding)}개 행에서 {funding_col} 값이 극단적입니다 (-0.15 미만 또는 0.15 초과). 데이터 유지됨.\")\n    \n    elif data_type == 'long_short_ratio':\n        # 롱/숏 퍼센티지 합이 100%인지 확인 (오차 범위 ±2%)\n        if 'long_percentage' in df.columns and 'short_percentage' in df.columns:\n            total = df['long_percentage'] + df['short_percentage']\n            invalid_rows = df[(total < 98) | (total > 102)]\n            if not invalid_rows.empty:\n                logger.warning(f\"{data_type}: {len(invalid_rows)}개 행에서 long_percentage와 short_percentage 합이 100%와 크게 다릅니다. 데이터 유지됨.\")\n    \n    return df\n\ndef process_ohlcv_data(data) -> pd.DataFrame:\n    \"\"\"OHLCV 데이터 처리 (대체 소스용)\"\"\"\n    history = extract_data_from_response(data, \"ohlcv\")\n    \n    if not history:\n        return pd.DataFrame()\n        \n    try:\n        df = pd.DataFrame(history)\n        df = normalize_columns(df, \"ohlcv\")\n        \n        if 'datetime' not in df.columns and 't' in df.columns:\n            df['datetime'] = pd.to_datetime(df['t'], unit='s')\n        \n        # 필수 OHLCV 컬럼 확인\n        required_cols = ['open', 'high', 'low', 'close']\n        missing_cols = [col for col in required_cols if col not in df.columns]\n        \n        if missing_cols:\n            logger.warning(f\"OHLCV 대체 소스에서 필수 컬럼 누락: {missing_cols}\")\n            return pd.DataFrame()\n            \n        return df\n    except Exception as e:\n        logger.error(f\"OHLCV 대체 소스 데이터 처리 오류: {str(e)}\")\n        return pd.DataFrame()\n\ndef merge_dataframes(all_data_frames: Dict[str, pd.DataFrame], symbol: str) -> pd.DataFrame:\n    \"\"\"여러 데이터프레임 병합 (소수점 보존)\"\"\"\n    if not all_data_frames:\n        return pd.DataFrame()\n    \n    # 가능한 모든 기준 프레임 후보 (우선순위 순)\n    base_frame_candidates = [\"ohlcv\", \"long_short_ratio\", \"funding\", \"oi\"]\n    base_df = None\n    \n    # 우선순위에 따라 기준 프레임 선택\n    for candidate in base_frame_candidates:\n        if candidate in all_data_frames and not all_data_frames[candidate].empty:\n            base_df = all_data_frames[candidate].copy()\n            logger.info(f\"{symbol} 기준 프레임으로 {candidate} 선택 ({len(base_df)}행)\")\n            break\n    \n    # 기준 프레임이 없으면 빈 DataFrame 반환\n    if base_df is None:\n        logger.warning(f\"{symbol} 병합할 기준 프레임이 없습니다.\")\n        return pd.DataFrame()\n    \n    # 심볼 정보 확인 및 추가\n    if 'symbol' not in base_df.columns:\n        base_df['symbol'] = symbol\n        \n    # 기본 프레임 컬럼 이름 확인 (중복 방지 위해)\n    # 문제가 되는 t, o, h, l, c 등 짧은 컬럼명을 미리 표준화\n    if 't' in base_df.columns:\n        base_df.rename(columns={'t': 'timestamp'}, inplace=True)\n    for col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n        if col_old in base_df.columns:\n            base_df.rename(columns={col_old: col_new}, inplace=True)\n            \n    # 표준 컬럼명 리스트\n    std_cols = {\n        'open', 'high', 'low', 'close', 'volume',\n        'open_interest', 'funding_rate', 'predicted_funding_rate',\n        'liquidation', 'long_percentage', 'short_percentage', 'long_short_ratio',\n        'datetime', 'symbol'\n    }\n    \n    # 각 데이터프레임을 병합 (왼쪽 조인 방식으로 변경)\n    for data_type, df in all_data_frames.items():\n        if data_type == list(base_frame_candidates)[0] or df.empty:\n            continue  # 기준 프레임이거나 빈 프레임은 건너뜀\n        \n        # 표준화된 컬럼명으로 변환\n        if 't' in df.columns:\n            df.rename(columns={'t': 'timestamp'}, inplace=True)\n        for col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n            if col_old in df.columns:\n                df.rename(columns={col_old: col_new}, inplace=True)\n        \n        # 기준 프레임에 없는 컬럼만 추출\n        columns_to_merge = []\n        for col in df.columns:\n            # 표준 컬럼 외의 컬럼은 고유한 이름으로 변경\n            if col not in std_cols and col in base_df.columns:\n                new_col = f\"{col}_{data_type}\"\n                df.rename(columns={col: new_col}, inplace=True)\n                columns_to_merge.append(new_col)\n            elif col not in base_df.columns and col != 'datetime' and col != 'symbol':\n                columns_to_merge.append(col)\n        \n        if not columns_to_merge and 'datetime' in df.columns:\n            # 추가할 컬럼이 없으면 다음으로\n            continue\n            \n        # 병합 (왼쪽 조인 방식으로 변경)\n        merge_df = df[['datetime'] + columns_to_merge].copy()\n        base_df = pd.merge(base_df, merge_df, on='datetime', how='left', suffixes=('', f'_{data_type}'))\n    \n    # 최종 정리\n    base_df.sort_values('datetime', inplace=True)\n    \n    return base_df\n\ndef fetch_historical_data(symbol: str, data_type: str, start_time: datetime, end_time: datetime, interval: str = \"5min\") -> pd.DataFrame:\n    \"\"\"특정 심볼의 역사적 데이터 수집 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} {data_type} 수집이 중단되었습니다 (종료 중)\")\n        return pd.DataFrame()\n    \n    endpoint_map = {\n        \"ohlcv\": \"ohlcv_history\",\n        \"oi\": \"open_interest_history\",\n        \"funding\": \"funding_rate_history\",\n        \"predicted_funding\": \"predicted_funding_rate_history\",\n        \"liquidation\": \"liquidation_history\",\n        \"long_short_ratio\": \"long_short_ratio_history\"\n    }\n    \n    if data_type not in endpoint_map:\n        logger.error(f\"지원되지 않는 데이터 타입: {data_type}\")\n        return pd.DataFrame()\n    \n    # 1. Coinalyze API 요청\n    params = {\n        \"symbols\": f\"{symbol}USDT_PERP.A\",\n        \"interval\": interval,\n        \"from\": int(start_time.timestamp()),\n        \"to\": int(end_time.timestamp())\n    }\n    \n    tier = \"tier\" + str(random.randint(1, 4))  # 티어 무작위 선택\n    data = make_request(ENDPOINTS[endpoint_map[data_type]], params, tier)\n    \n    # API 응답 처리\n    if data is not None:\n        # 데이터 추출\n        history = extract_data_from_response(data, data_type)\n        \n        # DataFrame 생성 및 처리\n        try:\n            df = pd.DataFrame(history)\n            \n            # 컬럼 정규화 (소수점 보존)\n            if not df.empty:\n                df = normalize_columns(df, data_type)\n                \n                # 심볼 정보 추가\n                if 'symbol' not in df.columns:\n                    df['symbol'] = symbol\n                \n                logger.info(f\"Coinalyze {data_type} 데이터 수집 성공: {symbol}, {len(df)}행\")\n                return df\n                \n        except Exception as e:\n            logger.error(f\"{symbol} DataFrame 처리 중 오류: {str(e)}\")\n    \n    # 2. Coinalyze API 실패 시 CCXT 시도 (OHLCV 및 펀딩비만 지원)\n    if data_type == \"ohlcv\":\n        logger.warning(f\"{symbol} Coinalyze OHLCV 데이터 수집 실패, CCXT로 시도...\")\n        df = ccxt_manager.fetch_ohlcv(symbol, interval, start_time, 1000)\n        if not df.empty:\n            return df\n    \n    elif data_type == \"funding\":\n        logger.warning(f\"{symbol} Coinalyze 펀딩비 데이터 수집 실패, CCXT로 시도...\")\n        df = ccxt_manager.fetch_funding_rate(symbol, interval, start_time, 500)\n        if not df.empty:\n            return df\n    \n    # 데이터를 가져오지 못한 경우\n    logger.warning(f\"{symbol} {data_type} 데이터 수집 실패 (Coinalyze 및 CCXT)\")\n    return pd.DataFrame()\n\ndef load_existing_data(symbol: str) -> pd.DataFrame:\n    \"\"\"기존 데이터 로드 (소수점 보존)\"\"\"\n    csv_path = LIVE_DIR / f\"{symbol}.csv\"\n    \n    if not csv_path.exists():\n        return pd.DataFrame()\n        \n    try:\n        # 소수점 반올림 없이 데이터 로드\n        df = pd.read_csv(csv_path)\n        df['datetime'] = pd.to_datetime(df['datetime'])\n        logger.info(f\"{symbol} 기존 데이터 로드: {len(df)}행\")\n        return df\n    except Exception as e:\n        logger.error(f\"{symbol} 데이터 로드 실패: {str(e)}\")\n        \n        # 손상된 파일 처리\n        try:\n            # 손상된 파일 백업\n            corrupted_dir = DATA_DIR / \"corrupted\"\n            corrupted_dir.mkdir(parents=True, exist_ok=True)\n            \n            backup_file = corrupted_dir / f\"{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n            os.rename(csv_path, backup_file)\n            logger.warning(f\"손상된 파일 이동: {csv_path} -> {backup_file}\")\n        except Exception as backup_err:\n            logger.error(f\"손상된 파일 백업 실패: {str(backup_err)}\")\n        \n        return pd.DataFrame()\n\ndef save_data(symbol: str, new_data: pd.DataFrame) -> bool:\n    \"\"\"데이터 저장 (기존 데이터와 병합, 소수점 보존)\"\"\"\n    if new_data.empty:\n        logger.warning(f\"{symbol}: 저장할 새 데이터가 없습니다\")\n        return False\n    \n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 저장이 중단되었습니다 (종료 중)\")\n        return False\n        \n    LIVE_DIR.mkdir(parents=True, exist_ok=True)\n    csv_path = LIVE_DIR / f\"{symbol}.csv\"\n    \n    try:\n        # 1. 기존 데이터 로드\n        existing_data = load_existing_data(symbol)\n        \n        # 2. 새 데이터와 기존 데이터 병합\n        if not existing_data.empty:\n            # 기존 데이터에 이미 있는 타임스탬프 제외\n            new_data_filtered = new_data[~new_data['datetime'].isin(existing_data['datetime'])]\n            \n            if not new_data_filtered.empty:\n                df = pd.concat([existing_data, new_data_filtered])\n            else:\n                logger.info(f\"{symbol}: 새 데이터가 이미 기존 데이터에 포함되어 있습니다\")\n                return True\n        else:\n            df = new_data\n            \n        # 3. 중복 컬럼 제거\n        column_mappings = {\n            'price_open': 'open',\n            'price_high': 'high',\n            'price_low': 'low',\n            'price_close': 'close'\n        }\n        \n        for old_col, new_col in column_mappings.items():\n            if old_col in df.columns and new_col in df.columns:\n                df = df.drop(old_col, axis=1)\n            elif old_col in df.columns:\n                df = df.rename(columns={old_col: new_col})\n            \n        # 4. 불필요한 컬럼 제거\n        columns_to_keep = [\n            'datetime', 'symbol',\n            'open', 'high', 'low', 'close', 'volume',\n            'open_interest',\n            'funding_rate', 'predicted_funding_rate',\n            'liquidation',\n            'long_percentage', 'short_percentage', 'long_short_ratio'\n        ]\n        \n        available_columns = [col for col in columns_to_keep if col in df.columns]\n        \n        # 핵심 컬럼이 없어도 최소한의 메타데이터만이라도 저장\n        min_required_cols = ['datetime', 'symbol']\n        if all(col in available_columns for col in min_required_cols):\n            df = df[available_columns]\n            \n            # 5. 중복 제거 및 정렬\n            df = df.drop_duplicates(subset=['datetime'])\n            df = df.sort_values('datetime')\n            \n            # 6. 저장 (소수점 보존)\n            # 파일 손상 방지를 위한, 임시 파일에 저장 후 이동\n            temp_path = csv_path.with_name(f\"{symbol}_temp.csv\")\n            \n            # 소수점 반올림 없이 저장 (float_format 인자 사용하지 않음)\n            df.to_csv(temp_path, index=False)\n            \n            # 임시 파일이 성공적으로 작성되었으면 원본 파일로 이동\n            if temp_path.exists():\n                if csv_path.exists():\n                    csv_path.unlink()  # 기존 파일 삭제\n                temp_path.rename(csv_path)  # 임시 파일을 원본 파일로 이동\n            \n            # 핵심 데이터 누락 시 경고\n            if 'open' not in df.columns or df['open'].isnull().all():\n                logger.warning(f\"{symbol} 가격 데이터(OHLCV)가 모두 누락되었습니다. 메타데이터만 저장됨.\")\n            \n            logger.info(f\"{symbol} 데이터 저장 완료: 총 {len(df)}행 (새 데이터: {len(new_data)}행)\")\n            \n            # 7. 데이터 검증\n            time_diffs = pd.Series(df['datetime'].diff().dt.total_seconds() / 60)\n            gaps = time_diffs[time_diffs > 10]  # 10분 이상의 간격이 있는 경우\n            \n            if not gaps.empty:\n                logger.warning(f\"{symbol} 데이터 누락 발견:\")\n                for idx in gaps.index:\n                    gap_start = df['datetime'].iloc[idx-1]\n                    gap_end = df['datetime'].iloc[idx]\n                    gap_minutes = gaps[idx]\n                    logger.warning(f\"  {gap_start} ~ {gap_end} ({gap_minutes:.0f}분 간격)\")\n            \n            return True\n        else:\n            logger.error(f\"{symbol} 필수 컬럼({min_required_cols})이 없어 저장할 수 없습니다.\")\n            return False\n            \n    except Exception as e:\n        logger.error(f\"{symbol} 데이터 저장 실패: {str(e)}\")\n        return False\n\ndef check_data_gaps(symbol: str, interval: str = \"5min\", days: int = 30) -> List[Tuple[datetime, datetime]]:\n    \"\"\"데이터 누락 구간 확인\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 누락 검사가 중단되었습니다 (종료 중)\")\n        return []\n    \n    df = load_existing_data(symbol)\n    \n    if df.empty:\n        # 데이터가 없으면 전체 기간을 누락으로 간주\n        end_time = datetime.now()\n        start_time = end_time - timedelta(days=days)\n        return [(start_time, end_time)]\n    \n    # 데이터 정렬\n    df = df.sort_values('datetime')\n    \n    # 타임라인 예상 범위 생성 (5분 간격)\n    start_time = max(df['datetime'].min(), datetime.now() - timedelta(days=days))\n    end_time = datetime.now()\n    \n    expected_timeline = pd.date_range(start=start_time, end=end_time, freq=interval)\n    actual_timeline = df['datetime']\n    \n    # 누락된 시간대 찾기\n    missing_times = pd.DatetimeIndex(expected_timeline).difference(actual_timeline)\n    \n    if len(missing_times) == 0:\n        return []\n    \n    # 연속된 누락 구간 그룹화\n    gaps = []\n    if len(missing_times) > 0:\n        gap_start = missing_times[0]\n        for i in range(1, len(missing_times)):\n            curr_time = missing_times[i]\n            prev_time = missing_times[i-1]\n            # 다음 시간과 현재 시간의 차이가 간격보다 크면 새 그룹 시작\n            if (curr_time - prev_time).total_seconds() > pd.Timedelta(interval).total_seconds() * 2:\n                gaps.append((gap_start, prev_time))\n                gap_start = curr_time\n                \n        # 마지막 그룹 추가\n        gaps.append((gap_start, missing_times[-1]))\n    \n    return gaps\n\ndef fill_data_gaps(symbol: str, days: int = 30) -> bool:\n    \"\"\"데이터 누락 구간 채우기 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 누락 채우기가 중단되었습니다 (종료 중)\")\n        return False\n    \n    gaps = check_data_gaps(symbol, days=days)\n    \n    if not gaps:\n        logger.info(f\"{symbol}: 데이터 누락 없음\")\n        return True\n    \n    logger.info(f\"{symbol}: {len(gaps)}개의 데이터 누락 구간 발견\")\n    \n    data_types = [\"ohlcv\", \"oi\", \"funding\", \"predicted_funding\", \"liquidation\", \"long_short_ratio\"]\n    successful_fills = 0\n    \n    for start_time, end_time in gaps:\n        if shutdown_flag.is_set():\n            logger.warning(f\"{symbol} 데이터 누락 채우기가 중단되었습니다 (종료 중)\")\n            break\n            \n        logger.info(f\"{symbol} 누락 구간 채우기: {start_time} ~ {end_time}\")\n        \n        # 데이터 타입별로 수집\n        all_data_frames = {}\n        gap_filled = False\n        \n        # 각 데이터 타입별로 누락 구간 데이터 수집\n        for data_type in data_types:\n            if shutdown_flag.is_set():\n                break\n                \n            df = fetch_historical_data(symbol, data_type, start_time, end_time)\n            if not df.empty:\n                all_data_frames[data_type] = df\n                gap_filled = True\n        \n        # 수집된 데이터가 있으면 병합하여 저장\n        if all_data_frames and not shutdown_flag.is_set():\n            # 데이터프레임 병합 (소수점 보존)\n            merged_df = merge_dataframes(all_data_frames, symbol)\n            \n            if not merged_df.empty:\n                # 저장 (소수점 보존)\n                if save_data(symbol, merged_df):\n                    successful_fills += 1\n    \n    return successful_fills > 0\n\ndef process_symbol_data(symbol: str, interval: str = \"5min\", mode: str = \"realtime\") -> dict:\n    \"\"\"한 심볼의 데이터 처리 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        return {\n            \"symbol\": symbol,\n            \"mode\": mode,\n            \"success\": False,\n            \"message\": \"종료 중\"\n        }\n    \n    result = {\n        \"symbol\": symbol,\n        \"mode\": mode,\n        \"success\": False,\n        \"start_time\": None,\n        \"end_time\": None,\n        \"rows_added\": 0\n    }\n    \n    try:\n        if mode == \"backfill\":\n            # 과거 데이터 누락 검사 및 채우기\n            gaps_filled = fill_data_gaps(symbol)\n            result[\"success\"] = gaps_filled\n            return result\n            \n        # 실시간 데이터 수집\n        current_time = datetime.now()\n        \n        # 일반적으로 30분 전 데이터부터 수집 (완전한 캔들을 위해)\n        start_time = current_time - timedelta(minutes=30)\n        \n        # 각 데이터 타입별로 최신 데이터 수집\n        data_types = [\"ohlcv\", \"oi\", \"funding\", \"predicted_funding\", \"liquidation\", \"long_short_ratio\"]\n        all_data_frames = {}\n        \n        for data_type in data_types:\n            if shutdown_flag.is_set():\n                result[\"message\"] = \"종료 중\"\n                return result\n                \n            df = fetch_historical_data(symbol, data_type, start_time, current_time, interval)\n            if not df.empty:\n                all_data_frames[data_type] = df\n        \n        # 수집된 데이터가 있으면 병합하여 저장\n        if all_data_frames and not shutdown_flag.is_set():\n            # 데이터프레임 병합 (소수점 보존)\n            merged_df = merge_dataframes(all_data_frames, symbol)\n            \n            if not merged_df.empty:\n                # 저장 (소수점 보존)\n                save_success = save_data(symbol, merged_df)\n                \n                result.update({\n                    \"success\": save_success,\n                    \"start_time\": merged_df['datetime'].min().isoformat() if not merged_df.empty else None,\n                    \"end_time\": merged_df['datetime'].max().isoformat() if not merged_df.empty else None,\n                    \"rows_added\": len(merged_df)\n                })\n            \n        return result\n        \n    except Exception as e:\n        logger.error(f\"{symbol} 처리 중 오류: {str(e)}\")\n        result[\"error\"] = str(e)\n        return result\n\ndef worker(task_queue, result_queue):\n    \"\"\"작업자 스레드 함수\"\"\"\n    while not shutdown_flag.is_set():\n        try:\n            # 종료 확인\n            if check_stop_file():\n                break\n                \n            # 1초 타임아웃으로 큐에서 작업 가져오기\n            try:\n                task = task_queue.get(timeout=1)\n            except queue.Empty:\n                continue\n                \n            # 종료 신호 확인\n            if task is None:\n                task_queue.task_done()\n                break\n                \n            symbol, interval, mode = task\n            result = process_symbol_data(symbol, interval, mode)\n            result_queue.put(result)\n            task_queue.task_done()\n            \n        except Exception as e:\n            logger.error(f\"작업자 스레드 오류: {str(e)}\")\n            time.sleep(1)  # 오류 시 잠시 대기\n    \n    logger.info(\"작업자 스레드 종료\")\n\ndef initialize_system():\n    \"\"\"시스템 초기화 및 심볼 매핑 생성\"\"\"\n    # 디렉토리 생성\n    for directory in [DATA_DIR, LIVE_DIR, BACKUP_DIR, ARCHIVE_DIR, LONG_TERM_DIR]:\n        directory.mkdir(parents=True, exist_ok=True)\n    \n    # CCXT 매니저 초기화\n    if not ccxt_manager.load_symbol_mappings():\n        # 저장된 매핑이 없으면 새로 생성\n        # 모든 심볼 목록 가져오기\n        symbols = get_all_symbols()\n        if symbols:\n            # 심볼 매핑 탐색\n            ccxt_manager.discover_symbol_mappings(symbols)\n    \n    logger.info(f\"시스템 초기화 완료: {len(ccxt_manager.symbol_mappings)}개 심볼 매핑됨\")\n\ndef continuous_data_collection(interval: str = \"5min\", num_threads: int = 8):\n    \"\"\"연속적인 데이터 수집 (멀티스레딩, Coinalyze 우선, CCXT 보완)\"\"\"\n    # 시스템 초기화\n    initialize_system()\n    \n    # 심볼 목록 로드\n    symbols = get_all_symbols()\n    if not symbols:\n        logger.error(\"심볼 목록을 가져올 수 없습니다\")\n        return\n    \n    logger.info(f\"데이터 수집 시작 ({interval} 간격): {len(symbols)}개 심볼\")\n    \n    # 작업 큐와 결과 큐 설정\n    task_queue = queue.Queue()\n    result_queue = queue.Queue()\n    \n    # 작업자 스레드 시작\n    threads = []\n    for _ in range(num_threads):\n        t = threading.Thread(target=worker, args=(task_queue, result_queue))\n        t.daemon = True  # 데몬 스레드로 설정\n        t.start()\n        threads.append(t)\n    \n    last_backup_check = datetime.now()\n    collection_start_time = time.time()\n    \n    while not shutdown_flag.is_set():\n        try:\n            # 종료 확인\n            if check_stop_file():\n                break\n            \n            # 모든 심볼에 대한 작업 생성\n            for symbol in symbols:\n                if shutdown_flag.is_set():\n                    break\n                task_queue.put((symbol, interval, \"realtime\"))\n            \n            # 모든 작업이 완료될 때까지 대기 (최대 대기 시간 설정)\n            wait_start = time.time()\n            while not task_queue.empty():\n                if shutdown_flag.is_set() or time.time() - wait_start > 600:  # 10분 초과 대기 시 타임아웃\n                    break\n                time.sleep(1)\n                check_stop_file()  # 정기적으로 종료 파일 확인\n            \n            # 작업 결과 처리\n            results_processed = 0\n            success_count = 0\n            fail_count = 0\n            \n            while not result_queue.empty() and results_processed < 1000:  # 최대 1000개 결과만 처리\n                result = result_queue.get()\n                if result[\"success\"]:\n                    logger.info(f\"{result['symbol']} 데이터 수집 성공: {result.get('rows_added', 0)}행 추가됨\")\n                    success_count += 1\n                else:\n                    logger.warning(f\"{result['symbol']} 데이터 수집 실패\")\n                    fail_count += 1\n                results_processed += 1\n            \n            logger.info(f\"데이터 수집 결과: 성공 {success_count}개, 실패 {fail_count}개\")\n            \n            # 수집 완료 시간 계산\n            collection_duration = time.time() - collection_start_time\n            \n            # 종료 확인\n            if shutdown_flag.is_set():\n                break\n            \n            # 다음 수집까지 대기\n            remaining_wait = max(0, COLLECTION_INTERVAL - collection_duration)\n            if remaining_wait > 0:\n                logger.info(f\"다음 데이터 수집까지 {remaining_wait:.1f}초 대기...\")\n                \n                # 대기 중에도 정기적으로 종료 확인\n                wait_start = time.time()\n                while time.time() - wait_start < remaining_wait:\n                    if shutdown_flag.is_set() or check_stop_file():\n                        break\n                    time.sleep(1)\n            \n            # 새로운 주기 시작\n            collection_start_time = time.time()\n            \n        except KeyboardInterrupt:\n            logger.info(\"사용자에 의해 프로그램이 중단되었습니다.\")\n            shutdown_flag.set()\n            break\n        except Exception as e:\n            logger.error(f\"데이터 수집 중 오류 발생: {str(e)}\")\n            time.sleep(60)  # 오류 발생 시 1분 대기 후 재시도\n    \n    # 안전한 종료 처리\n    logger.info(\"프로그램 종료 중...\")\n    \n    # 작업자 스레드 종료\n    for _ in threads:\n        task_queue.put(None)  # 종료 신호\n    \n    # 각 스레드가 안전하게 종료될 때까지 최대 30초 대기\n    for t in threads:\n        t.join(timeout=30)\n    \n    logger.info(\"데이터 수집이 완전히 종료되었습니다.\")\n\ndef main():\n    \"\"\"메인 함수\"\"\"\n    logger.info(\"암호화폐 데이터 통합 관리 시스템 시작\")\n    \n    try:\n        # 기존 종료 파일 확인 및 제거\n        if os.path.exists(\"stop.signal\"):\n            os.remove(\"stop.signal\")\n            logger.info(\"기존 종료 파일 제거됨\")\n        \n        # 시스템 초기화\n        initialize_system()\n        \n        # 실시간 데이터 수집 (5분 간격)\n        if not shutdown_flag.is_set():\n            logger.info(\"실시간 데이터 수집 시작...\")\n            continuous_data_collection(interval=\"5min\", num_threads=8)\n        \n    except KeyboardInterrupt:\n        logger.info(\"사용자에 의해 프로그램이 중단되었습니다.\")\n        shutdown_flag.set()\n    except Exception as e:\n        logger.error(f\"프로그램 실행 중 오류 발생: {str(e)}\")\n        shutdown_flag.set()\n    finally:\n        # 종료 마무리\n        if shutdown_flag.is_set():\n            logger.info(\"프로그램 종료 처리 완료\")\n\ndef create_stop_signal():\n    \"\"\"프로그램 종료 신호 파일 생성\"\"\"\n    try:\n        with open(\"stop.signal\", \"w\") as f:\n            f.write(f\"Stop requested at {datetime.now().isoformat()}\")\n        print(\"종료 신호가 생성되었습니다. 프로그램이 안전하게 종료됩니다.\")\n    except Exception as e:\n        print(f\"종료 신호 생성 실패: {str(e)}\")\n\nif __name__ == \"__main__\":\n    import sys\n    \n    # 명령행 인수 처리\n    if len(sys.argv) > 1 and sys.argv[1] == \"stop\":\n        create_stop_signal()\n    else:\n        main()\n\n이코드가 1번에 들어갈코드야 적용시키자 ","commandType":4},{"text":"@crypto_data_manager.py 이코드 구동할 수 있게 모듈 체크","commandType":4},{"text":"@crypto_data_manager.log @crypto_data_manager.py 로그파일보고 수정할 내용을 제안만해봐","commandType":4},{"text":"변경하지말고 코드만 제안해봐","commandType":4},{"text":"제공해주신 피드백을 바탕으로 코드의 문제점을 분석하고 수정하겠습니다. 아래는 주요 문제점과 수정 방안입니다:\n\n### 1. save_data 함수 수정 (타임스탬프 비교 개선)\n\n```python\ndef save_data(symbol: str, new_data: pd.DataFrame) -> bool:\n    \"\"\"데이터 저장 (기존 데이터와 병합, 소수점 보존)\"\"\"\n    if new_data.empty:\n        logger.warning(f\"{symbol}: 저장할 새 데이터가 없습니다\")\n        return False\n    \n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 저장이 중단되었습니다 (종료 중)\")\n        return False\n        \n    LIVE_DIR.mkdir(parents=True, exist_ok=True)\n    csv_path = LIVE_DIR / f\"{symbol}.csv\"\n    \n    try:\n        # 1. 기존 데이터 로드\n        existing_data = load_existing_data(symbol)\n        \n        # 2. 새 데이터와 기존 데이터 병합\n        if not existing_data.empty:\n            # 기존 데이터의 타임스탬프를 set으로 변환하여 비교 (성능 개선)\n            existing_timestamps = set(existing_data['datetime'].dt.to_pydatetime())\n            # set을 사용한 필터링으로 변경\n            mask = [dt not in existing_timestamps for dt in new_data['datetime'].dt.to_pydatetime()]\n            new_data_filtered = new_data.loc[mask] if any(mask) else pd.DataFrame()\n            \n            if not new_data_filtered.empty:\n                # 인덱스 리셋 후 병합하여 인덱스 충돌 방지\n                df = pd.concat([existing_data.reset_index(drop=True), \n                                new_data_filtered.reset_index(drop=True)], \n                               ignore_index=True)\n            else:\n                logger.info(f\"{symbol}: 새 데이터가 이미 기존 데이터에 포함되어 있습니다\")\n                return True\n        else:\n            df = new_data.copy()\n            \n        # 3. 중복 컬럼 제거\n        column_mappings = {\n            'price_open': 'open',\n            'price_high': 'high',\n            'price_low': 'low',\n            'price_close': 'close'\n        }\n        \n        for old_col, new_col in column_mappings.items():\n            if old_col in df.columns and new_col in df.columns:\n                df = df.drop(old_col, axis=1)\n            elif old_col in df.columns:\n                df = df.rename(columns={old_col: new_col})\n            \n        # 4. 불필요한 컬럼 제거\n        columns_to_keep = [\n            'datetime', 'symbol',\n            'open', 'high', 'low', 'close', 'volume',\n            'open_interest',\n            'funding_rate', 'predicted_funding_rate',\n            'liquidation',\n            'long_percentage', 'short_percentage', 'long_short_ratio'\n        ]\n        \n        available_columns = [col for col in columns_to_keep if col in df.columns]\n        \n        # 핵심 컬럼이 없어도 최소한의 메타데이터만이라도 저장\n        min_required_cols = ['datetime', 'symbol']\n        if all(col in available_columns for col in min_required_cols):\n            df = df[available_columns]\n            \n            # 5. 중복 제거 및 정렬\n            df = df.drop_duplicates(subset=['datetime']).reset_index(drop=True)\n            df = df.sort_values('datetime')\n            \n            # 6. 저장 (소수점 보존)\n            # 파일 손상 방지를 위한, 임시 파일에 저장 후 이동\n            temp_path = csv_path.with_name(f\"{symbol}_temp.csv\")\n            \n            # 소수점 반올림 없이 저장 (float_format 인자 사용하지 않음)\n            df.to_csv(temp_path, index=False)\n            \n            # 임시 파일이 성공적으로 작성되었으면 원본 파일로 이동\n            if temp_path.exists():\n                if csv_path.exists():\n                    csv_path.unlink()  # 기존 파일 삭제\n                temp_path.rename(csv_path)  # 임시 파일을 원본 파일로 이동\n            \n            # 핵심 데이터 누락 시 경고 (Series 직접 평가 수정)\n            has_open = 'open' in df.columns\n            missing_open_data = has_open and df['open'].isnull().all()\n            if not has_open or missing_open_data:\n                logger.warning(f\"{symbol} 가격 데이터(OHLCV)가 모두 누락되었습니다. 메타데이터만 저장됨.\")\n            \n            logger.info(f\"{symbol} 데이터 저장 완료: 총 {len(df)}행 (새 데이터: {len(new_data)}행)\")\n            \n            # 7. 데이터 검증\n            if len(df) > 1:  # 최소 2개 이상의 행이 있어야 diff 계산 가능\n                time_diffs = pd.Series(df['datetime'].diff().dt.total_seconds() / 60)\n                gaps = time_diffs[time_diffs > 10]  # 10분 이상의 간격이 있는 경우\n                \n                if not gaps.empty:\n                    logger.warning(f\"{symbol} 데이터 누락 발견:\")\n                    for idx in gaps.index:\n                        gap_start = df['datetime'].iloc[idx-1]\n                        gap_end = df['datetime'].iloc[idx]\n                        gap_minutes = gaps[idx]\n                        logger.warning(f\"  {gap_start} ~ {gap_end} ({gap_minutes:.0f}분 간격)\")\n            \n            return True\n        else:\n            logger.error(f\"{symbol} 필수 컬럼({min_required_cols})이 없어 저장할 수 없습니다.\")\n            return False\n            \n    except Exception as e:\n        logger.error(f\"{symbol} 데이터 저장 실패: {str(e)}\")\n        traceback.print_exc()  # 상세 오류 출력\n        return False\n```\n\n### 2. merge_dataframes 함수 수정 (데이터프레임 병합 방식 개선)\n\n```python\ndef merge_dataframes(all_data_frames: Dict[str, pd.DataFrame], symbol: str) -> pd.DataFrame:\n    \"\"\"여러 데이터프레임 병합 (소수점 보존)\"\"\"\n    if not all_data_frames:\n        return pd.DataFrame()\n    \n    # 가능한 모든 기준 프레임 후보 (우선순위 순)\n    base_frame_candidates = [\"ohlcv\", \"long_short_ratio\", \"funding\", \"oi\"]\n    base_df = None\n    \n    # 우선순위에 따라 기준 프레임 선택\n    for candidate in base_frame_candidates:\n        if candidate in all_data_frames and not all_data_frames[candidate].empty:\n            base_df = all_data_frames[candidate].copy()\n            logger.info(f\"{symbol} 기준 프레임으로 {candidate} 선택 ({len(base_df)}행)\")\n            break\n    \n    # 기준 프레임이 없으면 빈 DataFrame 반환\n    if base_df is None:\n        logger.warning(f\"{symbol} 병합할 기준 프레임이 없습니다.\")\n        return pd.DataFrame()\n    \n    # 심볼 정보 확인 및 추가\n    if 'symbol' not in base_df.columns:\n        base_df['symbol'] = symbol\n        \n    # 기본 프레임 컬럼 이름 확인 (중복 방지 위해)\n    # 문제가 되는 t, o, h, l, c 등 짧은 컬럼명을 미리 표준화\n    if 't' in base_df.columns:\n        base_df.rename(columns={'t': 'timestamp'}, inplace=True)\n    for col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n        if col_old in base_df.columns:\n            base_df.rename(columns={col_old: col_new}, inplace=True)\n            \n    # 인덱스 리셋 (병합을 위한 준비)\n    base_df = base_df.reset_index(drop=True)\n    \n    # 표준 컬럼명 리스트\n    std_cols = {\n        'open', 'high', 'low', 'close', 'volume',\n        'open_interest', 'funding_rate', 'predicted_funding_rate',\n        'liquidation', 'long_percentage', 'short_percentage', 'long_short_ratio',\n        'datetime', 'symbol'\n    }\n    \n    # 각 데이터프레임을 병합 (왼쪽 조인 방식으로 변경)\n    for data_type, df in all_data_frames.items():\n        # 기준 프레임 또는 빈 프레임 건너뛰기\n        is_base_frame = data_type == base_frame_candidates[0]\n        if is_base_frame or df.empty:\n            continue\n        \n        # 표준화된 컬럼명으로 변환\n        if 't' in df.columns:\n            df.rename(columns={'t': 'timestamp'}, inplace=True)\n        for col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n            if col_old in df.columns:\n                df.rename(columns={col_old: col_new}, inplace=True)\n        \n        # 기준 프레임에 없는 컬럼만 추출\n        columns_to_merge = []\n        for col in df.columns:\n            # 표준 컬럼 외의 컬럼은 고유한 이름으로 변경\n            if col not in std_cols and col in base_df.columns:\n                new_col = f\"{col}_{data_type}\"\n                df.rename(columns={col: new_col}, inplace=True)\n                columns_to_merge.append(new_col)\n            elif col not in base_df.columns and col != 'datetime' and col != 'symbol':\n                columns_to_merge.append(col)\n        \n        # 병합할 컬럼이 있고 datetime 열이 있는 경우에만 병합 진행\n        if len(columns_to_merge) > 0 and 'datetime' in df.columns:\n            # 인덱스 리셋 (병합을 위한 준비)\n            df_to_merge = df.reset_index(drop=True)\n            merge_df = df_to_merge[['datetime'] + columns_to_merge].copy()\n            \n            # 병합 (왼쪽 조인 방식으로 변경)\n            try:\n                base_df = pd.merge(base_df, merge_df, on='datetime', how='left', suffixes=('', f'_{data_type}'))\n            except Exception as e:\n                logger.error(f\"데이터프레임 병합 오류 ({data_type}): {str(e)}\")\n                # 병합 실패해도 계속 진행\n    \n    # 최종 정리\n    try:\n        base_df = base_df.sort_values('datetime').reset_index(drop=True)\n    except Exception as e:\n        logger.error(f\"데이터프레임 정렬 오류: {str(e)}\")\n    \n    return base_df\n```\n\n### 3. fetch_historical_data 함수 수정 (오류 처리 개선)\n\n```python\ndef fetch_historical_data(symbol: str, data_type: str, start_time: datetime, end_time: datetime, interval: str = \"5min\") -> pd.DataFrame:\n    \"\"\"특정 심볼의 역사적 데이터 수집 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} {data_type} 수집이 중단되었습니다 (종료 중)\")\n        return pd.DataFrame()\n    \n    endpoint_map = {\n        \"ohlcv\": \"ohlcv_history\",\n        \"oi\": \"open_interest_history\",\n        \"funding\": \"funding_rate_history\",\n        \"predicted_funding\": \"predicted_funding_rate_history\",\n        \"liquidation\": \"liquidation_history\",\n        \"long_short_ratio\": \"long_short_ratio_history\"\n    }\n    \n    if data_type not in endpoint_map:\n        logger.error(f\"지원되지 않는 데이터 타입: {data_type}\")\n        return pd.DataFrame()\n    \n    try:\n        # 1. Coinalyze API 요청\n        params = {\n            \"symbols\": f\"{symbol}USDT_PERP.A\",\n            \"interval\": interval,\n            \"from\": int(start_time.timestamp()),\n            \"to\": int(end_time.timestamp())\n        }\n        \n        tier = \"tier\" + str(random.randint(1, 4))  # 티어 무작위 선택\n        data = make_request(ENDPOINTS[endpoint_map[data_type]], params, tier)\n        \n        # API 응답 처리\n        if data is not None:\n            # 데이터 추출\n            history = extract_data_from_response(data, data_type)\n            \n            # DataFrame 생성 및 처리\n            try:\n                if not history:\n                    # 데이터 없음\n                    logger.warning(f\"{symbol} {data_type}: 데이터가 없습니다\")\n                else:\n                    df = pd.DataFrame(history)\n                    \n                    # 컬럼 정규화 (소수점 보존)\n                    if not df.empty:\n                        df = normalize_columns(df, data_type)\n                        \n                        # 심볼 정보 추가\n                        if 'symbol' not in df.columns:\n                            df['symbol'] = symbol\n                        \n                        logger.info(f\"Coinalyze {data_type} 데이터 수집 성공: {symbol}, {len(df)}행\")\n                        return df\n                        \n            except Exception as e:\n                logger.error(f\"{symbol} DataFrame 처리 중 오류: {str(e)}\")\n        \n        # 2. Coinalyze API 실패 시 CCXT 시도 (OHLCV 및 펀딩비만 지원)\n        if data_type == \"ohlcv\":\n            logger.warning(f\"{symbol} Coinalyze OHLCV 데이터 수집 실패, CCXT로 시도...\")\n            df = ccxt_manager.fetch_ohlcv(symbol, interval, start_time, 1000)\n            if not df.empty:\n                return df\n        \n        elif data_type == \"funding\":\n            logger.warning(f\"{symbol} Coinalyze 펀딩비 데이터 수집 실패, CCXT로 시도...\")\n            df = ccxt_manager.fetch_funding_rate(symbol, interval, start_time, 500)\n            if not df.empty:\n                return df\n    \n    except Exception as e:\n        logger.error(f\"{symbol} {data_type} 데이터 수집 중 오류 발생: {str(e)}\")\n    \n    # 데이터를 가져오지 못한 경우\n    logger.warning(f\"{symbol} {data_type} 데이터 수집 실패 (Coinalyze 및 CCXT)\")\n    return pd.DataFrame()\n```\n\n### 4. 기타 디버깅을 위한 추가 사항\n\n```python\n# import 섹션에 추가\nimport traceback  # 상세 오류 출력을 위해 추가\n\n# 모듈 시작 부분에 디버깅 레벨 설정 추가\n# 로깅 설정 (디버깅 레벨 변경)\nlogging.basicConfig(\n    level=logging.DEBUG,  # INFO에서 DEBUG로 변경하여 더 자세한 로그 확인\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler(\"crypto_data_manager.log\", encoding='utf-8')\n    ]\n)\n```\n\n이러한 수정사항들은 다음 문제들을 해결하기 위한 것입니다:\n\n1. 데이터프레임 병합 시 인덱스 충돌 문제\n2. Series 객체의 부울 평가 오류\n3. 데이터 저장 시 타임스탬프 비교 방식 개선\n4. 오류 발생 시 더 자세한 정보 제공\n\n이 수정사항들을 적용하면 이전에 발생했던 오류들이 해결될 것으로 예상됩니다.\n\n이런 의견도 있는데 너의 의견은?","commandType":4},{"text":"@crypto_data_manager.py 변경할거 변경해봐","commandType":4},{"text":"테스트 해볼까?","commandType":4},{"text":"여전히 문제가 생겼다 체크좀 해줄래?","commandType":4},{"text":"제대로 적용안되었는데? ","commandType":4},{"text":"적용해봐","commandType":4},{"text":"구동해봐","commandType":4},{"text":"@crypto_data_manager.log 로그체크하라","commandType":4},{"text":"질문이 있어 과거데이터는 못가져와?\ncoinalyze에서 7일치는 가져올 수 있었는데?","commandType":4},{"text":"과거데이터는 언제가져와?","commandType":4},{"text":"ccxt랑 coinalyze를 통해 7일전 데이터까지 불러 올 수 있는걸로 알 고 있어. 가능한가?","commandType":4},{"text":"수정해봐","commandType":4},{"text":"이 프로그램은 7일전 데이터만 보관하는게 아니야.\n7일전 데이터를 조회해서 누락된걸 채워야해.\n이 데이터는 지속적으로 보관해야해.\n데이터 누적치를 쌓아서 백테스트 데이터를 늘릴용도야.\n\n이해를 했는지 체크","commandType":4},{"text":"최근 7일치 데이터 누락이 있는지 신규 데이터를 가져오는지 7일치만 보관하면 안되고 시간대로 지속적으로 보관해야한다.< 이 코드가 그것을 구성하는것인가?","commandType":4},{"text":"자 그럼 시간대별로 정리해서 csv 파일로 저장\n7일데이터 체크 누락있으면 채우고\n신규데이터 채우고\n다음 넘어가는식으로 들어가는게 좋을것같은데 너의 의견은?","commandType":4},{"text":"그렇다고 기존데이터를 삭제하면 되? 안되?","commandType":4},{"text":"api키 16개로 심볼 전체를 돌리면 얼마나 걸릴까?","commandType":4},{"text":"안되 보통 1~5분안에 데이터를 가져와야하고 \n1분봉기준으로 땡겨와야할것같은데? 너의의견은?","commandType":4},{"text":"그럼 설명하고 코드변경해볼까?","commandType":4},{"text":"구동해봐도 될까?","commandType":4},{"text":"데이터 못가져오는 이유는?","commandType":4},{"text":"그럼 돌려놔볼까 10시쯤 데이터 분석해보면 제대로 가져왔는지 볼 수 있겠네?","commandType":4},{"text":"@BTC.csv 데이터 분석해봐","commandType":4},{"text":"기존데이터 한번날리고 1분봉기준으로만 만들까? 어떻게 생각해?","commandType":4},{"text":"아니면 1분봉 데이터를 가지고 5분봉으로 15분봉 30분 등등 시트를 만들 수 도 있는거아냐? 질문이야.","commandType":4},{"text":"그래 그럼 1분봉 기준으로만 하자 live 데이터 싹 지우고 ","commandType":4},{"text":"새로 돌릴게","commandType":4},{"text":"@crypto_pattern_analyzer.py 이거 이제 손봐야겠지?","commandType":4},{"text":"분석해서 live 폴더에 각 심볼로 json 파일 생성해볼까? 그리고 json 파일 자체를 검사해보자","commandType":4},{"text":"질문이 있어 왜 웹인터페이스가 있어야해?","commandType":4},{"text":"그냥 니가json 파일 가져오고 csv 파일 2개를 넣고 비교해보면되잖아?","commandType":4},{"text":"json 파일 분석결과 줘봐 알람화 할만해?","commandType":4},{"text":"그럼 단순히 json 내용만 나한테 전달해봐","commandType":4},{"text":"저 알람이 승율이나 100기준에 가능성 점수는?","commandType":4},{"text":"3개를 합쳐서 만들어야하는거아냐?","commandType":4},{"text":"이거를 종합적으로 해서 좋은신호를 만들어야 하는거아냐???????????","commandType":4},{"text":"신호가 발견안된애들은 변곡점이 없던거야?","commandType":4},{"text":"최소 1개를 가져가겐 하고싶은데?","commandType":4},{"text":"최소 1개는 만들고 \n다수가 만들어진건 점수화시켜서 공유하게 하자","commandType":4},{"text":"각 심볼별로 json파일 만들고 csv로 검증절차로 json파일에 점수화? ","commandType":4},{"text":"각 심볼별로 json파일 만들고 csv로 검증절차로 json파일에 점수화하는게 좋지 않을까?","commandType":4},{"text":"아까 로그파일 만들었잖아?? 이번엔 왜 안만들어?","commandType":4},{"text":"이내용에 대해서 분석하고 내용을 설명해줄래?","commandType":4},{"text":"기존에 내가 검증하던 코드를 줄게 그리고 이거 만들면서 파일을 많이 만들었는데 필요없는건 삭제해.","commandType":4},{"text":"import logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import List, Dict\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\nfrom src.analyzer.symbol_analyzer import SymbolAnalyzer\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(name)\n\nclass AnalysisWorker:\ndef init(self, max_workers: int = 4):\nself.max_workers = max_workers\nself.results = {}\n\npython\n\n실행\n\n복사\ndef get_symbols(self) -> List[str]:\n    \"\"\"분석할 심볼 목록 가져오기\"\"\"\n    try:\n        live_dir = Path(\"data/live\")\n        return [f.stem for f in live_dir.glob(\"*.csv\")]\n    except Exception as e:\n        logger.error(f\"심볼 목록 로드 중 오류: {str(e)}\")\n        return []\n        \ndef analyze_symbol(self, symbol: str) -> Dict:\n    \"\"\"단일 심볼 분석\"\"\"\n    try:\n        analyzer = SymbolAnalyzer(symbol)\n        results = analyzer.analyze()\n        logger.info(f\"{symbol} 분석 완료\")\n        return {symbol: results}\n    except Exception as e:\n        logger.error(f\"{symbol} 분석 중 오류: {str(e)}\")\n        return {symbol: {}}\n        \ndef run(self):\n    \"\"\"병렬 분석 실행\"\"\"\n    try:\n        # 1. 심볼 목록 가져오기\n        symbols = self.get_symbols()\n        if not symbols:\n            logger.error(\"분석할 심볼이 없습니다.\")\n            return\n            \n        logger.info(f\"총 {len(symbols)}개 심볼 분석 시작\")\n        start_time = datetime.now()\n        \n        # 2. 병렬 처리\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            future_to_symbol = {\n                executor.submit(self.analyze_symbol, symbol): symbol \n                for symbol in symbols\n            }\n            \n            # 결과 수집\n            for future in as_completed(future_to_symbol):\n                symbol = future_to_symbol[future]\n                try:\n                    result = future.result()\n                    self.results.update(result)\n                except Exception as e:\n                    logger.error(f\"{symbol} 처리 중 오류: {str(e)}\")\n        \n        # 실행 시간 계산\n        duration = datetime.now() - start_time\n        logger.info(f\"전체 분석 완료: {len(symbols)}개 심볼, 소요시간: {duration}\")\n        \n    except Exception as e:\n        logger.error(f\"분석 실행 중 오류: {str(e)}\")\n        \ndef main():\n\"\"\"메인 실행 함수\"\"\"\nworker = AnalysisWorker(max_workers=4)\nworker.run()\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport logging\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(name)\n\nclass SymbolAnalyzer:\ndef init(self, symbol: str):\nself.symbol = symbol\nself.data_dir = Path(\"data/live\")\nself.data_dir.mkdir(parents=True, exist_ok=True)\nself.config = self.load_config()\nself.df = None\n\npython\n\n실행\n\n복사\ndef load_config(self) -> dict:\n    \"\"\"심볼별 설정 로드\"\"\"\n    config_path = self.data_dir / f\"{self.symbol}_config.json\"\n    \n    # 기본 설정 로드\n    with open(\"data/config/default.json\", 'r', encoding='utf-8') as f:\n        config = json.load(f)\n        \n    # 심볼별 설정이 있으면 오버라이드\n    if config_path.exists():\n        with open(config_path, 'r', encoding='utf-8') as f:\n            symbol_config = json.load(f)\n            config.update(symbol_config)\n            \n    return config\n    \ndef load_data(self) -> bool:\n    \"\"\"데이터 로드\"\"\"\n    try:\n        data_path = self.data_dir / f\"{self.symbol}.csv\"\n        if not data_path.exists():\n            logger.error(f\"{self.symbol} 데이터 파일이 없습니다\")\n            return False\n            \n        # 데이터 로드\n        df = pd.read_csv(data_path)\n        \n        # datetime 처리\n        df['datetime'] = pd.to_datetime(df['datetime'])\n        \n        # 컬럼 타입 변환\n        numeric_columns = [\n            'open', 'high', 'low', 'close', 'volume',\n            'open_interest', 'funding_rate', 'predicted_funding_rate',\n            'liquidation', 'long_short_ratio'\n        ]\n        \n        for col in numeric_columns:\n            try:\n                if col in df.columns:\n                    # 문자열 전처리\n                    if df[col].dtype == object:\n                        df[col] = df[col].astype(str).str.extract('(\\d+\\.?\\d*)', expand=False)\n                    # 숫자로 변환\n                    df[col] = pd.to_numeric(df[col], errors='coerce')\n            except Exception as e:\n                logger.warning(f\"{self.symbol} {col} 컬럼 변환 중 오류: {str(e)}\")\n                \n        # 결측치 처리\n        df = df.fillna(method='ffill')\n        \n        # 데이터 정렬\n        df = df.sort_values('datetime')\n        \n        self.df = df\n        logger.info(f\"{self.symbol} 데이터 로드 완료: {len(df)}행\")\n        return True\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 데이터 로드 중 오류: {str(e)}\")\n        return False\n        \ndef prepare_timeframes(self) -> Dict[str, pd.DataFrame]:\n    \"\"\"멀티 타임프레임 데이터 준비\"\"\"\n    if self.df is None:\n        return {}\n        \n    timeframes = {\n        '5M': 1,    # 5분 = 기본 데이터\n        '30M': 6,   # 30분 = 6캔들\n        '1H': 12,   # 1시간 = 12캔들\n        '4H': 48    # 4시간 = 48캔들\n    }\n    \n    tf_data = {}\n    \n    for tf, period in timeframes.items():\n        # 리샘플링\n        resampled = self.df.resample(f'{period*5}min', on='datetime', label='right').agg({\n            'open': 'first',\n            'high': 'max',\n            'low': 'min',\n            'close': 'last',\n            'volume': 'sum',\n            'open_interest': 'last',\n            'funding_rate': 'last',\n            'predicted_funding_rate': 'last',\n            'liquidation': 'sum',\n            'long_short_ratio': 'last'\n        })\n        \n        # 지표 계산\n        df_tf = resampled.copy()\n        \n        # OI 관련 지표\n        df_tf['oi_change'] = df_tf['open_interest'].pct_change(fill_method=None)\n        df_tf['oi_ma'] = df_tf['open_interest'].rolling(window=20, min_periods=1).mean()\n        df_tf['oi_std'] = df_tf['open_interest'].rolling(window=20, min_periods=1).std()\n        \n        # 펀딩비 관련 지표\n        df_tf['funding_sign_change'] = np.sign(df_tf['funding_rate']) != np.sign(df_tf['funding_rate'].shift(1))\n        df_tf['funding_prediction_gap'] = df_tf['funding_rate'] - df_tf['predicted_funding_rate']\n        df_tf['funding_ma'] = df_tf['funding_rate'].rolling(window=20, min_periods=1).mean()\n        df_tf['funding_std'] = df_tf['funding_rate'].rolling(window=20, min_periods=1).std()\n        \n        # 거래량 관련 지표\n        df_tf['volume_ma'] = df_tf['volume'].rolling(window=20, min_periods=1).mean()\n        df_tf['volume_ratio'] = df_tf['volume'] / df_tf['volume_ma']\n        df_tf['volume_std'] = df_tf['volume'].rolling(window=20, min_periods=1).std()\n        \n        # 가격 변동성\n        df_tf['returns'] = df_tf['close'].pct_change(fill_method=None)\n        df_tf['volatility'] = df_tf['returns'].rolling(window=20, min_periods=1).std()\n        df_tf['volatility_ma'] = df_tf['volatility'].rolling(window=20, min_periods=1).mean()\n        \n        # 청산/롱숏 관련\n        df_tf['liquidation_ma'] = df_tf['liquidation'].rolling(window=20, min_periods=1).mean()\n        df_tf['liquidation_ratio'] = df_tf['liquidation'] / df_tf['liquidation_ma']\n        df_tf['ls_extreme'] = (df_tf['long_short_ratio'] > 1.25) | (df_tf['long_short_ratio'] < 0.75)\n        \n        # 추세 강도\n        df_tf['atr'] = self.calculate_atr(df_tf)\n        df_tf['trend_strength'] = abs(df_tf['close'] - df_tf['close'].shift(10)) / (df_tf['atr'] * 10)\n        \n        # NaN 값 처리\n        df_tf.fillna(0, inplace=True)\n        \n        tf_data[tf] = df_tf\n        \n    return tf_data\n    \ndef calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:\n    \"\"\"ATR(Average True Range) 계산\"\"\"\n    high = df['high']\n    low = df['low']\n    close = df['close']\n    \n    tr1 = high - low\n    tr2 = abs(high - close.shift())\n    tr3 = abs(low - close.shift())\n    \n    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n    return tr.rolling(window=period).mean()\n    \ndef analyze(self) -> Dict:\n    \"\"\"심볼 분석 실행\"\"\"\n    try:\n        # 1. 데이터 로드\n        if not self.load_data():\n            return {}\n            \n        # 2. 멀티 타임프레임 데이터 준비\n        tf_data = self.prepare_timeframes()\n        if not tf_data:\n            return {}\n            \n        # 3. 각 타임프레임별 분석\n        results = {}\n        for tf, df in tf_data.items():\n            tf_result = self.analyze_timeframe(tf, df)\n            if tf_result:\n                results[tf] = tf_result\n                \n        # 4. 결과 저장\n        self.save_results(results)\n        \n        return results\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 분석 중 오류: {str(e)}\")\n        return {}\n        \ndef analyze_timeframe(self, timeframe: str, df: pd.DataFrame) -> Dict:\n    \"\"\"단일 타임프레임 분석\"\"\"\n    try:\n        result = {\n            'timestamp': df.index[-1].isoformat(),\n            'patterns': []\n        }\n        \n        # 최소 20개의 데이터 필요\n        if len(df) < 20:\n            return result\n            \n        # 현재 데이터\n        current = df.iloc[-1]\n        \n        # 1. OI 기반 패턴\n        oi_pattern = self.detect_oi_pattern(df)\n        if oi_pattern:\n            result['patterns'].append(oi_pattern)\n            \n        # 2. 펀딩비 기반 패턴\n        funding_pattern = self.detect_funding_pattern(df)\n        if funding_pattern:\n            result['patterns'].append(funding_pattern)\n            \n        # 3. 복합 패턴\n        complex_pattern = self.detect_complex_pattern(df)\n        if complex_pattern:\n            result['patterns'].append(complex_pattern)\n            \n        return result\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} {timeframe} 분석 중 오류: {str(e)}\")\n        return {}\n        \ndef detect_oi_pattern(self, df: pd.DataFrame) -> Optional[Dict]:\n    \"\"\"OI 기반 패턴 감지\"\"\"\n    try:\n        current = df.iloc[-1]\n        prev = df.iloc[-2]\n        \n        # OI 급증 후 하락\n        oi_surge_drop = (\n            df['oi_change'].iloc[-4:-1].mean() > 0.07 and \n            current['oi_change'] < -0.01\n        )\n        \n        # OI 급락 후 반등\n        oi_drop_bounce = (\n            df['oi_change'].iloc[-4:-1].mean() < -0.10 and\n            current['oi_change'] > 0.01\n        )\n        \n        # OI 횡보 후 급변\n        lookback = 20\n        oi_breakout = (\n            abs(df['oi_change'].iloc[-lookback:-1].max()) <= 0.03 and\n            abs(current['oi_change']) > 0.05\n        )\n        \n        if any([oi_surge_drop, oi_drop_bounce, oi_breakout]):\n            pattern_type = (\"OI_SURGE_DROP\" if oi_surge_drop else\n                          \"OI_DROP_BOUNCE\" if oi_drop_bounce else\n                          \"OI_BREAKOUT\")\n                          \n            return {\n                \"type\": pattern_type,\n                \"confidence\": self.calculate_confidence(current),\n                \"metrics\": {\n                    \"oi_change\": current['oi_change'],\n                    \"volume_ratio\": current['volume_ratio'],\n                    \"trend_strength\": current['trend_strength']\n                }\n            }\n            \n        return None\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} OI 패턴 감지 중 오류: {str(e)}\")\n        return None\n        \ndef detect_funding_pattern(self, df: pd.DataFrame) -> Optional[Dict]:\n    \"\"\"펀딩비 기반 패턴 감지\"\"\"\n    try:\n        current = df.iloc[-1]\n        \n        # 펀딩비 방향 전환\n        funding_flip = current['funding_sign_change']\n        \n        # 극단적 펀딩비\n        funding_extreme = abs(current['funding_rate']) > 0.001\n        \n        # 예측값과 실제값 차이\n        funding_divergence = abs(current['funding_prediction_gap']) > 0.0005\n        \n        if any([funding_flip, funding_extreme, funding_divergence]):\n            pattern_type = (\"FUNDING_FLIP\" if funding_flip else\n                          \"FUNDING_EXTREME\" if funding_extreme else\n                          \"FUNDING_DIVERGENCE\")\n                          \n            return {\n                \"type\": pattern_type,\n                \"confidence\": self.calculate_confidence(current),\n                \"metrics\": {\n                    \"funding_rate\": current['funding_rate'],\n                    \"funding_prediction_gap\": current['funding_prediction_gap'],\n                    \"volume_ratio\": current['volume_ratio']\n                }\n            }\n            \n        return None\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 펀딩비 패턴 감지 중 오류: {str(e)}\")\n        return None\n        \ndef detect_complex_pattern(self, df: pd.DataFrame) -> Optional[Dict]:\n    \"\"\"복합 패턴 감지\"\"\"\n    try:\n        current = df.iloc[-1]\n        \n        # OI와 펀딩비 다이버전스\n        oi_funding_divergence = (\n            (current['oi_change'] > 0.08 and current['funding_rate'] < -0.0005) or\n            (current['oi_change'] < -0.08 and current['funding_rate'] > 0.0005)\n        )\n        \n        # 거래량 급증 + 롱숏 극단\n        volume_ls_extreme = (\n            current['volume_ratio'] > 5.0 and\n            abs(current['long_short_ratio'] - 1) > 0.25\n        )\n        \n        # 대량 청산\n        liquidation_surge = current['liquidation_ratio'] > 3.0\n        \n        if any([oi_funding_divergence, volume_ls_extreme, liquidation_surge]):\n            pattern_type = (\"OI_FUNDING_DIVERGENCE\" if oi_funding_divergence else\n                          \"VOLUME_LS_EXTREME\" if volume_ls_extreme else\n                          \"LIQUIDATION_SURGE\")\n                          \n            return {\n                \"type\": pattern_type,\n                \"confidence\": self.calculate_confidence(current),\n                \"metrics\": {\n                    \"oi_change\": current['oi_change'],\n                    \"funding_rate\": current['funding_rate'],\n                    \"volume_ratio\": current['volume_ratio'],\n                    \"long_short_ratio\": current['long_short_ratio'],\n                    \"liquidation_ratio\": current['liquidation_ratio']\n                }\n            }\n            \n        return None\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 복합 패턴 감지 중 오류: {str(e)}\")\n        return None\n        \ndef calculate_confidence(self, current: pd.Series) -> float:\n    \"\"\"신뢰도 점수 계산\"\"\"\n    confidence = 60  # 기본 점수\n    \n    # 보조 지표 가점\n    if current['volume_ratio'] > 3.0: confidence += 10\n    if current['ls_extreme']: confidence += 10\n    if current['liquidation_ratio'] > 2.0: confidence += 10\n    if current['trend_strength'] > 1.0: confidence += 5\n    \n    return min(100, confidence)\n    \ndef save_results(self, results: Dict):\n    \"\"\"분석 결과 저장\"\"\"\n    try:\n        # 결과 파일 저장\n        output_file = self.data_dir / f\"{self.symbol}.json\"\n        with open(output_file, 'w') as f:\n            json.dump({\n                \"timestamp\": datetime.now().isoformat(),\n                \"symbol\": self.symbol,\n                \"results\": results\n            }, f, indent=2)\n            \n        logger.info(f\"{self.symbol} 분석 결과 저장 완료\")\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 결과 저장 중 오류: {str(e)}\") \n\n기존에 이 2가지 코드로 json값을 분석했다. 이내용을 파악하고 지금이랑 차이점을 판단하라.\n","commandType":4},{"text":"내가준 코드 기준으로 짜줄래?","commandType":4},{"text":"import logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import List, Dict\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\nfrom src.analyzer.symbol_analyzer import SymbolAnalyzer\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(name)\n\nclass AnalysisWorker:\ndef init(self, max_workers: int = 4):\nself.max_workers = max_workers\nself.results = {}\n\npython\n\n실행\n\n복사\ndef get_symbols(self) -> List[str]:\n    \"\"\"분석할 심볼 목록 가져오기\"\"\"\n    try:\n        live_dir = Path(\"data/live\")\n        return [f.stem for f in live_dir.glob(\"*.csv\")]\n    except Exception as e:\n        logger.error(f\"심볼 목록 로드 중 오류: {str(e)}\")\n        return []\n        \ndef analyze_symbol(self, symbol: str) -> Dict:\n    \"\"\"단일 심볼 분석\"\"\"\n    try:\n        analyzer = SymbolAnalyzer(symbol)\n        results = analyzer.analyze()\n        logger.info(f\"{symbol} 분석 완료\")\n        return {symbol: results}\n    except Exception as e:\n        logger.error(f\"{symbol} 분석 중 오류: {str(e)}\")\n        return {symbol: {}}\n        \ndef run(self):\n    \"\"\"병렬 분석 실행\"\"\"\n    try:\n        # 1. 심볼 목록 가져오기\n        symbols = self.get_symbols()\n        if not symbols:\n            logger.error(\"분석할 심볼이 없습니다.\")\n            return\n            \n        logger.info(f\"총 {len(symbols)}개 심볼 분석 시작\")\n        start_time = datetime.now()\n        \n        # 2. 병렬 처리\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            future_to_symbol = {\n                executor.submit(self.analyze_symbol, symbol): symbol \n                for symbol in symbols\n            }\n            \n            # 결과 수집\n            for future in as_completed(future_to_symbol):\n                symbol = future_to_symbol[future]\n                try:\n                    result = future.result()\n                    self.results.update(result)\n                except Exception as e:\n                    logger.error(f\"{symbol} 처리 중 오류: {str(e)}\")\n        \n        # 실행 시간 계산\n        duration = datetime.now() - start_time\n        logger.info(f\"전체 분석 완료: {len(symbols)}개 심볼, 소요시간: {duration}\")\n        \n    except Exception as e:\n        logger.error(f\"분석 실행 중 오류: {str(e)}\")\n        \ndef main():\n\"\"\"메인 실행 함수\"\"\"\nworker = AnalysisWorker(max_workers=4)\nworker.run()\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport logging\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\n\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(name)\n\nclass SymbolAnalyzer:\ndef init(self, symbol: str):\nself.symbol = symbol\nself.data_dir = Path(\"data/live\")\nself.data_dir.mkdir(parents=True, exist_ok=True)\nself.config = self.load_config()\nself.df = None\n\npython\n\n실행\n\n복사\ndef load_config(self) -> dict:\n    \"\"\"심볼별 설정 로드\"\"\"\n    config_path = self.data_dir / f\"{self.symbol}_config.json\"\n    \n    # 기본 설정 로드\n    with open(\"data/config/default.json\", 'r', encoding='utf-8') as f:\n        config = json.load(f)\n        \n    # 심볼별 설정이 있으면 오버라이드\n    if config_path.exists():\n        with open(config_path, 'r', encoding='utf-8') as f:\n            symbol_config = json.load(f)\n            config.update(symbol_config)\n            \n    return config\n    \ndef load_data(self) -> bool:\n    \"\"\"데이터 로드\"\"\"\n    try:\n        data_path = self.data_dir / f\"{self.symbol}.csv\"\n        if not data_path.exists():\n            logger.error(f\"{self.symbol} 데이터 파일이 없습니다\")\n            return False\n            \n        # 데이터 로드\n        df = pd.read_csv(data_path)\n        \n        # datetime 처리\n        df['datetime'] = pd.to_datetime(df['datetime'])\n        \n        # 컬럼 타입 변환\n        numeric_columns = [\n            'open', 'high', 'low', 'close', 'volume',\n            'open_interest', 'funding_rate', 'predicted_funding_rate',\n            'liquidation', 'long_short_ratio'\n        ]\n        \n        for col in numeric_columns:\n            try:\n                if col in df.columns:\n                    # 문자열 전처리\n                    if df[col].dtype == object:\n                        df[col] = df[col].astype(str).str.extract('(\\d+\\.?\\d*)', expand=False)\n                    # 숫자로 변환\n                    df[col] = pd.to_numeric(df[col], errors='coerce')\n            except Exception as e:\n                logger.warning(f\"{self.symbol} {col} 컬럼 변환 중 오류: {str(e)}\")\n                \n        # 결측치 처리\n        df = df.fillna(method='ffill')\n        \n        # 데이터 정렬\n        df = df.sort_values('datetime')\n        \n        self.df = df\n        logger.info(f\"{self.symbol} 데이터 로드 완료: {len(df)}행\")\n        return True\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 데이터 로드 중 오류: {str(e)}\")\n        return False\n        \ndef prepare_timeframes(self) -> Dict[str, pd.DataFrame]:\n    \"\"\"멀티 타임프레임 데이터 준비\"\"\"\n    if self.df is None:\n        return {}\n        \n    timeframes = {\n        '5M': 1,    # 5분 = 기본 데이터\n        '30M': 6,   # 30분 = 6캔들\n        '1H': 12,   # 1시간 = 12캔들\n        '4H': 48    # 4시간 = 48캔들\n    }\n    \n    tf_data = {}\n    \n    for tf, period in timeframes.items():\n        # 리샘플링\n        resampled = self.df.resample(f'{period*5}min', on='datetime', label='right').agg({\n            'open': 'first',\n            'high': 'max',\n            'low': 'min',\n            'close': 'last',\n            'volume': 'sum',\n            'open_interest': 'last',\n            'funding_rate': 'last',\n            'predicted_funding_rate': 'last',\n            'liquidation': 'sum',\n            'long_short_ratio': 'last'\n        })\n        \n        # 지표 계산\n        df_tf = resampled.copy()\n        \n        # OI 관련 지표\n        df_tf['oi_change'] = df_tf['open_interest'].pct_change(fill_method=None)\n        df_tf['oi_ma'] = df_tf['open_interest'].rolling(window=20, min_periods=1).mean()\n        df_tf['oi_std'] = df_tf['open_interest'].rolling(window=20, min_periods=1).std()\n        \n        # 펀딩비 관련 지표\n        df_tf['funding_sign_change'] = np.sign(df_tf['funding_rate']) != np.sign(df_tf['funding_rate'].shift(1))\n        df_tf['funding_prediction_gap'] = df_tf['funding_rate'] - df_tf['predicted_funding_rate']\n        df_tf['funding_ma'] = df_tf['funding_rate'].rolling(window=20, min_periods=1).mean()\n        df_tf['funding_std'] = df_tf['funding_rate'].rolling(window=20, min_periods=1).std()\n        \n        # 거래량 관련 지표\n        df_tf['volume_ma'] = df_tf['volume'].rolling(window=20, min_periods=1).mean()\n        df_tf['volume_ratio'] = df_tf['volume'] / df_tf['volume_ma']\n        df_tf['volume_std'] = df_tf['volume'].rolling(window=20, min_periods=1).std()\n        \n        # 가격 변동성\n        df_tf['returns'] = df_tf['close'].pct_change(fill_method=None)\n        df_tf['volatility'] = df_tf['returns'].rolling(window=20, min_periods=1).std()\n        df_tf['volatility_ma'] = df_tf['volatility'].rolling(window=20, min_periods=1).mean()\n        \n        # 청산/롱숏 관련\n        df_tf['liquidation_ma'] = df_tf['liquidation'].rolling(window=20, min_periods=1).mean()\n        df_tf['liquidation_ratio'] = df_tf['liquidation'] / df_tf['liquidation_ma']\n        df_tf['ls_extreme'] = (df_tf['long_short_ratio'] > 1.25) | (df_tf['long_short_ratio'] < 0.75)\n        \n        # 추세 강도\n        df_tf['atr'] = self.calculate_atr(df_tf)\n        df_tf['trend_strength'] = abs(df_tf['close'] - df_tf['close'].shift(10)) / (df_tf['atr'] * 10)\n        \n        # NaN 값 처리\n        df_tf.fillna(0, inplace=True)\n        \n        tf_data[tf] = df_tf\n        \n    return tf_data\n    \ndef calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:\n    \"\"\"ATR(Average True Range) 계산\"\"\"\n    high = df['high']\n    low = df['low']\n    close = df['close']\n    \n    tr1 = high - low\n    tr2 = abs(high - close.shift())\n    tr3 = abs(low - close.shift())\n    \n    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n    return tr.rolling(window=period).mean()\n    \ndef analyze(self) -> Dict:\n    \"\"\"심볼 분석 실행\"\"\"\n    try:\n        # 1. 데이터 로드\n        if not self.load_data():\n            return {}\n            \n        # 2. 멀티 타임프레임 데이터 준비\n        tf_data = self.prepare_timeframes()\n        if not tf_data:\n            return {}\n            \n        # 3. 각 타임프레임별 분석\n        results = {}\n        for tf, df in tf_data.items():\n            tf_result = self.analyze_timeframe(tf, df)\n            if tf_result:\n                results[tf] = tf_result\n                \n        # 4. 결과 저장\n        self.save_results(results)\n        \n        return results\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 분석 중 오류: {str(e)}\")\n        return {}\n        \ndef analyze_timeframe(self, timeframe: str, df: pd.DataFrame) -> Dict:\n    \"\"\"단일 타임프레임 분석\"\"\"\n    try:\n        result = {\n            'timestamp': df.index[-1].isoformat(),\n            'patterns': []\n        }\n        \n        # 최소 20개의 데이터 필요\n        if len(df) < 20:\n            return result\n            \n        # 현재 데이터\n        current = df.iloc[-1]\n        \n        # 1. OI 기반 패턴\n        oi_pattern = self.detect_oi_pattern(df)\n        if oi_pattern:\n            result['patterns'].append(oi_pattern)\n            \n        # 2. 펀딩비 기반 패턴\n        funding_pattern = self.detect_funding_pattern(df)\n        if funding_pattern:\n            result['patterns'].append(funding_pattern)\n            \n        # 3. 복합 패턴\n        complex_pattern = self.detect_complex_pattern(df)\n        if complex_pattern:\n            result['patterns'].append(complex_pattern)\n            \n        return result\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} {timeframe} 분석 중 오류: {str(e)}\")\n        return {}\n        \ndef detect_oi_pattern(self, df: pd.DataFrame) -> Optional[Dict]:\n    \"\"\"OI 기반 패턴 감지\"\"\"\n    try:\n        current = df.iloc[-1]\n        prev = df.iloc[-2]\n        \n        # OI 급증 후 하락\n        oi_surge_drop = (\n            df['oi_change'].iloc[-4:-1].mean() > 0.07 and \n            current['oi_change'] < -0.01\n        )\n        \n        # OI 급락 후 반등\n        oi_drop_bounce = (\n            df['oi_change'].iloc[-4:-1].mean() < -0.10 and\n            current['oi_change'] > 0.01\n        )\n        \n        # OI 횡보 후 급변\n        lookback = 20\n        oi_breakout = (\n            abs(df['oi_change'].iloc[-lookback:-1].max()) <= 0.03 and\n            abs(current['oi_change']) > 0.05\n        )\n        \n        if any([oi_surge_drop, oi_drop_bounce, oi_breakout]):\n            pattern_type = (\"OI_SURGE_DROP\" if oi_surge_drop else\n                          \"OI_DROP_BOUNCE\" if oi_drop_bounce else\n                          \"OI_BREAKOUT\")\n                          \n            return {\n                \"type\": pattern_type,\n                \"confidence\": self.calculate_confidence(current),\n                \"metrics\": {\n                    \"oi_change\": current['oi_change'],\n                    \"volume_ratio\": current['volume_ratio'],\n                    \"trend_strength\": current['trend_strength']\n                }\n            }\n            \n        return None\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} OI 패턴 감지 중 오류: {str(e)}\")\n        return None\n        \ndef detect_funding_pattern(self, df: pd.DataFrame) -> Optional[Dict]:\n    \"\"\"펀딩비 기반 패턴 감지\"\"\"\n    try:\n        current = df.iloc[-1]\n        \n        # 펀딩비 방향 전환\n        funding_flip = current['funding_sign_change']\n        \n        # 극단적 펀딩비\n        funding_extreme = abs(current['funding_rate']) > 0.001\n        \n        # 예측값과 실제값 차이\n        funding_divergence = abs(current['funding_prediction_gap']) > 0.0005\n        \n        if any([funding_flip, funding_extreme, funding_divergence]):\n            pattern_type = (\"FUNDING_FLIP\" if funding_flip else\n                          \"FUNDING_EXTREME\" if funding_extreme else\n                          \"FUNDING_DIVERGENCE\")\n                          \n            return {\n                \"type\": pattern_type,\n                \"confidence\": self.calculate_confidence(current),\n                \"metrics\": {\n                    \"funding_rate\": current['funding_rate'],\n                    \"funding_prediction_gap\": current['funding_prediction_gap'],\n                    \"volume_ratio\": current['volume_ratio']\n                }\n            }\n            \n        return None\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 펀딩비 패턴 감지 중 오류: {str(e)}\")\n        return None\n        \ndef detect_complex_pattern(self, df: pd.DataFrame) -> Optional[Dict]:\n    \"\"\"복합 패턴 감지\"\"\"\n    try:\n        current = df.iloc[-1]\n        \n        # OI와 펀딩비 다이버전스\n        oi_funding_divergence = (\n            (current['oi_change'] > 0.08 and current['funding_rate'] < -0.0005) or\n            (current['oi_change'] < -0.08 and current['funding_rate'] > 0.0005)\n        )\n        \n        # 거래량 급증 + 롱숏 극단\n        volume_ls_extreme = (\n            current['volume_ratio'] > 5.0 and\n            abs(current['long_short_ratio'] - 1) > 0.25\n        )\n        \n        # 대량 청산\n        liquidation_surge = current['liquidation_ratio'] > 3.0\n        \n        if any([oi_funding_divergence, volume_ls_extreme, liquidation_surge]):\n            pattern_type = (\"OI_FUNDING_DIVERGENCE\" if oi_funding_divergence else\n                          \"VOLUME_LS_EXTREME\" if volume_ls_extreme else\n                          \"LIQUIDATION_SURGE\")\n                          \n            return {\n                \"type\": pattern_type,\n                \"confidence\": self.calculate_confidence(current),\n                \"metrics\": {\n                    \"oi_change\": current['oi_change'],\n                    \"funding_rate\": current['funding_rate'],\n                    \"volume_ratio\": current['volume_ratio'],\n                    \"long_short_ratio\": current['long_short_ratio'],\n                    \"liquidation_ratio\": current['liquidation_ratio']\n                }\n            }\n            \n        return None\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 복합 패턴 감지 중 오류: {str(e)}\")\n        return None\n        \ndef calculate_confidence(self, current: pd.Series) -> float:\n    \"\"\"신뢰도 점수 계산\"\"\"\n    confidence = 60  # 기본 점수\n    \n    # 보조 지표 가점\n    if current['volume_ratio'] > 3.0: confidence += 10\n    if current['ls_extreme']: confidence += 10\n    if current['liquidation_ratio'] > 2.0: confidence += 10\n    if current['trend_strength'] > 1.0: confidence += 5\n    \n    return min(100, confidence)\n    \ndef save_results(self, results: Dict):\n    \"\"\"분석 결과 저장\"\"\"\n    try:\n        # 결과 파일 저장\n        output_file = self.data_dir / f\"{self.symbol}.json\"\n        with open(output_file, 'w') as f:\n            json.dump({\n                \"timestamp\": datetime.now().isoformat(),\n                \"symbol\": self.symbol,\n                \"results\": results\n            }, f, indent=2)\n            \n        logger.info(f\"{self.symbol} 분석 결과 저장 완료\")\n        \n    except Exception as e:\n        logger.error(f\"{self.symbol} 결과 저장 중 오류: {str(e)}\") \n\n기존에 이 2가지 코드로 json값을 분석했다. 이내용을 파악하고 지금이랑 차이점을 판단하라.\n\n이 코드기준으로 판단하자 일단? log 파일 만들고 json 파일은 저장하고.","commandType":4},{"text":"순서대로 구성해봐","commandType":4},{"text":"구동해봐 제대로 되는지 보자","commandType":4},{"text":"@integrated_report_20250312_124930.json 이거 내용이잖아? 전체심볼에서 확률 높은 구간을 1개라도 해야하잖아? 방법을 고안만 좀해보자.","commandType":4},{"text":"우리가 심볼이 몇개야?","commandType":4},{"text":"그럼 json 파일 다조회해봐 제대로 점수랑 다되있는지?","commandType":4},{"text":"그럼 저거 알람 어떻게 진행 할거야? 부근에 왔을때? 예상했을때? 아니면 딱 맞았을때? 완성도에 따라 할거야 ?","commandType":4},{"text":"json 파일에서 근접 알람시스템을 만들어야겠네? 그럼?","commandType":4},{"text":"1분마다 감지하는거 만들었다 그치?\njson 파일 생성하는거 만들었다.\n알람화 시키는거 만들고있어. \n여기까지 한거맞지?\n\n정리좀하고가게.","commandType":4},{"text":"다시 \n1번 1분마다 데이터가져오는거 만들었지?(과거데이터 누락도 가져오는거지?)\n\n2번 1번 데이터에 대해서 json파일 생성(패턴감지)\n\n3번 2번이만든 패턴감지에 대해서 csv 실시간 데이터 가져오는거에서 문제있는지 검수하다가 알람이 생겼을때 알람화 <\n\n이렇게 인식해봐","commandType":4},{"text":"all_symbols_signals_*.json이거 그냥 심볼.json으로 통일하면 안되니?","commandType":4},{"text":"그리고 json파일이랑 csv 파일로 검증한번하면 좋겠어 몇번정도 신호가 왔고 예상은 어떻게 되었다 등등 \n\n그리고 Json 파일에 대한거 하나로 합쳤으면 좋겠어.\n\n여러개 분산되어있는게 불편하다. ","commandType":4},{"text":"아니 JSON 파일생성기에 대해서 말하는거야.","commandType":4},{"text":"다시설명해봐............. JSON 파일 생성기 이후에 검증절차 점수화 작업까지 해야하잖아?","commandType":4},{"text":"JSON는 각각 심볼로되어야해<","commandType":4},{"text":"매일 9시에 내용 진행 할거거든? \n각심볼 CSV분석해서 알람 기준 JSON 파일 만들고\n2차 검증 3차 대조 \n하는 프로그램을 만들거야. \n\n기존 내용들을 합쳐서 만들어줄래? \n\n1개의 PY로","commandType":4},{"text":"왜 만들다 더이상진행이 안되니?","commandType":4},{"text":"그럼 나눠서 해","commandType":4},{"text":"구동 잘되는거지?","commandType":4},{"text":"@csv_validation 이폴더가 보관내용이야?","commandType":4},{"text":"@1INCH_changes.json 이거는?","commandType":4},{"text":"이파일이 알람 조건이야?","commandType":4},{"text":"그럼 알람 json파일은 어디있는데?","commandType":4},{"text":"@all_symbols_best_signals_20250312_125127.json 1개로 관리하지말고 live폴더에 각심볼이름으로 저장하고싶은데? ","commandType":4},{"text":"구조상 문제가 될부분이 있나? ","commandType":4},{"text":"그럼 전체 심볼모아놓은거 1개랑 각심볼마다 저장하게 2가지로 나눠보는건어때?","commandType":4},{"text":"csv 파일 탐색기 exe1개\n지금 json 파일 생성 관리기 exe 1개\n이렇게 만들수있는거야?","commandType":4},{"text":"내말을 이해못하는듯..","commandType":4},{"text":"@crypto_data_manager.py 이게 1번 exe해야할거<","commandType":4},{"text":"@crypto_analysis_system.py 얘가 json 파일 관련된애 아님?","commandType":4},{"text":"그럼 1번2번 exe 는 만들었잖아? 2번 기준점을 1번csv파일을 감시해야하잖아? 근데 똑같은 상황은 안나올 수 있고 알람이 와야하잖아? 변곡은 결국 데이터 싸움이라고 나는 생각하거든? 그걸 판단할 수 있는 감시 기능을 만들어야할것같은데? 너의 생각은?","commandType":4},{"text":"그럼 1번2번 exe 는 만들었잖아? 2번 기준점을 1번csv파일을 감시해야하잖아? 근데 똑같은 상황은 안나올 수 있고 알람이 와야하잖아? 변곡은 결국 데이터 싸움이라고 나는 생각하거든? 그걸 판단할 수 있는 감시 기능을 만들어야할것같은데? 너의 생각은?\n\n이야기해봐","commandType":4},{"text":"너무길어서 문제면 나눠서 작성해","commandType":4},{"text":"매일 9시에 최신 csv파일을 기준으로 변곡점 알람 기준을 만드는거잖아? 그치? 그게 간극이 있는거야? 내가보기엔 1개는 실시간 1개는 누적데이터를 가지고하는거라 문제되어 보이지 않는데?","commandType":4},{"text":"csv json 파일로 알람화 하는거 코드짜야하잖아?","commandType":4},{"text":"필요파일 제외하고 일단 삭제하자","commandType":4},{"text":"지금 뭐하고있는지설명해","commandType":4},{"text":"그 애널리스 시스템 스케쥴화 시켜서 만들어봐","commandType":4},{"text":"exe 파일만들어야지?","commandType":4},{"text":"그렇게하면 애널리시스 py프로그램이 필요한거지?","commandType":4},{"text":"포함해서 exe를 만들어봐","commandType":4},{"text":"아니  2개만 포함해서 만들라고 스캐쥴이랑 애널리시스랑","commandType":4},{"text":"아 그래? py파일은 폴더에 없어도되?","commandType":4},{"text":"자 그럼 이제 csv 파일을 json 부근에 왔을때 알람화 시키는걸 만들어야겠지?","commandType":4},{"text":"구동해보자고","commandType":4},{"text":"신호기준에서 과거 시간프레임위치에서부터 알람 거는 위치를 알려줘봐","commandType":4},{"text":"csv 파일을 가지고 실제로 알람이 구동했을 법한 자리들을 공유해줘","commandType":4},{"text":"아니 내가 원하는 프로그램으로 백테스트 할 수 있는 기능도 포함 실시간 감지도 포함 해줘","commandType":4},{"text":"백테스트를 돌려보자 어느시점에 알람이 올거고 어느시점에 반등이 왔고를 싹분석해보자 백테스트기능은","commandType":4},{"text":"그걸 니가분석하지말고 프로그램이 분석해서 알람을 줘야지? 예시로 백테스트를 만드는건데","commandType":4},{"text":"ㅇㅇ","commandType":4},{"text":"실행해보자","commandType":4},{"text":"??","commandType":4},{"text":"백테스트하자니까??","commandType":4},{"text":"알람을 줘야지? 어떻게줄건데 ","commandType":4},{"text":"백테스트 진행하면서 알람이 오는 위치의 타임프레임을 제공하라 btc, eth, sol, xrp, ada, link 6개 점검좀해보자","commandType":4},{"text":"아니 csv 파일로 json기준의 알람을 보고싶은거야 타임프레임기준으로 ","commandType":4},{"text":"@LINK.json @LINK.csv csv파일에서 json 파일 기준으로 알람이 올만한자리를 타임프레임기준으로 나한테 설명하라 분단위까지 ","commandType":4},{"text":"우리가 알람시스템을 만들고 있잖아? 알람이 올만한 시간대 타임프레임을 실시간으로 보고싶으니까? 알람이 올자리에서 알람을 달라고 ","commandType":4},{"text":"그래서 알람이 언제오냐고?","commandType":4},{"text":"@LINK.json 여기에 변곡점 알람이 언제로 되어있는데?","commandType":4},{"text":"@LINK.json 얘는 어떤 알람에 신호를 주라는거야?","commandType":4},{"text":"아니 신호 기준을 말해줘","commandType":4},{"text":"체링은 OI값이랑 펀딩비는 무의미했던건가?","commandType":4},{"text":"니가 수정하면 안된다","commandType":4},{"text":"@LINK.csv 그럼 그시점의 OI값이랑 펀딩비의 내용을 체크하라","commandType":4},{"text":"크립토 데이터 매니저 삭제했냐?","commandType":4},{"text":"exe파일있는데 분해못해?","commandType":4},{"text":"@dist 여기 exe 파일있어","commandType":4},{"text":"py파일로 분해하라고","commandType":4},{"text":"암호화폐 데이터 통합 관리 시스템 (CCXT 데이터 보완 및 장기 보관 기능 추가)\n\n과거 데이터 누락 검사 및 복구 (CCXT로 보완)\n실시간 데이터 수집 및 저장\n단일 CSV 파일에 시간순 정렬 저장\n자동 심볼 추가 및 관리\n안전한 종료 메커니즘\n데이터 장기 보관\n소수점 정확도 유지\n\"\"\"\nimport os\nimport logging\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nimport time\nimport base64\nimport signal\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Union\nimport random\nimport threading\nimport queue\nimport ccxt\n\n안전한 종료를 위한 전역 플래그\nshutdown_flag = threading.Event()\n\n기본 설정\nBASE_URL = \"https://api.coinalyze.net/v1\"\nDATA_DIR = Path(\"data\")\nLIVE_DIR = DATA_DIR / \"live\"\nBACKUP_DIR = DATA_DIR / \"backup\"\nARCHIVE_DIR = DATA_DIR / \"archive\"\nLONG_TERM_DIR = DATA_DIR / \"longterm\"\n\nAPI 요청 간격 설정 (초)\nAPI_REQUEST_INTERVAL = 0.5 # API 요청 사이 대기 시간 (초)\nCOLLECTION_INTERVAL = 300 # 데이터 수집 간격 (초) - 5분\n\nAPI 엔드포인트\nENDPOINTS = {\n\"exchanges\": \"/exchanges\",\n\"future_markets\": \"/future-markets\",\n\"spot_markets\": \"/spot-markets\",\n\"open_interest\": \"/open-interest\",\n\"funding_rate\": \"/funding-rate\",\n\"predicted_funding_rate\": \"/predicted-funding-rate\",\n\"open_interest_history\": \"/open-interest-history\",\n\"funding_rate_history\": \"/funding-rate-history\",\n\"predicted_funding_rate_history\": \"/predicted-funding-rate-history\",\n\"liquidation_history\": \"/liquidation-history\",\n\"long_short_ratio_history\": \"/long-short-ratio-history\",\n\"ohlcv_history\": \"/ohlcv-history\"\n}\n\nAPI 키 설정 (16개 키를 4개 티어로 구분)\nAPI_KEYS = {\n\"tier1\": [\n\"0d0ba171-4185-4a58-bc02-8c8627cd1f54\",\n\"e629efa9-68b0-4b6b-b794-fa2d9e379b79\",\n\"c9779cfd-a85c-4e58-a378-9474174a075e\",\n\"58a39d7b-84e5-4852-8375-90de055cba18\"\n],\n\"tier2\": [\n\"7b80a59d-f0f9-4a83-81f7-1314dbdd9dc7\",\n\"2b951698-64e7-4a86-9930-c503d4e29e54\",\n\"10b24c79-ddd1-4046-94b6-0af0d23b241e\",\n\"fac040b5-5043-4351-b8f3-09c1d8cfd78f\"\n],\n\"tier3\": [\n\"fe1caf7e-5e27-4f0c-9d14-2a4611db625f\",\n\"fc9e5080-9607-46e8-b48c-deca57591990\",\n\"a82fd8a4-873f-4619-9b9f-d8a29373b5b8\",\n\"6431161c-8815-4d18-846d-55e00863682e\"\n],\n\"tier4\": [\n\"07cc6de0-0d02-41b8-acdc-dd1284bf5730\",\n\"f2842bed-d43e-4152-a0b5-68d9d9ed30b0\",\n\"013d0da7-76ea-4699-97d7-6e0f66857939\",\n\"7906d7bf-b493-42ad-a578-0cd13c6a070c\"\n]\n}\n\nCCXT 설정 - 공개 데이터만 사용하므로 API 키 불필요\nCCXT_EXCHANGES = [\n\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"\n]\n\n로깅 설정\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[\nlogging.StreamHandler(),\nlogging.FileHandler(\"crypto_data_manager.log\", encoding='utf-8')\n]\n)\nlogger = logging.getLogger(name)\n\ndef signal_handler(sig, frame):\n\"\"\"시스템 종료 신호를 처리하는 핸들러\"\"\"\nlogger.warning(\"종료 신호를 받았습니다. 안전하게 종료합니다...\")\nshutdown_flag.set()\n\n시그널 핸들러 등록\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\n\ndef check_stop_file():\n\"\"\"종료 파일 확인\"\"\"\nif os.path.exists(\"stop.signal\"):\nlogger.warning(\"종료 파일이 감지되었습니다. 안전하게 종료합니다...\")\ntry:\nos.remove(\"stop.signal\") # 종료 파일 삭제\nexcept:\npass\nshutdown_flag.set()\nreturn True\nreturn False\n\nclass APIKeyManager:\n\"\"\"API 키 관리 클래스\"\"\"\ndef init(self):\nself.tier_keys = API_KEYS\nself.current_key_index = {tier: 0 for tier in API_KEYS.keys()}\nself.key_locks = {tier: threading.Lock() for tier in API_KEYS.keys()}\nself.key_rate_limits = {key: {'last_used': 0, 'count': 0} for tier in API_KEYS.values() for key in tier}\nself.rate_limit_reset_time = time.time() + 60 # 1분 후 리셋\n\npython\n\n실행\n\n복사\ndef get_next_key(self, tier: str) -> str:\n    \"\"\"라운드 로빈 방식으로 다음 API 키 반환\"\"\"\n    if tier not in self.tier_keys or not self.tier_keys[tier]:\n        available_tiers = [t for t in self.tier_keys if self.tier_keys[t]]\n        if not available_tiers:\n            raise ValueError(\"사용 가능한 API 키가 없습니다\")\n        tier = random.choice(available_tiers)\n        \n    with self.key_locks[tier]:\n        keys = self.tier_keys[tier]\n        current_index = self.current_key_index[tier]\n        \n        # 현재 시간이 리셋 시간을 초과했는지 확인\n        current_time = time.time()\n        if current_time > self.rate_limit_reset_time:\n            # 사용량 카운터 리셋\n            for key_info in self.key_rate_limits.values():\n                key_info['count'] = 0\n            self.rate_limit_reset_time = current_time + 60  # 다음 1분 후 리셋\n        \n        # 사용량이 가장 적은 키를 찾아 반환\n        min_count = float('inf')\n        selected_key = None\n        \n        for i in range(len(keys)):\n            idx = (current_index + i) % len(keys)\n            key = keys[idx]\n            \n            if self.key_rate_limits[key]['count'] < min_count:\n                min_count = self.key_rate_limits[key]['count']\n                selected_key = key\n                self.current_key_index[tier] = (idx + 1) % len(keys)\n        \n        # 키 사용량 기록\n        self.key_rate_limits[selected_key]['last_used'] = current_time\n        self.key_rate_limits[selected_key]['count'] += 1\n        \n        # API 요청 간격 조절을 위한 대기\n        last_used = self.key_rate_limits[selected_key]['last_used']\n        if current_time - last_used < API_REQUEST_INTERVAL:\n            time.sleep(API_REQUEST_INTERVAL - (current_time - last_used))\n        \n        return selected_key\nclass CcxtManager:\n\"\"\"CCXT 거래소 관리 및 심볼 매핑 클래스\"\"\"\ndef init(self):\nself.exchanges = {}\nself.symbol_mappings = {} # base_symbol -> {exchange_id -> ccxt_symbol}\nself.exchange_priority = [\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"]\nself.init_exchanges()\nself.supported_symbols = set() # CCXT에서 지원하는 심볼 목록\nself.timeframe_map = {\n\"1min\": \"1m\", \"5min\": \"5m\", \"15min\": \"15m\", \"30min\": \"30m\",\n\"1h\": \"1h\", \"4h\": \"4h\", \"1d\": \"1d\", \"1w\": \"1w\"\n}\n\npython\n\n실행\n\n복사\ndef init_exchanges(self):\n    \"\"\"CCXT 거래소 초기화 (API 키 없이)\"\"\"\n    for exchange_id in CCXT_EXCHANGES:\n        try:\n            if exchange_id in ccxt.exchanges:\n                exchange_class = getattr(ccxt, exchange_id)\n                self.exchanges[exchange_id] = exchange_class({\n                    'enableRateLimit': True,\n                    'options': {\n                        'defaultType': 'future'  # 선물 거래 기본 설정\n                    }\n                })\n                logger.info(f\"CCXT {exchange_id} 거래소 초기화 성공\")\n        except Exception as e:\n            logger.error(f\"CCXT {exchange_id} 거래소 초기화 실패: {str(e)}\")\n\ndef load_all_markets(self):\n    \"\"\"모든 거래소의 시장 정보 로드\"\"\"\n    for exchange_id, exchange in self.exchanges.items():\n        try:\n            exchange.load_markets()\n            logger.info(f\"{exchange_id} 시장 정보 로드 성공: {len(exchange.symbols)}개 심볼\")\n        except Exception as e:\n            logger.error(f\"{exchange_id} 시장 정보 로드 실패: {str(e)}\")\n\ndef discover_symbol_mappings(self, base_symbols):\n    \"\"\"여러 기본 심볼에 대한 CCXT 매핑 자동 탐색\"\"\"\n    logger.info(f\"CCXT 심볼 매핑 탐색 시작: {len(base_symbols)}개 심볼\")\n    \n    # 거래소 시장 정보 로드\n    self.load_all_markets()\n    \n    # 각 심볼별 매핑 탐색\n    for base_symbol in base_symbols:\n        self.discover_symbol_mapping(base_symbol)\n    \n    # 매핑 정보 저장\n    self.save_symbol_mappings()\n    \n    logger.info(f\"CCXT 심볼 매핑 탐색 완료: {len(self.symbol_mappings)}개 심볼\")\n\ndef discover_symbol_mapping(self, base_symbol):\n    \"\"\"단일 기본 심볼에 대한 CCXT 매핑 탐색\"\"\"\n    if base_symbol in self.symbol_mappings:\n        return\n        \n    self.symbol_mappings[base_symbol] = {}\n    \n    # 거래소별 검색 패턴\n    search_patterns = {\n        \"binance\": [\n            f\"{base_symbol}/USDT:USDT\",  # 영구 선물\n            f\"{base_symbol}USDT_PERP\",\n            f\"{base_symbol}/USDT\"        # 현물\n        ],\n        \"bybit\": [\n            f\"{base_symbol}USDT\",        # 영구 선물\n            f\"{base_symbol}/USDT\"        # 현물\n        ],\n        \"okx\": [\n            f\"{base_symbol}-USDT-SWAP\",  # 영구 선물\n            f\"{base_symbol}/USDT:USDT\",\n            f\"{base_symbol}/USDT\"        # 현물\n        ],\n        \"kucoin\": [\n            f\"{base_symbol}USDTM\",       # 영구 선물\n            f\"{base_symbol}-USDT\",       # 영구 선물 (다른 형식)\n            f\"{base_symbol}/USDT\"        # 현물\n        ],\n        \"bitget\": [\n            f\"{base_symbol}USDT_UMCBL\",  # 영구 선물\n            f\"{base_symbol}/USDT\"        # 현물\n        ]\n    }\n    \n    # 거래소 우선순위대로 검색\n    for exchange_id in self.exchange_priority:\n        if exchange_id not in self.exchanges:\n            continue\n            \n        exchange = self.exchanges[exchange_id]\n        patterns = search_patterns.get(exchange_id, [f\"{base_symbol}/USDT\"])\n        \n        for pattern in patterns:\n            found = False\n            for symbol in exchange.symbols:\n                normalized_symbol = symbol.upper()\n                normalized_pattern = pattern.upper()\n                \n                # 정확한 일치 또는 포함 관계 확인\n                if normalized_symbol == normalized_pattern or normalized_pattern in normalized_symbol:\n                    self.symbol_mappings[base_symbol][exchange_id] = symbol\n                    logger.info(f\"심볼 매핑 발견: {base_symbol} -> {exchange_id}:{symbol}\")\n                    self.supported_symbols.add(base_symbol)\n                    found = True\n                    break\n            \n            if found:\n                break\n\ndef save_symbol_mappings(self):\n    \"\"\"심볼 매핑 정보 파일로 저장\"\"\"\n    try:\n        filepath = DATA_DIR / \"symbol_mappings.json\"\n        with open(filepath, 'w') as f:\n            json.dump({\n                \"mappings\": self.symbol_mappings,\n                \"supported_symbols\": list(self.supported_symbols),\n                \"timestamp\": datetime.now().isoformat()\n            }, f, indent=2)\n        logger.info(f\"심볼 매핑 정보 저장 완료: {filepath}\")\n    except Exception as e:\n        logger.error(f\"심볼 매핑 정보 저장 실패: {str(e)}\")\n\ndef load_symbol_mappings(self):\n    \"\"\"저장된 심볼 매핑 정보 로드\"\"\"\n    try:\n        filepath = DATA_DIR / \"symbol_mappings.json\"\n        if filepath.exists():\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n                self.symbol_mappings = data.get(\"mappings\", {})\n                self.supported_symbols = set(data.get(\"supported_symbols\", []))\n            logger.info(f\"심볼 매핑 정보 로드 완료: {len(self.symbol_mappings)}개 심볼\")\n            return True\n        return False\n    except Exception as e:\n        logger.error(f\"심볼 매핑 정보 로드 실패: {str(e)}\")\n        return False\n\ndef get_ccxt_symbol(self, base_symbol, exchange_id=None):\n    \"\"\"CCXT 심볼 가져오기 (특정 거래소 또는 우선순위 기반)\"\"\"\n    # 매핑 정보 확인\n    if base_symbol not in self.symbol_mappings:\n        self.discover_symbol_mapping(base_symbol)\n    \n    # 특정 거래소 지정한 경우\n    if exchange_id:\n        return self.symbol_mappings.get(base_symbol, {}).get(exchange_id)\n    \n    # 거래소 우선순위 기준 탐색\n    for ex_id in self.exchange_priority:\n        if ex_id in self.symbol_mappings.get(base_symbol, {}):\n            return self.symbol_mappings[base_symbol][ex_id], ex_id\n    \n    return None, None\n\ndef fetch_ohlcv(self, base_symbol, timeframe='5m', since=None, limit=1000):\n    \"\"\"CCXT를 통해 OHLCV 데이터 가져오기\"\"\"\n    if shutdown_flag.is_set():\n        return pd.DataFrame()\n        \n    # 심볼 매핑 확인 및 가져오기\n    ccxt_symbol, exchange_id = self.get_ccxt_symbol(base_symbol)\n    \n    if not ccxt_symbol or not exchange_id:\n        logger.warning(f\"CCXT 심볼 매핑을 찾을 수 없음: {base_symbol}\")\n        return pd.DataFrame()\n    \n    # 타임프레임 변환\n    ccxt_timeframe = self.timeframe_map.get(timeframe, timeframe)\n    \n    # since 변환 (timestamp to milliseconds)\n    since_ms = int(since.timestamp() * 1000) if since else None\n    \n    # 데이터 가져오기 시도\n    try:\n        exchange = self.exchanges[exchange_id]\n        \n        # OHLCV 데이터 가져오기\n        ohlcv = exchange.fetch_ohlcv(ccxt_symbol, ccxt_timeframe, since_ms, limit)\n        \n        if not ohlcv:\n            logger.warning(f\"CCXT에서 데이터 없음: {exchange_id}:{ccxt_symbol}\")\n            return pd.DataFrame()\n        \n        # DataFrame 변환\n        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n        df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n        df['symbol'] = base_symbol\n        df['exchange'] = exchange_id\n        \n        logger.info(f\"CCXT 데이터 수집 성공: {exchange_id}:{ccxt_symbol}, {len(df)}행\")\n        return df\n        \n    except Exception as e:\n        logger.error(f\"CCXT 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n        return pd.DataFrame()\n\ndef fetch_funding_rate(self, base_symbol, timeframe='1h', since=None, limit=500):\n    \"\"\"CCXT를 통해 펀딩비 데이터 가져오기 (지원하는 거래소만)\"\"\"\n    if shutdown_flag.is_set():\n        return pd.DataFrame()\n    \n    # 펀딩비를 지원하는 거래소들\n    funding_supported = [\"binance\", \"bybit\", \"okx\"]\n    funding_data = []\n    \n    # 지원하는 거래소에서 데이터 수집 시도\n    for exchange_id in funding_supported:\n        if exchange_id not in self.exchanges:\n            continue\n            \n        ccxt_symbol = self.get_ccxt_symbol(base_symbol, exchange_id)\n        if not ccxt_symbol:\n            continue\n            \n        try:\n            exchange = self.exchanges[exchange_id]\n            \n            # 1. fetchFundingRateHistory 사용 (최신 데이터)\n            if hasattr(exchange, 'fetchFundingRateHistory') and callable(getattr(exchange, 'fetchFundingRateHistory')):\n                since_ms = int(since.timestamp() * 1000) if since else None\n                funding_history = exchange.fetch_funding_rate_history(ccxt_symbol, since_ms, limit)\n                \n                if funding_history:\n                    for entry in funding_history:\n                        funding_data.append({\n                            'datetime': pd.to_datetime(entry['timestamp'], unit='ms'),\n                            'symbol': base_symbol,\n                            'exchange': exchange_id,\n                            'funding_rate': entry['fundingRate']\n                        })\n            \n            # 2. fetchFundingRate 사용 (현재 펀딩비)\n            elif hasattr(exchange, 'fetchFundingRate') and callable(getattr(exchange, 'fetchFundingRate')):\n                funding_info = exchange.fetch_funding_rate(ccxt_symbol)\n                if funding_info:\n                    funding_data.append({\n                        'datetime': pd.to_datetime(funding_info['timestamp'], unit='ms'),\n                        'symbol': base_symbol,\n                        'exchange': exchange_id,\n                        'funding_rate': funding_info['fundingRate']\n                    })\n        \n        except Exception as e:\n            logger.error(f\"CCXT 펀딩비 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n    \n    # 수집된 데이터를 DataFrame으로 변환\n    if funding_data:\n        df = pd.DataFrame(funding_data)\n        logger.info(f\"CCXT 펀딩비 데이터 수집 성공: {base_symbol}, {len(df)}행\")\n        return df\n    else:\n        return pd.DataFrame()\nAPI 키 매니저 인스턴스 생성\napi_key_manager = APIKeyManager()\n\nCCXT 매니저 인스턴스 생성\nccxt_manager = CcxtManager()\n\ndef get_simple_symbol(symbol: str) -> str:\n\"\"\"심볼명을 단순화 (예: BTCUSDT_PERP.A -> BTC)\"\"\"\nif 'USDT_PERP.A' in symbol:\nreturn symbol.split('USDT_')[0]\nreturn symbol.replace('.csv', '')\n\ndef exponential_backoff(attempt, base_delay=2, max_delay=120):\n\"\"\"지수 백오프 알고리즘\"\"\"\ndelay = min(max_delay, base_delay * (2 ** attempt))\njitter = random.uniform(0, 0.1 * delay) # 10% 지터 추가\nreturn delay + jitter\n\ndef make_request(endpoint: str, params: dict = None, tier: str = \"tier1\", max_retries: int = 5) -> Optional[dict]:\n\"\"\"API 요청 수행 (지수 백오프 적용)\"\"\"\nurl = f\"{BASE_URL}{endpoint}\"\n\npython\n\n실행\n\n복사\n# 종료 플래그 확인\nif shutdown_flag.is_set():\n    logger.warning(\"종료 중에는 API 요청을 수행하지 않습니다\")\n    return None\n\nfor attempt in range(max_retries):\n    try:\n        api_key = api_key_manager.get_next_key(tier)\n        headers = {\n            \"Accept\": \"application/json\",\n            \"Api-Key\": api_key\n        }\n        \n        response = requests.get(url, headers=headers, params=params, timeout=60)\n        \n        if response.status_code == 429:  # Rate limit exceeded\n            sleep_time = exponential_backoff(attempt)\n            logger.warning(f\"API 속도 제한 초과. {sleep_time:.2f}초 후 재시도... (시도 {attempt+1}/{max_retries})\")\n            time.sleep(sleep_time)\n            continue\n            \n        if response.status_code == 401:  # 인증 실패\n            logger.error(f\"API 키 인증 실패: {api_key}\")\n            continue\n            \n        response.raise_for_status()\n        return response.json()\n        \n    except requests.exceptions.Timeout:\n        sleep_time = exponential_backoff(attempt)\n        logger.error(f\"API 요청 타임아웃 ({endpoint}) - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n        time.sleep(sleep_time)\n    except requests.exceptions.RequestException as e:\n        sleep_time = exponential_backoff(attempt)\n        logger.error(f\"API 요청 실패 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n        time.sleep(sleep_time)\n    except Exception as e:\n        sleep_time = exponential_backoff(attempt)\n        logger.error(f\"예상치 못한 오류 발생 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n        time.sleep(sleep_time)\n\nreturn None\ndef get_all_symbols() -> List[str]:\n\"\"\"사용 가능한 모든 심볼 목록 조회\"\"\"\ndata = make_request(ENDPOINTS[\"future_markets\"])\n\nscheme\n\n복사\nif not data:\n    logger.error(\"심볼 목록을 가져올 수 없습니다\")\n    return []\n    \nsymbols = []\nif isinstance(data, dict):\n    markets = data.get('markets', [])\nelse:\n    markets = data\n    \nfor market in markets:\n    symbol = market.get('symbol', '')\n    if symbol.endswith('USDT_PERP.A'):\n        base_symbol = get_simple_symbol(symbol)\n        symbols.append(base_symbol)\n\nlogger.info(f\"총 {len(symbols)}개의 심볼을 찾았습니다\")\n\n# 심볼 목록 파일로 저장\nwith open(DATA_DIR / 'symbols.json', 'w') as f:\n    json.dump({\n        'symbols': symbols,\n        'count': len(symbols),\n        'timestamp': datetime.now().isoformat()\n    }, f, indent=2)\n    \nreturn sorted(symbols)\ndef extract_data_from_response(data, data_type: str):\n\"\"\"API 응답에서 데이터 추출 개선\"\"\"\nhistory = []\n\npowershell\n\n복사\ntry:\n    # 응답 데이터 구조 확인\n    if isinstance(data, list):\n        # 리스트 응답 처리\n        if len(data) > 0:\n            if isinstance(data[0], dict):\n                if 'history' in data[0]:\n                    # 기존 구조: [{\"history\": [...]}]\n                    history = data[0]['history']\n                else:\n                    # 새 구조: 리스트가 직접 데이터인 경우\n                    history = data\n            else:\n                # 직접 데이터 리스트인 경우\n                history = data\n    elif isinstance(data, dict):\n        # 딕셔너리 응답 처리\n        if 'history' in data:\n            history = data['history']\n        elif 'data' in data:\n            history = data['data']\n        elif 'result' in data:\n            history = data['result']\n        elif 'items' in data:\n            history = data['items']\n        else:\n            # 다른 키가 없으면 응답 전체를 사용\n            potential_data = []\n            for key, value in data.items():\n                if isinstance(value, list) and len(value) > 0:\n                    potential_data.extend(value)\n            if potential_data:\n                history = potential_data\nexcept Exception as e:\n    logger.error(f\"{data_type} 데이터 파싱 중 오류: {str(e)}\")\n\nreturn history\ndef normalize_columns(df, data_type: str) -> pd.DataFrame:\n\"\"\"데이터 프레임 컬럼 정규화 (소수점 보존)\"\"\"\n# 타임스탬프 변환\nif 't' in df.columns:\ndf['datetime'] = pd.to_datetime(df['t'], unit='s')\nelif 'timestamp' in df.columns:\ndf['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\nelif 'time' in df.columns:\ndf['datetime'] = pd.to_datetime(df['time'], unit='s')\n\nscheme\n\n복사\n# 데이터 타입별 컬럼 처리\nif data_type == 'ohlcv':\n    column_mappings = {\n        'o': 'open', 'open': 'open', 'price_open': 'open',\n        'h': 'high', 'high': 'high', 'price_high': 'high',\n        'l': 'low', 'low': 'low', 'price_low': 'low',\n        'c': 'close', 'close': 'close', 'price_close': 'close',\n        'v': 'volume', 'volume': 'volume', 'vol': 'volume'\n    }\nelif data_type == 'oi':\n    column_mappings = {\n        'c': 'open_interest', 'value': 'open_interest', \n        'openInterest': 'open_interest', 'oi': 'open_interest'\n    }\nelif data_type == 'funding':\n    column_mappings = {\n        'c': 'funding_rate', 'rate': 'funding_rate', \n        'fundingRate': 'funding_rate', 'funding': 'funding_rate'\n    }\nelif data_type == 'predicted_funding':\n    column_mappings = {\n        'c': 'predicted_funding_rate', 'rate': 'predicted_funding_rate',\n        'predictedRate': 'predicted_funding_rate', 'predicted': 'predicted_funding_rate'\n    }\nelif data_type == 'liquidation':\n    column_mappings = {\n        'c': 'liquidation', 'value': 'liquidation',\n        'liq': 'liquidation', 'liquidations': 'liquidation'\n    }\nelif data_type == 'long_short_ratio':\n    column_mappings = {\n        'c': 'long_short_ratio', 'ratio': 'long_short_ratio',\n        'l': 'long_percentage', 'long': 'long_percentage', 'longPercentage': 'long_percentage',\n        's': 'short_percentage', 'short': 'short_percentage', 'shortPercentage': 'short_percentage'\n    }\nelse:\n    column_mappings = {}\n\n# 컬럼 매핑 적용\nfor old_col, new_col in column_mappings.items():\n    if old_col in df.columns and old_col != new_col:\n        df[new_col] = df[old_col]\n\nreturn df\ndef validate_data(df, data_type: str) -> pd.DataFrame:\n\"\"\"데이터 검증 (소수점 보존)\"\"\"\nif df.empty:\nreturn df\n\nscheme\n\n복사\n# 타임스탬프가 없는 행 제거\nif 'datetime' in df.columns:\n    df = df[df['datetime'].notna()]\n\n# 데이터 타입별 검증\nif data_type == 'ohlcv':\n    # 가격 데이터 검증 (음수 값 제거, 소수점은 유지)\n    for col in ['open', 'high', 'low', 'close']:\n        if col in df.columns:\n            # 0 이하 값 확인 후 경고만 발생 (제거하지 않음)\n            zero_or_neg = df[df[col] <= 0]\n            if not zero_or_neg.empty:\n                logger.warning(f\"{data_type}: {len(zero_or_neg)}개 행에서 {col} 값이 0 이하입니다. 데이터 유지됨.\")\n    \n    # 볼륨 검증 (음수 제거)\n    if 'volume' in df.columns:\n        neg_volume = df[df['volume'] < 0]\n        if not neg_volume.empty:\n            logger.warning(f\"{data_type}: {len(neg_volume)}개 행에서 volume 값이 음수입니다. 해당 값을 0으로 설정합니다.\")\n            df.loc[df['volume'] < 0, 'volume'] = 0\n\nelif data_type == 'funding' or data_type == 'predicted_funding':\n    # 극단적인 펀딩비 값 확인 (소수점은 유지)\n    funding_col = 'funding_rate' if data_type == 'funding' else 'predicted_funding_rate'\n    if funding_col in df.columns:\n        extreme_funding = df[(df[funding_col] < -0.15) | (df[funding_col] > 0.15)]\n        if not extreme_funding.empty:\n            logger.warning(f\"{data_type}: {len(extreme_funding)}개 행에서 {funding_col} 값이 극단적입니다 (-0.15 미만 또는 0.15 초과). 데이터 유지됨.\")\n\nelif data_type == 'long_short_ratio':\n    # 롱/숏 퍼센티지 합이 100%인지 확인 (오차 범위 ±2%)\n    if 'long_percentage' in df.columns and 'short_percentage' in df.columns:\n        total = df['long_percentage'] + df['short_percentage']\n        invalid_rows = df[(total < 98) | (total > 102)]\n        if not invalid_rows.empty:\n            logger.warning(f\"{data_type}: {len(invalid_rows)}개 행에서 long_percentage와 short_percentage 합이 100%와 크게 다릅니다. 데이터 유지됨.\")\n\nreturn df\ndef process_ohlcv_data(data) -> pd.DataFrame:\n\"\"\"OHLCV 데이터 처리 (대체 소스용)\"\"\"\nhistory = extract_data_from_response(data, \"ohlcv\")\n\nscheme\n\n복사\nif not history:\n    return pd.DataFrame()\n    \ntry:\n    df = pd.DataFrame(history)\n    df = normalize_columns(df, \"ohlcv\")\n    \n    if 'datetime' not in df.columns and 't' in df.columns:\n        df['datetime'] = pd.to_datetime(df['t'], unit='s')\n    \n    # 필수 OHLCV 컬럼 확인\n    required_cols = ['open', 'high', 'low', 'close']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        logger.warning(f\"OHLCV 대체 소스에서 필수 컬럼 누락: {missing_cols}\")\n        return pd.DataFrame()\n        \n    return df\nexcept Exception as e:\n    logger.error(f\"OHLCV 대체 소스 데이터 처리 오류: {str(e)}\")\n    return pd.DataFrame()\ndef merge_dataframes(all_data_frames: Dict[str, pd.DataFrame], symbol: str) -> pd.DataFrame:\n\"\"\"여러 데이터프레임 병합 (소수점 보존)\"\"\"\nif not all_data_frames:\nreturn pd.DataFrame()\n\nscheme\n\n복사\n# 가능한 모든 기준 프레임 후보 (우선순위 순)\nbase_frame_candidates = [\"ohlcv\", \"long_short_ratio\", \"funding\", \"oi\"]\nbase_df = None\n\n# 우선순위에 따라 기준 프레임 선택\nfor candidate in base_frame_candidates:\n    if candidate in all_data_frames and not all_data_frames[candidate].empty:\n        base_df = all_data_frames[candidate].copy()\n        logger.info(f\"{symbol} 기준 프레임으로 {candidate} 선택 ({len(base_df)}행)\")\n        break\n\n# 기준 프레임이 없으면 빈 DataFrame 반환\nif base_df is None:\n    logger.warning(f\"{symbol} 병합할 기준 프레임이 없습니다.\")\n    return pd.DataFrame()\n\n# 심볼 정보 확인 및 추가\nif 'symbol' not in base_df.columns:\n    base_df['symbol'] = symbol\n    \n# 기본 프레임 컬럼 이름 확인 (중복 방지 위해)\n# 문제가 되는 t, o, h, l, c 등 짧은 컬럼명을 미리 표준화\nif 't' in base_df.columns:\n    base_df.rename(columns={'t': 'timestamp'}, inplace=True)\nfor col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n    if col_old in base_df.columns:\n        base_df.rename(columns={col_old: col_new}, inplace=True)\n        \n# 표준 컬럼명 리스트\nstd_cols = {\n    'open', 'high', 'low', 'close', 'volume',\n    'open_interest', 'funding_rate', 'predicted_funding_rate',\n    'liquidation', 'long_percentage', 'short_percentage', 'long_short_ratio',\n    'datetime', 'symbol'\n}\n\n# 각 데이터프레임을 병합 (왼쪽 조인 방식으로 변경)\nfor data_type, df in all_data_frames.items():\n    if data_type == list(base_frame_candidates)[0] or df.empty:\n        continue  # 기준 프레임이거나 빈 프레임은 건너뜀\n    \n    # 표준화된 컬럼명으로 변환\n    if 't' in df.columns:\n        df.rename(columns={'t': 'timestamp'}, inplace=True)\n    for col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n        if col_old in df.columns:\n            df.rename(columns={col_old: col_new}, inplace=True)\n    \n    # 기준 프레임에 없는 컬럼만 추출\n    columns_to_merge = []\n    for col in df.columns:\n        # 표준 컬럼 외의 컬럼은 고유한 이름으로 변경\n        if col not in std_cols and col in base_df.columns:\n            new_col = f\"{col}_{data_type}\"\n            df.rename(columns={col: new_col}, inplace=True)\n            columns_to_merge.append(new_col)\n        elif col not in base_df.columns and col != 'datetime' and col != 'symbol':\n            columns_to_merge.append(col)\n    \n    if not columns_to_merge and 'datetime' in df.columns:\n        # 추가할 컬럼이 없으면 다음으로\n        continue\n        \n    # 병합 (왼쪽 조인 방식으로 변경)\n    merge_df = df[['datetime'] + columns_to_merge].copy()\n    base_df = pd.merge(base_df, merge_df, on='datetime', how='left', suffixes=('', f'_{data_type}'))\n\n# 최종 정리\nbase_df.sort_values('datetime', inplace=True)\n\nreturn base_df\ndef fetch_historical_data(symbol: str, data_type: str, start_time: datetime, end_time: datetime, interval: str = \"5min\") -> pd.DataFrame:\n\"\"\"특정 심볼의 역사적 데이터 수집 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nlogger.warning(f\"{symbol} {data_type} 수집이 중단되었습니다 (종료 중)\")\nreturn pd.DataFrame()\n\nhy\n\n복사\nendpoint_map = {\n    \"ohlcv\": \"ohlcv_history\",\n    \"oi\": \"open_interest_history\",\n    \"funding\": \"funding_rate_history\",\n    \"predicted_funding\": \"predicted_funding_rate_history\",\n    \"liquidation\": \"liquidation_history\",\n    \"long_short_ratio\": \"long_short_ratio_history\"\n}\n\nif data_type not in endpoint_map:\n    logger.error(f\"지원되지 않는 데이터 타입: {data_type}\")\n    return pd.DataFrame()\n\n# 1. Coinalyze API 요청\nparams = {\n    \"symbols\": f\"{symbol}USDT_PERP.A\",\n    \"interval\": interval,\n    \"from\": int(start_time.timestamp()),\n    \"to\": int(end_time.timestamp())\n}\n\ntier = \"tier\" + str(random.randint(1, 4))  # 티어 무작위 선택\ndata = make_request(ENDPOINTS[endpoint_map[data_type]], params, tier)\n\n# API 응답 처리\nif data is not None:\n    # 데이터 추출\n    history = extract_data_from_response(data, data_type)\n    \n    # DataFrame 생성 및 처리\n    try:\n        df = pd.DataFrame(history)\n        \n        # 컬럼 정규화 (소수점 보존)\n        if not df.empty:\n            df = normalize_columns(df, data_type)\n            \n            # 심볼 정보 추가\n            if 'symbol' not in df.columns:\n                df['symbol'] = symbol\n            \n            logger.info(f\"Coinalyze {data_type} 데이터 수집 성공: {symbol}, {len(df)}행\")\n            return df\n            \n    except Exception as e:\n        logger.error(f\"{symbol} DataFrame 처리 중 오류: {str(e)}\")\n\n# 2. Coinalyze API 실패 시 CCXT 시도 (OHLCV 및 펀딩비만 지원)\nif data_type == \"ohlcv\":\n    logger.warning(f\"{symbol} Coinalyze OHLCV 데이터 수집 실패, CCXT로 시도...\")\n    df = ccxt_manager.fetch_ohlcv(symbol, interval, start_time, 1000)\n    if not df.empty:\n        return df\n\nelif data_type == \"funding\":\n    logger.warning(f\"{symbol} Coinalyze 펀딩비 데이터 수집 실패, CCXT로 시도...\")\n    df = ccxt_manager.fetch_funding_rate(symbol, interval, start_time, 500)\n    if not df.empty:\n        return df\n\n# 데이터를 가져오지 못한 경우\nlogger.warning(f\"{symbol} {data_type} 데이터 수집 실패 (Coinalyze 및 CCXT)\")\nreturn pd.DataFrame()\ndef load_existing_data(symbol: str) -> pd.DataFrame:\n\"\"\"기존 데이터 로드 (소수점 보존)\"\"\"\ncsv_path = LIVE_DIR / f\"{symbol}.csv\"\n\npython\n\n실행\n\n복사\nif not csv_path.exists():\n    return pd.DataFrame()\n    \ntry:\n    # 소수점 반올림 없이 데이터 로드\n    df = pd.read_csv(csv_path)\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    logger.info(f\"{symbol} 기존 데이터 로드: {len(df)}행\")\n    return df\nexcept Exception as e:\n    logger.error(f\"{symbol} 데이터 로드 실패: {str(e)}\")\n    \n    # 손상된 파일 처리\n    try:\n        # 손상된 파일 백업\n        corrupted_dir = DATA_DIR / \"corrupted\"\n        corrupted_dir.mkdir(parents=True, exist_ok=True)\n        \n        backup_file = corrupted_dir / f\"{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n        os.rename(csv_path, backup_file)\n        logger.warning(f\"손상된 파일 이동: {csv_path} -> {backup_file}\")\n    except Exception as backup_err:\n        logger.error(f\"손상된 파일 백업 실패: {str(backup_err)}\")\n    \n    return pd.DataFrame()\ndef save_data(symbol: str, new_data: pd.DataFrame) -> bool:\n\"\"\"데이터 저장 (기존 데이터와 병합, 소수점 보존)\"\"\"\nif new_data.empty:\nlogger.warning(f\"{symbol}: 저장할 새 데이터가 없습니다\")\nreturn False\n\nscheme\n\n복사\n# 종료 플래그 확인\nif shutdown_flag.is_set():\n    logger.warning(f\"{symbol} 데이터 저장이 중단되었습니다 (종료 중)\")\n    return False\n    \nLIVE_DIR.mkdir(parents=True, exist_ok=True)\ncsv_path = LIVE_DIR / f\"{symbol}.csv\"\n\ntry:\n    # 1. 기존 데이터 로드\n    existing_data = load_existing_data(symbol)\n    \n    # 2. 새 데이터와 기존 데이터 병합\n    if not existing_data.empty:\n        # 기존 데이터에 이미 있는 타임스탬프 제외\n        new_data_filtered = new_data[~new_data['datetime'].isin(existing_data['datetime'])]\n        \n        if not new_data_filtered.empty:\n            df = pd.concat([existing_data, new_data_filtered])\n        else:\n            logger.info(f\"{symbol}: 새 데이터가 이미 기존 데이터에 포함되어 있습니다\")\n            return True\n    else:\n        df = new_data\n        \n    # 3. 중복 컬럼 제거\n    column_mappings = {\n        'price_open': 'open',\n        'price_high': 'high',\n        'price_low': 'low',\n        'price_close': 'close'\n    }\n    \n    for old_col, new_col in column_mappings.items():\n        if old_col in df.columns and new_col in df.columns:\n            df = df.drop(old_col, axis=1)\n        elif old_col in df.columns:\n            df = df.rename(columns={old_col: new_col})\n        \n    # 4. 불필요한 컬럼 제거\n    columns_to_keep = [\n        'datetime', 'symbol',\n        'open', 'high', 'low', 'close', 'volume',\n        'open_interest',\n        'funding_rate', 'predicted_funding_rate',\n        'liquidation',\n        'long_percentage', 'short_percentage', 'long_short_ratio'\n    ]\n    \n    available_columns = [col for col in columns_to_keep if col in df.columns]\n    \n    # 핵심 컬럼이 없어도 최소한의 메타데이터만이라도 저장\n    min_required_cols = ['datetime', 'symbol']\n    if all(col in available_columns for col in min_required_cols):\n        df = df[available_columns]\n        \n        # 5. 중복 제거 및 정렬\n        df = df.drop_duplicates(subset=['datetime'])\n        df = df.sort_values('datetime')\n        \n        # 6. 저장 (소수점 보존)\n        # 파일 손상 방지를 위한, 임시 파일에 저장 후 이동\n        temp_path = csv_path.with_name(f\"{symbol}_temp.csv\")\n        \n        # 소수점 반올림 없이 저장 (float_format 인자 사용하지 않음)\n        df.to_csv(temp_path, index=False)\n        \n        # 임시 파일이 성공적으로 작성되었으면 원본 파일로 이동\n        if temp_path.exists():\n            if csv_path.exists():\n                csv_path.unlink()  # 기존 파일 삭제\n            temp_path.rename(csv_path)  # 임시 파일을 원본 파일로 이동\n        \n        # 핵심 데이터 누락 시 경고\n        if 'open' not in df.columns or df['open'].isnull().all():\n            logger.warning(f\"{symbol} 가격 데이터(OHLCV)가 모두 누락되었습니다. 메타데이터만 저장됨.\")\n        \n        logger.info(f\"{symbol} 데이터 저장 완료: 총 {len(df)}행 (새 데이터: {len(new_data)}행)\")\n        \n        # 7. 데이터 검증\n        time_diffs = pd.Series(df['datetime'].diff().dt.total_seconds() / 60)\n        gaps = time_diffs[time_diffs > 10]  # 10분 이상의 간격이 있는 경우\n        \n        if not gaps.empty:\n            logger.warning(f\"{symbol} 데이터 누락 발견:\")\n            for idx in gaps.index:\n                gap_start = df['datetime'].iloc[idx-1]\n                gap_end = df['datetime'].iloc[idx]\n                gap_minutes = gaps[idx]\n                logger.warning(f\"  {gap_start} ~ {gap_end} ({gap_minutes:.0f}분 간격)\")\n        \n        return True\n    else:\n        logger.error(f\"{symbol} 필수 컬럼({min_required_cols})이 없어 저장할 수 없습니다.\")\n        return False\n        \nexcept Exception as e:\n    logger.error(f\"{symbol} 데이터 저장 실패: {str(e)}\")\n    return False\ndef check_data_gaps(symbol: str, interval: str = \"5min\", days: int = 30) -> List[Tuple[datetime, datetime]]:\n\"\"\"데이터 누락 구간 확인\"\"\"\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nlogger.warning(f\"{symbol} 데이터 누락 검사가 중단되었습니다 (종료 중)\")\nreturn []\n\nini\n\n복사\ndf = load_existing_data(symbol)\n\nif df.empty:\n    # 데이터가 없으면 전체 기간을 누락으로 간주\n    end_time = datetime.now()\n    start_time = end_time - timedelta(days=days)\n    return [(start_time, end_time)]\n\n# 데이터 정렬\ndf = df.sort_values('datetime')\n\n# 타임라인 예상 범위 생성 (5분 간격)\nstart_time = max(df['datetime'].min(), datetime.now() - timedelta(days=days))\nend_time = datetime.now()\n\nexpected_timeline = pd.date_range(start=start_time, end=end_time, freq=interval)\nactual_timeline = df['datetime']\n\n# 누락된 시간대 찾기\nmissing_times = pd.DatetimeIndex(expected_timeline).difference(actual_timeline)\n\nif len(missing_times) == 0:\n    return []\n\n# 연속된 누락 구간 그룹화\ngaps = []\nif len(missing_times) > 0:\n    gap_start = missing_times[0]\n    for i in range(1, len(missing_times)):\n        curr_time = missing_times[i]\n        prev_time = missing_times[i-1]\n        # 다음 시간과 현재 시간의 차이가 간격보다 크면 새 그룹 시작\n        if (curr_time - prev_time).total_seconds() > pd.Timedelta(interval).total_seconds() * 2:\n            gaps.append((gap_start, prev_time))\n            gap_start = curr_time\n            \n    # 마지막 그룹 추가\n    gaps.append((gap_start, missing_times[-1]))\n\nreturn gaps\ndef fill_data_gaps(symbol: str, days: int = 30) -> bool:\n\"\"\"데이터 누락 구간 채우기 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nlogger.warning(f\"{symbol} 데이터 누락 채우기가 중단되었습니다 (종료 중)\")\nreturn False\n\npython\n\n실행\n\n복사\ngaps = check_data_gaps(symbol, days=days)\n\nif not gaps:\n    logger.info(f\"{symbol}: 데이터 누락 없음\")\n    return True\n\nlogger.info(f\"{symbol}: {len(gaps)}개의 데이터 누락 구간 발견\")\n\ndata_types = [\"ohlcv\", \"oi\", \"funding\", \"predicted_funding\", \"liquidation\", \"long_short_ratio\"]\nsuccessful_fills = 0\n\nfor start_time, end_time in gaps:\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 누락 채우기가 중단되었습니다 (종료 중)\")\n        break\n        \n    logger.info(f\"{symbol} 누락 구간 채우기: {start_time} ~ {end_time}\")\n    \n    # 데이터 타입별로 수집\n    all_data_frames = {}\n    gap_filled = False\n    \n    # 각 데이터 타입별로 누락 구간 데이터 수집\n    for data_type in data_types:\n        if shutdown_flag.is_set():\n            break\n            \n        df = fetch_historical_data(symbol, data_type, start_time, end_time)\n        if not df.empty:\n            all_data_frames[data_type] = df\n            gap_filled = True\n    \n    # 수집된 데이터가 있으면 병합하여 저장\n    if all_data_frames and not shutdown_flag.is_set():\n        # 데이터프레임 병합 (소수점 보존)\n        merged_df = merge_dataframes(all_data_frames, symbol)\n        \n        if not merged_df.empty:\n            # 저장 (소수점 보존)\n            if save_data(symbol, merged_df):\n                successful_fills += 1\n\nreturn successful_fills > 0\ndef process_symbol_data(symbol: str, interval: str = \"5min\", mode: str = \"realtime\") -> dict:\n\"\"\"한 심볼의 데이터 처리 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nreturn {\n\"symbol\": symbol,\n\"mode\": mode,\n\"success\": False,\n\"message\": \"종료 중\"\n}\n\njson\n\n복사\nresult = {\n    \"symbol\": symbol,\n    \"mode\": mode,\n    \"success\": False,\n    \"start_time\": None,\n    \"end_time\": None,\n    \"rows_added\": 0\n}\n\ntry:\n    if mode == \"backfill\":\n        # 과거 데이터 누락 검사 및 채우기\n        gaps_filled = fill_data_gaps(symbol)\n        result[\"success\"] = gaps_filled\n        return result\n        \n    # 실시간 데이터 수집\n    current_time = datetime.now()\n    \n    # 일반적으로 30분 전 데이터부터 수집 (완전한 캔들을 위해)\n    start_time = current_time - timedelta(minutes=30)\n    \n    # 각 데이터 타입별로 최신 데이터 수집\n    data_types = [\"ohlcv\", \"oi\", \"funding\", \"predicted_funding\", \"liquidation\", \"long_short_ratio\"]\n    all_data_frames = {}\n    \n    for data_type in data_types:\n        if shutdown_flag.is_set():\n            result[\"message\"] = \"종료 중\"\n            return result\n            \n        df = fetch_historical_data(symbol, data_type, start_time, current_time, interval)\n        if not df.empty:\n            all_data_frames[data_type] = df\n    \n    # 수집된 데이터가 있으면 병합하여 저장\n    if all_data_frames and not shutdown_flag.is_set():\n        # 데이터프레임 병합 (소수점 보존)\n        merged_df = merge_dataframes(all_data_frames, symbol)\n        \n        if not merged_df.empty:\n            # 저장 (소수점 보존)\n            save_success = save_data(symbol, merged_df)\n            \n            result.update({\n                \"success\": save_success,\n                \"start_time\": merged_df['datetime'].min().isoformat() if not merged_df.empty else None,\n                \"end_time\": merged_df['datetime'].max().isoformat() if not merged_df.empty else None,\n                \"rows_added\": len(merged_df)\n            })\n        \n    return result\n    \nexcept Exception as e:\n    logger.error(f\"{symbol} 처리 중 오류: {str(e)}\")\n    result[\"error\"] = str(e)\n    return result\ndef worker(task_queue, result_queue):\n\"\"\"작업자 스레드 함수\"\"\"\nwhile not shutdown_flag.is_set():\ntry:\n# 종료 확인\nif check_stop_file():\nbreak\n\napplescript\n\n복사\n        # 1초 타임아웃으로 큐에서 작업 가져오기\n        try:\n            task = task_queue.get(timeout=1)\n        except queue.Empty:\n            continue\n            \n        # 종료 신호 확인\n        if task is None:\n            task_queue.task_done()\n            break\n            \n        symbol, interval, mode = task\n        result = process_symbol_data(symbol, interval, mode)\n        result_queue.put(result)\n        task_queue.task_done()\n        \n    except Exception as e:\n        logger.error(f\"작업자 스레드 오류: {str(e)}\")\n        time.sleep(1)  # 오류 시 잠시 대기\n\nlogger.info(\"작업자 스레드 종료\")\ndef initialize_system():\n\"\"\"시스템 초기화 및 심볼 매핑 생성\"\"\"\n# 디렉토리 생성\nfor directory in [DATA_DIR, LIVE_DIR, BACKUP_DIR, ARCHIVE_DIR, LONG_TERM_DIR]:\ndirectory.mkdir(parents=True, exist_ok=True)\n\n\n복사\n# CCXT 매니저 초기화\nif not ccxt_manager.load_symbol_mappings():\n    # 저장된 매핑이 없으면 새로 생성\n    # 모든 심볼 목록 가져오기\n    symbols = get_all_symbols()\n    if symbols:\n        # 심볼 매핑 탐색\n        ccxt_manager.discover_symbol_mappings(symbols)\n\nlogger.info(f\"시스템 초기화 완료: {len(ccxt_manager.symbol_mappings)}개 심볼 매핑됨\")\ndef continuous_data_collection(interval: str = \"5min\", num_threads: int = 8):\n\"\"\"연속적인 데이터 수집 (멀티스레딩, Coinalyze 우선, CCXT 보완)\"\"\"\n# 시스템 초기화\ninitialize_system()\n\nlisp\n\n복사\n# 심볼 목록 로드\nsymbols = get_all_symbols()\nif not symbols:\n    logger.error(\"심볼 목록을 가져올 수 없습니다\")\n    return\n\nlogger.info(f\"데이터 수집 시작 ({interval} 간격): {len(symbols)}개 심볼\")\n\n# 작업 큐와 결과 큐 설정\ntask_queue = queue.Queue()\nresult_queue = queue.Queue()\n\n# 작업자 스레드 시작\nthreads = []\nfor _ in range(num_threads):\n    t = threading.Thread(target=worker, args=(task_queue, result_queue))\n    t.daemon = True  # 데몬 스레드로 설정\n    t.start()\n    threads.append(t)\n\nlast_backup_check = datetime.now()\ncollection_start_time = time.time()\n\nwhile not shutdown_flag.is_set():\n    try:\n        # 종료 확인\n        if check_stop_file():\n            break\n        \n        # 모든 심볼에 대한 작업 생성\n        for symbol in symbols:\n            if shutdown_flag.is_set():\n                break\n            task_queue.put((symbol, interval, \"realtime\"))\n        \n        # 모든 작업이 완료될 때까지 대기 (최대 대기 시간 설정)\n        wait_start = time.time()\n        while not task_queue.empty():\n            if shutdown_flag.is_set() or time.time() - wait_start > 600:  # 10분 초과 대기 시 타임아웃\n                break\n            time.sleep(1)\n            check_stop_file()  # 정기적으로 종료 파일 확인\n        \n        # 작업 결과 처리\n        results_processed = 0\n        success_count = 0\n        fail_count = 0\n        \n        while not result_queue.empty() and results_processed < 1000:  # 최대 1000개 결과만 처리\n            result = result_queue.get()\n            if result[\"success\"]:\n                logger.info(f\"{result['symbol']} 데이터 수집 성공: {result.get('rows_added', 0)}행 추가됨\")\n                success_count += 1\n            else:\n                logger.warning(f\"{result['symbol']} 데이터 수집 실패\")\n                fail_count += 1\n            results_processed += 1\n        \n        logger.info(f\"데이터 수집 결과: 성공 {success_count}개, 실패 {fail_count}개\")\n        \n        # 수집 완료 시간 계산\n        collection_duration = time.time() - collection_start_time\n        \n        # 종료 확인\n        if shutdown_flag.is_set():\n            break\n        \n        # 다음 수집까지 대기\n        remaining_wait = max(0, COLLECTION_INTERVAL - collection_duration)\n        if remaining_wait > 0:\n            logger.info(f\"다음 데이터 수집까지 {remaining_wait:.1f}초 대기...\")\n            \n            # 대기 중에도 정기적으로 종료 확인\n            wait_start = time.time()\n            while time.time() - wait_start < remaining_wait:\n                if shutdown_flag.is_set() or check_stop_file():\n                    break\n                time.sleep(1)\n        \n        # 새로운 주기 시작\n        collection_start_time = time.time()\n        \n    except KeyboardInterrupt:\n        logger.info(\"사용자에 의해 프로그램이 중단되었습니다.\")\n        shutdown_flag.set()\n        break\n    except Exception as e:\n        logger.error(f\"데이터 수집 중 오류 발생: {str(e)}\")\n        time.sleep(60)  # 오류 발생 시 1분 대기 후 재시도\n\n# 안전한 종료 처리\nlogger.info(\"프로그램 종료 중...\")\n\n# 작업자 스레드 종료\nfor _ in threads:\n    task_queue.put(None)  # 종료 신호\n\n# 각 스레드가 안전하게 종료될 때까지 최대 30초 대기\nfor t in threads:\n    t.join(timeout=30)\n\nlogger.info(\"데이터 수집이 완전히 종료되었습니다.\")\ndef main():\n\"\"\"메인 함수\"\"\"\nlogger.info(\"암호화폐 데이터 통합 관리 시스템 시작\")\n\nrouteros\n\n복사\ntry:\n    # 기존 종료 파일 확인 및 제거\n    if os.path.exists(\"stop.signal\"):\n        os.remove(\"stop.signal\")\n        logger.info(\"기존 종료 파일 제거됨\")\n    \n    # 시스템 초기화\n    initialize_system()\n    \n    # 실시간 데이터 수집 (5분 간격)\n    if not shutdown_flag.is_set():\n        logger.info(\"실시간 데이터 수집 시작...\")\n        continuous_data_collection(interval=\"5min\", num_threads=8)\n    \nexcept KeyboardInterrupt:\n    logger.info(\"사용자에 의해 프로그램이 중단되었습니다.\")\n    shutdown_flag.set()\nexcept Exception as e:\n    logger.error(f\"프로그램 실행 중 오류 발생: {str(e)}\")\n    shutdown_flag.set()\nfinally:\n    # 종료 마무리\n    if shutdown_flag.is_set():\n        logger.info(\"프로그램 종료 처리 완료\")\ndef create_stop_signal():\n\"\"\"프로그램 종료 신호 파일 생성\"\"\"\ntry:\nwith open(\"stop.signal\", \"w\") as f:\nf.write(f\"Stop requested at {datetime.now().isoformat()}\")\nprint(\"종료 신호가 생성되었습니다. 프로그램이 안전하게 종료됩니다.\")\nexcept Exception as e:\nprint(f\"종료 신호 생성 실패: {str(e)}\")\n\nif name == \"main\":\nimport sys\n\n\n복사\n# 명령행 인수 처리\nif len(sys.argv) > 1 and sys.argv[1] == \"stop\":\n    create_stop_signal()\nelse:\n    main()\n\n대충 이코드였는데 찾아와","commandType":4},{"text":"삭제한거 복원은 안되는거야?","commandType":4},{"text":"그러니까 exe를 파일화하라고","commandType":4},{"text":"해봐","commandType":4},{"text":"2번으로 해보자고","commandType":4},{"text":"2번으로 하자고","commandType":4},{"text":"Microsoft Windows [Version 10.0.26100.3194]\n(c) Microsoft Corporation. All rights reserved.\n\nC:\\Windows\\System32>cd /d F:\\Oi funding\n\nF:\\Oi funding>curl -LO https://github.com/extremecoders-re/pyinstxtractor/archive/refs/heads/master.ziptar -xf master.zip\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0curl: (5) Could not resolve proxy: f\ncurl: (5) Could not resolve proxy: f\n\nF:\\Oi funding>cd pyinstxtractor-masterpython pyinstxtractor.py ..\\dist\\crypto_data_manager.exe\n지정된 경로를 찾을 수 없습니다.\n\nF:\\Oi funding>cd crypto_data_manager_extractedpython -m decompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\n지정된 경로를 찾을 수 없습니다.\n\nF:\\Oi funding>\n\n\n\n\n\n\n\n\n\n체크해봐","commandType":4},{"text":"F:\\Oi funding>curl -L -o master.zip https://github.com/extremecoders-re/pyinstxtractor/archive/refs/heads/master.zip\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 19000    0 19000    0     0  34749      0 --:--:-- --:--:-- --:--:-- 34749\n\nF:\\Oi funding>curl -L -o master.zip https://github.com/extremecoders-re/pyinstxtractor/archive/refs/heads/master.ziptar -xf master.zipcd pyinstxtractor-masterpython pyinstxtractor.py ..\\dist\\crypto_data_manager.exe\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0curl: (5) Could not resolve proxy: f\ncurl: (5) Could not resolve proxy: f\ncurl: (5) Could not resolve proxy: f\ncurl: (5) Could not resolve proxy: f\ncurl: (3) URL rejected: Bad hostname\n\nF:\\Oi funding>\n\n\n\n\n","commandType":4},{"text":"파워쉘에서하라고???????????","commandType":4},{"text":"\nF:\\Oi funding>tar -xf master.zip\n\nF:\\Oi funding>powershell Expand-Archive master.zip .\nExpandArchiveHelper : 'F:\\Oi funding\\pyinstxtractor-master\\LICENSE' 파일이 이미 있으므로 보관 파일 'F:\\Oi funding\\ma\nster.zip' 콘텐츠를 확장하는 동안 'F:\\Oi funding\\pyinstxtractor-master\\LICENSE' 파일을 만들지 못했습니다. 보관 파일을\n 확장할 때 기존 디렉터리 'F:\\Oi funding\\pyinstxtractor-master\\LICENSE' 콘텐츠를 덮어쓰려는 경우 -Force 매개 변수를\n사용하십시오.\n위치 C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\\Microsoft.PowerShell.Archive\\Microsoft.PowerShell.Archive.ps\nm1:397 문자:17\n+ ...             ExpandArchiveHelper $resolvedSourcePaths $resolvedDestina ...\n+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : InvalidOperation: (F:\\Oi funding\\p...-master\\LICENSE:String) [Write-Error], IOExcepti\n   on\n    + FullyQualifiedErrorId : ExpandArchiveFileExists,ExpandArchiveHelper\n\nExpandArchiveHelper : 'F:\\Oi funding\\pyinstxtractor-master\\README.md' 파일이 이미 있으므로 보관 파일 'F:\\Oi funding\\\nmaster.zip' 콘텐츠를 확장하는 동안 'F:\\Oi funding\\pyinstxtractor-master\\README.md' 파일을 만들지 못했습니다. 보관 파\n일을 확장할 때 기존 디렉터리 'F:\\Oi funding\\pyinstxtractor-master\\README.md' 콘텐츠를 덮어쓰려는 경우 -Force 매개 변\n수를 사용하십시오.\n위치 C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\\Microsoft.PowerShell.Archive\\Microsoft.PowerShell.Archive.ps\nm1:397 문자:17\n+ ...             ExpandArchiveHelper $resolvedSourcePaths $resolvedDestina ...\n+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : InvalidOperation: (F:\\Oi funding\\p...aster\\README.md:String) [Write-Error], IOExcepti\n   on\n    + FullyQualifiedErrorId : ExpandArchiveFileExists,ExpandArchiveHelper\n\nExpandArchiveHelper : 'F:\\Oi funding\\pyinstxtractor-master\\pyinstxtractor.py' 파일이 이미 있으므로 보관 파일 'F:\\Oi\nfunding\\master.zip' 콘텐츠를 확장하는 동안 'F:\\Oi funding\\pyinstxtractor-master\\pyinstxtractor.py' 파일을 만들지 못\n했습니다. 보관 파일을 확장할 때 기존 디렉터리 'F:\\Oi funding\\pyinstxtractor-master\\pyinstxtractor.py' 콘텐츠를 덮어\n쓰려는 경우 -Force 매개 변수를 사용하십시오.\n위치 C:\\WINDOWS\\system32\\WindowsPowerShell\\v1.0\\Modules\\Microsoft.PowerShell.Archive\\Microsoft.PowerShell.Archive.ps\nm1:397 문자:17\n+ ...             ExpandArchiveHelper $resolvedSourcePaths $resolvedDestina ...\n+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : InvalidOperation: (F:\\Oi funding\\p...instxtractor.py:String) [Write-Error], IOExcepti\n   on\n    + FullyQualifiedErrorId : ExpandArchiveFileExists,ExpandArchiveHelper\n\n\nF:\\Oi funding>cd pyinstxtractor-master\n\nF:\\Oi funding\\pyinstxtractor-master>tar -xf master.zip\ntar: Error opening archive: Failed to open 'master.zip'\n\nF:\\Oi funding\\pyinstxtractor-master>\n\n해결해라","commandType":4},{"text":"F:\\Oi funding>cd pyinstxtractor-master\n\nF:\\Oi funding\\pyinstxtractor-master>tar -xf master.zip\ntar: Error opening archive: Failed to open 'master.zip'\n\nF:\\Oi funding\\pyinstxtractor-master>cd ..rmdir /s /q pyinstxtractor-masterdel master.zip\n지정된 경로를 찾을 수 없습니다.\n\nF:\\Oi funding\\pyinstxtractor-master>curl -L -o master.zip https://github.com/extremecoders-re/pyinstxtractor/archive/refs/heads/master.ziptar -xf master.zip\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0curl: (5) Could not resolve proxy: f\ncurl: (5) Could not resolve proxy: f\n\nF:\\Oi funding\\pyinstxtractor-master>cd pyinstxtractor-master\n지정된 경로를 찾을 수 없습니다.\n\nF:\\Oi funding\\pyinstxtractor-master>python pyinstxtractor.py ..\\dist\\crypto_data_manager.exe\n[+] Processing ..\\dist\\crypto_data_manager.exe\n[+] Pyinstaller version: 2.1+\n[+] Python version: 3.11\n[+] Length of package: 87965860 bytes\n[+] Found 1971 files in CArchive\n[+] Beginning extraction...please standby\n[+] Possible entry point: pyiboot01_bootstrap.pyc\n[+] Possible entry point: pyi_rth_inspect.pyc\n[+] Possible entry point: pyi_rth_cryptography_openssl.pyc\n[+] Possible entry point: pyi_rth_pkgutil.pyc\n[+] Possible entry point: pyi_rth_multiprocessing.pyc\n[+] Possible entry point: pyi_rth__tkinter.pyc\n[+] Possible entry point: pyi_rth_pkgres.pyc\n[+] Possible entry point: pyi_rth_mplconfig.pyc\n[+] Possible entry point: pyi_rth_setuptools.pyc\n[+] Possible entry point: crypto_data_manager.pyc\n[+] Found 2095 files in PYZ archive\n[+] Successfully extracted pyinstaller archive: ..\\dist\\crypto_data_manager.exe\n\nYou can now use a python decompiler on the pyc files within the extracted directory\n\nF:\\Oi funding\\pyinstxtractor-master>\n\n된거야?","commandType":4},{"text":"\nF:\\Oi funding\\pyinstxtractor-master>cd crypto_data_manager_exe_extractedpython -m decompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\n지정된 경로를 찾을 수 없습니다.\n","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master>dir\n F 드라이브의 볼륨에는 이름이 없습니다.\n 볼륨 일련 번호: 9C33-B2A6\n\n F:\\Oi funding\\pyinstxtractor-master 디렉터리\n\n2025-03-13  오전 09:47    <DIR>          .\n2025-03-13  오전 09:40    <DIR>          ..\n2025-03-13  오전 09:36    <DIR>          crypto_data_manager.exe_extracted\n2025-03-13  오전 09:47                 0 crypto_data_manager_decompiled.py\n2025-02-22  오후 06:58            35,149 LICENSE\n2025-02-22  오후 06:58            17,542 pyinstxtractor.py\n2025-02-22  오후 06:58             2,928 README.md\n               4개 파일              55,619 바이트\n               3개 디렉터리  222,136,430,592 바이트 남음","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master>cd crypto_data_manager.exe_extractedpython -m decompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\n지정된 경로를 찾을 수 없습니다.\n","commandType":4},{"text":"다음","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>python -m decompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nC:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named decompyle6\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>dir crypto_data_manager.pyc\n F 드라이브의 볼륨에는 이름이 없습니다.\n 볼륨 일련 번호: 9C33-B2A6\n\n F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted 디렉터리\n\n2025-03-13  오전 09:46            73,924 crypto_data_manager.pyc\n               1개 파일              73,924 바이트\n               0개 디렉터리  222,142,083,072 바이트 남음\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>pip install decompyle6\nERROR: Could not find a version that satisfies the requirement decompyle6 (from versions: none)\nERROR: No matching distribution found for decompyle6\n\n[notice] A new release of pip is available: 24.0 -> 25.0.1\n[notice] To update, run: C:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n에러발생","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>pip install uncompyle6\nCollecting uncompyle6\n  Using cached uncompyle6-3.9.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: click in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uncompyle6) (8.1.8)\nCollecting spark-parser<1.9.0,>=1.8.9 (from uncompyle6)\n  Using cached spark_parser-1.8.9-py3-none-any.whl.metadata (3.0 kB)\nCollecting xdis<6.2.0,>=6.1.0 (from uncompyle6)\n  Using cached xdis-6.1.3-py310-none-any.whl.metadata (11 kB)\nRequirement already satisfied: six>=1.10.0 in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xdis<6.2.0,>=6.1.0->uncompyle6) (1.17.0)\nRequirement already satisfied: colorama in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->uncompyle6) (0.4.6)\nUsing cached uncompyle6-3.9.2-py3-none-any.whl (358 kB)\nUsing cached spark_parser-1.8.9-py3-none-any.whl (17 kB)\nUsing cached xdis-6.1.3-py310-none-any.whl (223 kB)\nInstalling collected packages: xdis, spark-parser, uncompyle6\n  WARNING: The script pydisasm.exe is installed in 'C:\\Users\\woojupapa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The scripts uncompyle6-tokenize.exe and uncompyle6.exe are installed in 'C:\\Users\\woojupapa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed spark-parser-1.8.9 uncompyle6-3.9.2 xdis-6.1.3\n\n[notice] A new release of pip is available: 24.0 -> 25.0.1\n[notice] To update, run: C:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>pip install pycdc\nERROR: Could not find a version that satisfies the requirement pycdc (from versions: none)\nERROR: No matching distribution found for pycdc\n\n[notice] A new release of pip is available: 24.0 -> 25.0.1\n[notice] To update, run: C:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>python -m uncompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nC:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named uncompyle6.__main__; 'uncompyle6' is a package and cannot be directly executed\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>C:\\Users\\woojupapa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts\\uncompyle6.exe crypto_data_manager.pyc > crypto_data_manager_decompiled.py\n\n# Unsupported bytecode in file crypto_data_manager.pyc\n# Unsupported Python version, 3.11, for decompilation\n","commandType":4},{"text":"다른방법으로 안되?","commandType":4},{"text":"pip install decompyle3","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>pip install decompyle3\nCollecting decompyle3\n  Downloading decompyle3-3.9.2-py3-none-any.whl.metadata (9.6 kB)\nRequirement already satisfied: click in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from decompyle3) (8.1.8)\nRequirement already satisfied: spark-parser<1.9.0,>=1.8.9 in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from decompyle3) (1.8.9)\nRequirement already satisfied: xdis<6.2.0,>=6.1.1 in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from decompyle3) (6.1.3)\nRequirement already satisfied: six>=1.10.0 in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xdis<6.2.0,>=6.1.1->decompyle3) (1.17.0)\nRequirement already satisfied: colorama in c:\\users\\woojupapa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->decompyle3) (0.4.6)\nDownloading decompyle3-3.9.2-py3-none-any.whl (272 kB)\n   ---------------------------------------- 272.5/272.5 kB 5.6 MB/s eta 0:00:00\nInstalling collected packages: decompyle3\n  WARNING: The scripts decompyle3-code.exe, decompyle3-tokens.exe and decompyle3.exe are installed in 'C:\\Users\\woojupapa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed decompyle3-3.9.2\n\n[notice] A new release of pip is available: 24.0 -> 25.0.1\n[notice] To update, run: C:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>\n명령 구문이 올바르지 않습니다.\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>python -m decompyle3.bin.decompile_file crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nC:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named decompyle3.bin.decompile_file\n","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>python -m decompyle3.bin.decompile_file crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nC:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named decompyle3.bin.decompile_file\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>python -m decompyle3 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nC:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named decompyle3.__main__; 'decompyle3' is a package and cannot be directly executed\n","commandType":4},{"text":"설치완료했어","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>python -m decompyle3 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nC:\\Users\\woojupapa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named decompyle3.__main__; 'decompyle3' is a package and cannot be directly executed\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>py -3.7 -m pip install uncompyle6cd F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extractedpy -3.7 -m uncompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\n\nUsage:\n  C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n  C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n  C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install [options] [-e] <vcs project url> ...\n  C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install [options] [-e] <local project path> ...\n  C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install [options] <archive url/path> ...\n\nno such option: -3\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>py -3.7 -m pip install uncompyle6\nCollecting uncompyle6\n  Downloading uncompyle6-3.9.2-py3-none-any.whl (358 kB)\n     |████████████████████████████████| 358 kB ...\nCollecting xdis<6.2.0,>=6.1.0\n  Downloading xdis-6.1.3-py37-none-any.whl (223 kB)\n     |████████████████████████████████| 223 kB 6.4 MB/s\nCollecting click\n  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n     |████████████████████████████████| 98 kB ...\nCollecting spark-parser<1.9.0,>=1.8.9\n  Downloading spark_parser-1.8.9-py3-none-any.whl (17 kB)\nCollecting six>=1.10.0\n  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nCollecting colorama; platform_system == \"Windows\"\n  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nCollecting importlib-metadata; python_version < \"3.8\"\n  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\nCollecting zipp>=0.5\n  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\nCollecting typing-extensions>=3.6.4; python_version < \"3.8\"\n  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nInstalling collected packages: six, colorama, zipp, typing-extensions, importlib-metadata, click, xdis, spark-parser, uncompyle6\n  WARNING: The script pydisasm.exe is installed in 'C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n  WARNING: The scripts uncompyle6-tokenize.exe and uncompyle6.exe are installed in 'C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\Scripts' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\nSuccessfully installed click-8.1.8 colorama-0.4.6 importlib-metadata-6.7.0 six-1.17.0 spark-parser-1.8.9 typing-extensions-4.7.1 uncompyle6-3.9.2 xdis-6.1.3 zipp-3.15.0\nWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\nYou should consider upgrading via the 'C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n\nF:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>py -3.7 -m uncompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nTraceback (most recent call last):\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 183, in _run_module_as_main\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 142, in _get_module_details\n    return _get_module_details(pkg_main_name, error)\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 109, in _get_module_details\n    __import__(pkg_name)\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\uncompyle6\\__init__.py\", line 39, in <module>\n    from uncompyle6.semantics import fragments, pysource\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\uncompyle6\\semantics\\fragments.py\", line 71, in <module>\n    from spark_parser import DEFAULT_DEBUG as PARSER_DEFAULT_DEBUG\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spark_parser\\__init__.py\", line 12, in <module>\n    from spark_parser.spark import DEFAULT_DEBUG\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spark_parser\\spark.py\", line 25, in <module>\n    import os, pickle, re, sys\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\pickle.py\", line 33, in <module>\n    from struct import pack, unpack\nImportError: bad magic number in 'struct': b'\\xa7\\r\\r\\n'\n","commandType":4},{"text":"F:\\Oi funding\\pyinstxtractor-master\\crypto_data_manager.exe_extracted>py -3.7 -m uncompyle6 crypto_data_manager.pyc > crypto_data_manager_decompiled.py\nTraceback (most recent call last):\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 183, in _run_module_as_main\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 142, in _get_module_details\n    return _get_module_details(pkg_main_name, error)\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\runpy.py\", line 109, in _get_module_details\n    __import__(pkg_name)\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\uncompyle6\\__init__.py\", line 53, in <module>\n    import uncompyle6.semantics.pysource\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\uncompyle6\\semantics\\pysource.py\", line 138, in <module>\n    from xdis import iscode, COMPILER_FLAG_BIT, sysinfo2float\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\xdis\\__init__.py\", line 73, in <module>\n    from xdis.load import (\n  File \"C:\\Users\\woojupapa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\xdis\\load.py\", line 17, in <module>\n    from struct import unpack, pack\nImportError: bad magic number in 'struct': b'\\xa7\\r\\r\\n'\n","commandType":4},{"text":"\"\"\"\n암호화폐 데이터 통합 관리 시스템 (CCXT 데이터 보완 및 장기 보관 기능 추가)\n- 과거 데이터 누락 검사 및 복구 (CCXT로 보완)\n- 실시간 데이터 수집 및 저장\n- 단일 CSV 파일에 시간순 정렬 저장\n- 자동 심볼 추가 및 관리\n- 안전한 종료 메커니즘\n- 데이터 장기 보관\n- 소수점 정확도 유지\n\"\"\"\n\nimport os\nimport logging\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nimport time\nimport base64\nimport signal\nimport traceback\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Union\nimport random\nimport threading\nimport queue\nimport ccxt\n\n# 안전한 종료를 위한 전역 플래그\nshutdown_flag = threading.Event()\n\n# 기본 설정\nBASE_URL = \"https://api.coinalyze.net/v1\"\nDATA_DIR = Path(\"data\")\nLIVE_DIR = DATA_DIR / \"live\"\nBACKUP_DIR = DATA_DIR / \"backup\"\nARCHIVE_DIR = DATA_DIR / \"archive\"\nLONG_TERM_DIR = DATA_DIR / \"longterm\"\n\n# API 요청 간격 설정 (초)\nAPI_REQUEST_INTERVAL = 0.5  # API 요청 사이 대기 시간 (초)\nCOLLECTION_INTERVAL = 300   # 데이터 수집 간격 (초) - 5분\n\n# API 엔드포인트\nENDPOINTS = {\n    \"exchanges\": \"/exchanges\",\n    \"future_markets\": \"/future-markets\",\n    \"spot_markets\": \"/spot-markets\",\n    \"open_interest\": \"/open-interest\",\n    \"funding_rate\": \"/funding-rate\",\n    \"predicted_funding_rate\": \"/predicted-funding-rate\",\n    \"open_interest_history\": \"/open-interest-history\",\n    \"funding_rate_history\": \"/funding-rate-history\",\n    \"predicted_funding_rate_history\": \"/predicted-funding-rate-history\",\n    \"liquidation_history\": \"/liquidation-history\",\n    \"long_short_ratio_history\": \"/long-short-ratio-history\",\n    \"ohlcv_history\": \"/ohlcv-history\"\n}\n\n# API 키 설정 (16개 키를 4개 티어로 구분)\nAPI_KEYS = {\n    \"tier1\": [\n        \"0d0ba171-4185-4a58-bc02-8c8627cd1f54\",\n        \"e629efa9-68b0-4b6b-b794-fa2d9e379b79\",\n        \"c9779cfd-a85c-4e58-a378-9474174a075e\",\n        \"58a39d7b-84e5-4852-8375-90de055cba18\"\n    ],\n    \"tier2\": [\n        \"7b80a59d-f0f9-4a83-81f7-1314dbdd9dc7\",\n        \"2b951698-64e7-4a86-9930-c503d4e29e54\",\n        \"10b24c79-ddd1-4046-94b6-0af0d23b241e\",\n        \"fac040b5-5043-4351-b8f3-09c1d8cfd78f\"\n    ],\n    \"tier3\": [\n        \"fe1caf7e-5e27-4f0c-9d14-2a4611db625f\",\n        \"fc9e5080-9607-46e8-b48c-deca57591990\",\n        \"a82fd8a4-873f-4619-9b9f-d8a29373b5b8\",\n        \"6431161c-8815-4d18-846d-55e00863682e\"\n    ],\n    \"tier4\": [\n        \"07cc6de0-0d02-41b8-acdc-dd1284bf5730\",\n        \"f2842bed-d43e-4152-a0b5-68d9d9ed30b0\",\n        \"013d0da7-76ea-4699-97d7-6e0f66857939\",\n        \"7906d7bf-b493-42ad-a578-0cd13c6a070c\"\n    ]\n}\n\n# CCXT 설정 - 공개 데이터만 사용하므로 API 키 불필요\nCCXT_EXCHANGES = [\n    \"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"\n]\n\n# 로깅 설정\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler(\"crypto_data_manager.log\", encoding='utf-8')\n    ]\n)\nlogger = logging.getLogger(__name__)\n\ndef signal_handler(sig, frame):\n    \"\"\"시스템 종료 신호를 처리하는 핸들러\"\"\"\n    logger.warning(\"종료 신호를 받았습니다. 안전하게 종료합니다...\")\n    shutdown_flag.set()\n\n# 시그널 핸들러 등록\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\n\ndef check_stop_file():\n    \"\"\"종료 파일 확인\"\"\"\n    if os.path.exists(\"stop.signal\"):\n        logger.warning(\"종료 파일이 감지되었습니다. 안전하게 종료합니다...\")\n        try:\n            os.remove(\"stop.signal\")  # 종료 파일 삭제\n        except:\n            pass\n        shutdown_flag.set()\n        return True\n    return False\n\nclass APIKeyManager:\n    \"\"\"API 키 관리 클래스\"\"\"\n    def __init__(self):\n        self.tier_keys = API_KEYS\n        self.current_key_index = {tier: 0 for tier in API_KEYS.keys()}\n        self.key_locks = {tier: threading.Lock() for tier in API_KEYS.keys()}\n        self.key_rate_limits = {key: {'last_used': 0, 'count': 0} for tier in API_KEYS.values() for key in tier}\n        self.rate_limit_reset_time = time.time() + 60  # 1분 후 리셋\n        \n    def get_next_key(self, tier: str) -> str:\n        \"\"\"라운드 로빈 방식으로 다음 API 키 반환\"\"\"\n        if tier not in self.tier_keys or not self.tier_keys[tier]:\n            available_tiers = [t for t in self.tier_keys if self.tier_keys[t]]\n            if not available_tiers:\n                raise ValueError(\"사용 가능한 API 키가 없습니다\")\n            tier = random.choice(available_tiers)\n            \n        with self.key_locks[tier]:\n            keys = self.tier_keys[tier]\n            current_index = self.current_key_index[tier]\n            \n            # 현재 시간이 리셋 시간을 초과했는지 확인\n            current_time = time.time()\n            if current_time > self.rate_limit_reset_time:\n                # 사용량 카운터 리셋\n                for key_info in self.key_rate_limits.values():\n                    key_info['count'] = 0\n                self.rate_limit_reset_time = current_time + 60  # 다음 1분 후 리셋\n            \n            # 사용량이 가장 적은 키를 찾아 반환\n            min_count = float('inf')\n            selected_key = None\n            \n            for i in range(len(keys)):\n                idx = (current_index + i) % len(keys)\n                key = keys[idx]\n                \n                if self.key_rate_limits[key]['count'] < min_count:\n                    min_count = self.key_rate_limits[key]['count']\n                    selected_key = key\n                    self.current_key_index[tier] = (idx + 1) % len(keys)\n            \n            # 키 사용량 기록\n            self.key_rate_limits[selected_key]['last_used'] = current_time\n            self.key_rate_limits[selected_key]['count'] += 1\n            \n            # API 요청 간격 조절을 위한 대기\n            last_used = self.key_rate_limits[selected_key]['last_used']\n            if current_time - last_used < API_REQUEST_INTERVAL:\n                time.sleep(API_REQUEST_INTERVAL - (current_time - last_used))\n            \n            return selected_key\n\nclass CcxtManager:\n    \"\"\"CCXT 거래소 관리 및 심볼 매핑 클래스\"\"\"\n    def __init__(self):\n        self.exchanges = {}\n        self.symbol_mappings = {}  # base_symbol -> {exchange_id -> ccxt_symbol}\n        self.exchange_priority = [\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"]\n        self.init_exchanges()\n        self.supported_symbols = set()  # CCXT에서 지원하는 심볼 목록\n        self.timeframe_map = {\n            \"1min\": \"1m\", \"5min\": \"5m\", \"15min\": \"15m\", \"30min\": \"30m\",\n            \"1h\": \"1h\", \"4h\": \"4h\", \"1d\": \"1d\", \"1w\": \"1w\"\n        }\n        \n    def init_exchanges(self):\n        \"\"\"CCXT 거래소 초기화 (API 키 없이)\"\"\"\n        for exchange_id in CCXT_EXCHANGES:\n            try:\n                if exchange_id in ccxt.exchanges:\n                    exchange_class = getattr(ccxt, exchange_id)\n                    self.exchanges[exchange_id] = exchange_class({\n                        'enableRateLimit': True,\n                        'options': {\n                            'defaultType': 'future'  # 선물 거래 기본 설정\n                        }\n                    })\n                    logger.info(f\"CCXT {exchange_id} 거래소 초기화 성공\")\n            except Exception as e:\n                logger.error(f\"CCXT {exchange_id} 거래소 초기화 실패: {str(e)}\")\n    \n    def load_all_markets(self):\n        \"\"\"모든 거래소의 시장 정보 로드\"\"\"\n        for exchange_id, exchange in self.exchanges.items():\n            try:\n                exchange.load_markets()\n                logger.info(f\"{exchange_id} 시장 정보 로드 성공: {len(exchange.symbols)}개 심볼\")\n            except Exception as e:\n                logger.error(f\"{exchange_id} 시장 정보 로드 실패: {str(e)}\")\n    \n    def discover_symbol_mappings(self, base_symbols):\n        \"\"\"여러 기본 심볼에 대한 CCXT 매핑 자동 탐색\"\"\"\n        logger.info(f\"CCXT 심볼 매핑 탐색 시작: {len(base_symbols)}개 심볼\")\n        \n        # 거래소 시장 정보 로드\n        self.load_all_markets()\n        \n        # 각 심볼별 매핑 탐색\n        for base_symbol in base_symbols:\n            self.discover_symbol_mapping(base_symbol)\n        \n        # 매핑 정보 저장\n        self.save_symbol_mappings()\n        \n        logger.info(f\"CCXT 심볼 매핑 탐색 완료: {len(self.symbol_mappings)}개 심볼\")\n    \n    def discover_symbol_mapping(self, base_symbol):\n        \"\"\"단일 기본 심볼에 대한 CCXT 매핑 탐색\"\"\"\n        if base_symbol in self.symbol_mappings:\n            return\n            \n        self.symbol_mappings[base_symbol] = {}\n        \n        # 거래소별 검색 패턴\n        search_patterns = {\n            \"binance\": [\n                f\"{base_symbol}/USDT:USDT\",  # 영구 선물\n                f\"{base_symbol}USDT_PERP\",\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"bybit\": [\n                f\"{base_symbol}USDT\",        # 영구 선물\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"okx\": [\n                f\"{base_symbol}-USDT-SWAP\",  # 영구 선물\n                f\"{base_symbol}/USDT:USDT\",\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"kucoin\": [\n                f\"{base_symbol}USDTM\",       # 영구 선물\n                f\"{base_symbol}-USDT\",       # 영구 선물 (다른 형식)\n                f\"{base_symbol}/USDT\"        # 현물\n            ],\n            \"bitget\": [\n                f\"{base_symbol}USDT_UMCBL\",  # 영구 선물\n                f\"{base_symbol}/USDT\"        # 현물\n            ]\n        }\n        \n        # 거래소 우선순위대로 검색\n        for exchange_id in self.exchange_priority:\n            if exchange_id not in self.exchanges:\n                continue\n                \n            exchange = self.exchanges[exchange_id]\n            patterns = search_patterns.get(exchange_id, [f\"{base_symbol}/USDT\"])\n            \n            for pattern in patterns:\n                found = False\n                for symbol in exchange.symbols:\n                    normalized_symbol = symbol.upper()\n                    normalized_pattern = pattern.upper()\n                    \n                    # 정확한 일치 또는 포함 관계 확인\n                    if normalized_symbol == normalized_pattern or normalized_pattern in normalized_symbol:\n                        self.symbol_mappings[base_symbol][exchange_id] = symbol\n                        logger.info(f\"심볼 매핑 발견: {base_symbol} -> {exchange_id}:{symbol}\")\n                        self.supported_symbols.add(base_symbol)\n                        found = True\n                        break\n                \n                if found:\n                    break\n    \n    def save_symbol_mappings(self):\n        \"\"\"심볼 매핑 정보 파일로 저장\"\"\"\n        try:\n            filepath = DATA_DIR / \"symbol_mappings.json\"\n            with open(filepath, 'w') as f:\n                json.dump({\n                    \"mappings\": self.symbol_mappings,\n                    \"supported_symbols\": list(self.supported_symbols),\n                    \"timestamp\": datetime.now().isoformat()\n                }, f, indent=2)\n            logger.info(f\"심볼 매핑 정보 저장 완료: {filepath}\")\n        except Exception as e:\n            logger.error(f\"심볼 매핑 정보 저장 실패: {str(e)}\")\n    \n    def load_symbol_mappings(self):\n        \"\"\"저장된 심볼 매핑 정보 로드\"\"\"\n        try:\n            filepath = DATA_DIR / \"symbol_mappings.json\"\n            if filepath.exists():\n                with open(filepath, 'r') as f:\n                    data = json.load(f)\n                    self.symbol_mappings = data.get(\"mappings\", {})\n                    self.supported_symbols = set(data.get(\"supported_symbols\", []))\n                logger.info(f\"심볼 매핑 정보 로드 완료: {len(self.symbol_mappings)}개 심볼\")\n                return True\n            return False\n        except Exception as e:\n            logger.error(f\"심볼 매핑 정보 로드 실패: {str(e)}\")\n            return False\n    \n    def get_ccxt_symbol(self, base_symbol, exchange_id=None):\n        \"\"\"CCXT 심볼 가져오기 (특정 거래소 또는 우선순위 기반)\"\"\"\n        # 매핑 정보 확인\n        if base_symbol not in self.symbol_mappings:\n            self.discover_symbol_mapping(base_symbol)\n        \n        # 특정 거래소 지정한 경우\n        if exchange_id:\n            return self.symbol_mappings.get(base_symbol, {}).get(exchange_id)\n        \n        # 거래소 우선순위 기준 탐색\n        for ex_id in self.exchange_priority:\n            if ex_id in self.symbol_mappings.get(base_symbol, {}):\n                return self.symbol_mappings[base_symbol][ex_id], ex_id\n        \n        return None, None\n    \n    def fetch_ohlcv(self, base_symbol, timeframe='5m', since=None, limit=1000):\n        \"\"\"CCXT를 통해 OHLCV 데이터 가져오기\"\"\"\n        if shutdown_flag.is_set():\n            return pd.DataFrame()\n            \n        # 심볼 매핑 확인 및 가져오기\n        ccxt_symbol, exchange_id = self.get_ccxt_symbol(base_symbol)\n        \n        if not ccxt_symbol or not exchange_id:\n            logger.warning(f\"CCXT 심볼 매핑을 찾을 수 없음: {base_symbol}\")\n            return pd.DataFrame()\n        \n        # 타임프레임 변환\n        ccxt_timeframe = self.timeframe_map.get(timeframe, timeframe)\n        \n        # since 변환 (timestamp to milliseconds)\n        since_ms = int(since.timestamp() * 1000) if since else None\n        \n        # 데이터 가져오기 시도\n        try:\n            exchange = self.exchanges[exchange_id]\n            \n            # OHLCV 데이터 가져오기\n            ohlcv = exchange.fetch_ohlcv(ccxt_symbol, ccxt_timeframe, since_ms, limit)\n            \n            if not ohlcv:\n                logger.warning(f\"CCXT에서 데이터 없음: {exchange_id}:{ccxt_symbol}\")\n                return pd.DataFrame()\n            \n            # DataFrame 변환\n            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n            df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n            df['symbol'] = base_symbol\n            df['exchange'] = exchange_id\n            \n            logger.info(f\"CCXT 데이터 수집 성공: {exchange_id}:{ccxt_symbol}, {len(df)}행\")\n            return df\n            \n        except Exception as e:\n            logger.error(f\"CCXT 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n            return pd.DataFrame()\n    \n    def fetch_funding_rate(self, base_symbol, timeframe='1h', since=None, limit=500):\n        \"\"\"CCXT를 통해 펀딩비 데이터 가져오기 (지원하는 거래소만)\"\"\"\n        if shutdown_flag.is_set():\n            return pd.DataFrame()\n        \n        # 펀딩비를 지원하는 거래소들\n        funding_supported = [\"binance\", \"bybit\", \"okx\"]\n        funding_data = []\n        \n        # 지원하는 거래소에서 데이터 수집 시도\n        for exchange_id in funding_supported:\n            if exchange_id not in self.exchanges:\n                continue\n                \n            ccxt_symbol = self.get_ccxt_symbol(base_symbol, exchange_id)\n            if not ccxt_symbol:\n                continue\n                \n            try:\n                exchange = self.exchanges[exchange_id]\n                \n                # 1. fetchFundingRateHistory 사용 (최신 데이터)\n                if hasattr(exchange, 'fetchFundingRateHistory') and callable(getattr(exchange, 'fetchFundingRateHistory')):\n                    since_ms = int(since.timestamp() * 1000) if since else None\n                    funding_history = exchange.fetch_funding_rate_history(ccxt_symbol, since_ms, limit)\n                    \n                    if funding_history:\n                        for entry in funding_history:\n                            funding_data.append({\n                                'datetime': pd.to_datetime(entry['timestamp'], unit='ms'),\n                                'symbol': base_symbol,\n                                'exchange': exchange_id,\n                                'funding_rate': entry['fundingRate']\n                            })\n                \n                # 2. fetchFundingRate 사용 (현재 펀딩비)\n                elif hasattr(exchange, 'fetchFundingRate') and callable(getattr(exchange, 'fetchFundingRate')):\n                    funding_info = exchange.fetch_funding_rate(ccxt_symbol)\n                    if funding_info:\n                        funding_data.append({\n                            'datetime': pd.to_datetime(funding_info['timestamp'], unit='ms'),\n                            'symbol': base_symbol,\n                            'exchange': exchange_id,\n                            'funding_rate': funding_info['fundingRate']\n                        })\n            \n            except Exception as e:\n                logger.error(f\"CCXT 펀딩비 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n        \n        # 수집된 데이터를 DataFrame으로 변환\n        if funding_data:\n            df = pd.DataFrame(funding_data)\n            logger.info(f\"CCXT 펀딩비 데이터 수집 성공: {base_symbol}, {len(df)}행\")\n            return df\n        else:\n            return pd.DataFrame()\n\n# API 키 매니저 인스턴스 생성\napi_key_manager = APIKeyManager()\n\n# CCXT 매니저 인스턴스 생성\nccxt_manager = CcxtManager()\n\ndef get_simple_symbol(symbol: str) -> str:\n    \"\"\"심볼명을 단순화 (예: BTCUSDT_PERP.A -> BTC)\"\"\"\n    if 'USDT_PERP.A' in symbol:\n        return symbol.split('USDT_')[0]\n    return symbol.replace('.csv', '')\n\ndef exponential_backoff(attempt, base_delay=2, max_delay=120):\n    \"\"\"지수 백오프 알고리즘\"\"\"\n    delay = min(max_delay, base_delay * (2 ** attempt))\n    jitter = random.uniform(0, 0.1 * delay)  # 10% 지터 추가\n    return delay + jitter\n\ndef make_request(endpoint: str, params: dict = None, tier: str = \"tier1\", max_retries: int = 5) -> Optional[dict]:\n    \"\"\"API 요청 수행 (지수 백오프 적용)\"\"\"\n    url = f\"{BASE_URL}{endpoint}\"\n    \n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(\"종료 중에는 API 요청을 수행하지 않습니다\")\n        return None\n    \n    for attempt in range(max_retries):\n        try:\n            api_key = api_key_manager.get_next_key(tier)\n            headers = {\n                \"Accept\": \"application/json\",\n                \"Api-Key\": api_key\n            }\n            \n            response = requests.get(url, headers=headers, params=params, timeout=60)\n            \n            if response.status_code == 429:  # Rate limit exceeded\n                sleep_time = exponential_backoff(attempt)\n                logger.warning(f\"API 속도 제한 초과. {sleep_time:.2f}초 후 재시도... (시도 {attempt+1}/{max_retries})\")\n                time.sleep(sleep_time)\n                continue\n                \n            if response.status_code == 401:  # 인증 실패\n                logger.error(f\"API 키 인증 실패: {api_key}\")\n                continue\n                \n            response.raise_for_status()\n            return response.json()\n            \n        except requests.exceptions.Timeout:\n            sleep_time = exponential_backoff(attempt)\n            logger.error(f\"API 요청 타임아웃 ({endpoint}) - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n            time.sleep(sleep_time)\n        except requests.exceptions.RequestException as e:\n            sleep_time = exponential_backoff(attempt)\n            logger.error(f\"API 요청 실패 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n            time.sleep(sleep_time)\n        except Exception as e:\n            sleep_time = exponential_backoff(attempt)\n            logger.error(f\"예상치 못한 오류 발생 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\n            time.sleep(sleep_time)\n    \n    return None\n\ndef get_all_symbols() -> List[str]:\n    \"\"\"사용 가능한 모든 심볼 목록 조회\"\"\"\n    data = make_request(ENDPOINTS[\"future_markets\"])\n    \n    if not data:\n        logger.error(\"심볼 목록을 가져올 수 없습니다\")\n        return []\n        \n    symbols = []\n    if isinstance(data, dict):\n        markets = data.get('markets', [])\n    else:\n        markets = data\n        \n    for market in markets:\n        symbol = market.get('symbol', '')\n        if symbol.endswith('USDT_PERP.A'):\n            base_symbol = get_simple_symbol(symbol)\n            symbols.append(base_symbol)\n    \n    logger.info(f\"총 {len(symbols)}개의 심볼을 찾았습니다\")\n    \n    # 심볼 목록 파일로 저장\n    with open(DATA_DIR / 'symbols.json', 'w') as f:\n        json.dump({\n            'symbols': symbols,\n            'count': len(symbols),\n            'timestamp': datetime.now().isoformat()\n        }, f, indent=2)\n        \n    return sorted(symbols)\n\ndef extract_data_from_response(data, data_type: str):\n    \"\"\"API 응답에서 데이터 추출 개선\"\"\"\n    history = []\n    \n    try:\n        # 응답 데이터 구조 확인\n        if isinstance(data, list):\n            # 리스트 응답 처리\n            if len(data) > 0:\n                if isinstance(data[0], dict):\n                    if 'history' in data[0]:\n                        # 기존 구조: [{\"history\": [...]}]\n                        history = data[0]['history']\n                    else:\n                        # 새 구조: 리스트가 직접 데이터인 경우\n                        history = data\n                else:\n                    # 직접 데이터 리스트인 경우\n                    history = data\n        elif isinstance(data, dict):\n            # 딕셔너리 응답 처리\n            if 'history' in data:\n                history = data['history']\n            elif 'data' in data:\n                history = data['data']\n            elif 'result' in data:\n                history = data['result']\n            elif 'items' in data:\n                history = data['items']\n            else:\n                # 다른 키가 없으면 응답 전체를 사용\n                potential_data = []\n                for key, value in data.items():\n                    if isinstance(value, list) and len(value) > 0:\n                        potential_data.extend(value)\n                if potential_data:\n                    history = potential_data\n    except Exception as e:\n        logger.error(f\"{data_type} 데이터 파싱 중 오류: {str(e)}\")\n    \n    return history\n\ndef normalize_columns(df, data_type: str) -> pd.DataFrame:\n    \"\"\"데이터 프레임 컬럼 정규화 (소수점 보존)\"\"\"\n    # 타임스탬프 변환\n    if 't' in df.columns:\n        df['datetime'] = pd.to_datetime(df['t'], unit='s')\n    elif 'timestamp' in df.columns:\n        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n    elif 'time' in df.columns:\n        df['datetime'] = pd.to_datetime(df['time'], unit='s')\n    \n    # 데이터 타입별 컬럼 처리\n    if data_type == 'ohlcv':\n        column_mappings = {\n            'o': 'open', 'open': 'open', 'price_open': 'open',\n            'h': 'high', 'high': 'high', 'price_high': 'high',\n            'l': 'low', 'low': 'low', 'price_low': 'low',\n            'c': 'close', 'close': 'close', 'price_close': 'close',\n            'v': 'volume', 'volume': 'volume', 'vol': 'volume'\n        }\n    elif data_type == 'oi':\n        column_mappings = {\n            'c': 'open_interest', 'value': 'open_interest', \n            'openInterest': 'open_interest', 'oi': 'open_interest'\n        }\n    elif data_type == 'funding':\n        column_mappings = {\n            'c': 'funding_rate', 'rate': 'funding_rate', \n            'fundingRate': 'funding_rate', 'funding': 'funding_rate'\n        }\n    elif data_type == 'predicted_funding':\n        column_mappings = {\n            'c': 'predicted_funding_rate', 'rate': 'predicted_funding_rate',\n            'predictedRate': 'predicted_funding_rate', 'predicted': 'predicted_funding_rate'\n        }\n    elif data_type == 'liquidation':\n        column_mappings = {\n            'c': 'liquidation', 'value': 'liquidation',\n            'liq': 'liquidation', 'liquidations': 'liquidation'\n        }\n    elif data_type == 'long_short_ratio':\n        column_mappings = {\n            'c': 'long_short_ratio', 'ratio': 'long_short_ratio',\n            'l': 'long_percentage', 'long': 'long_percentage', 'longPercentage': 'long_percentage',\n            's': 'short_percentage', 'short': 'short_percentage', 'shortPercentage': 'short_percentage'\n        }\n    else:\n        column_mappings = {}\n    \n    # 컬럼 매핑 적용\n    for old_col, new_col in column_mappings.items():\n        if old_col in df.columns and old_col != new_col:\n            df[new_col] = df[old_col]\n    \n    return df\n\ndef validate_data(df, data_type: str) -> pd.DataFrame:\n    \"\"\"데이터 검증 (소수점 보존)\"\"\"\n    if df.empty:\n        return df\n    \n    # 타임스탬프가 없는 행 제거\n    if 'datetime' in df.columns:\n        df = df[df['datetime'].notna()]\n    \n    # 데이터 타입별 검증\n    if data_type == 'ohlcv':\n        # 가격 데이터 검증 (음수 값 제거, 소수점은 유지)\n        for col in ['open', 'high', 'low', 'close']:\n            if col in df.columns:\n                # 0 이하 값 확인 후 경고만 발생 (제거하지 않음)\n                zero_or_neg = df[df[col] <= 0]\n                if not zero_or_neg.empty:\n                    logger.warning(f\"{data_type}: {len(zero_or_neg)}개 행에서 {col} 값이 0 이하입니다. 데이터 유지됨.\")\n        \n        # 볼륨 검증 (음수 제거)\n        if 'volume' in df.columns:\n            neg_volume = df[df['volume'] < 0]\n            if not neg_volume.empty:\n                logger.warning(f\"{data_type}: {len(neg_volume)}개 행에서 volume 값이 음수입니다. 해당 값을 0으로 설정합니다.\")\n                df.loc[df['volume'] < 0, 'volume'] = 0\n    \n    elif data_type == 'funding' or data_type == 'predicted_funding':\n        # 극단적인 펀딩비 값 확인 (소수점은 유지)\n        funding_col = 'funding_rate' if data_type == 'funding' else 'predicted_funding_rate'\n        if funding_col in df.columns:\n            extreme_funding = df[(df[funding_col] < -0.15) | (df[funding_col] > 0.15)]\n            if not extreme_funding.empty:\n                logger.warning(f\"{data_type}: {len(extreme_funding)}개 행에서 {funding_col} 값이 극단적입니다 (-0.15 미만 또는 0.15 초과). 데이터 유지됨.\")\n    \n    elif data_type == 'long_short_ratio':\n        # 롱/숏 퍼센티지 합이 100%인지 확인 (오차 범위 ±2%)\n        if 'long_percentage' in df.columns and 'short_percentage' in df.columns:\n            total = df['long_percentage'] + df['short_percentage']\n            invalid_rows = df[(total < 98) | (total > 102)]\n            if not invalid_rows.empty:\n                logger.warning(f\"{data_type}: {len(invalid_rows)}개 행에서 long_percentage와 short_percentage 합이 100%와 크게 다릅니다. 데이터 유지됨.\")\n    \n    return df\n\ndef process_ohlcv_data(data) -> pd.DataFrame:\n    \"\"\"OHLCV 데이터 처리 (대체 소스용)\"\"\"\n    history = extract_data_from_response(data, \"ohlcv\")\n    \n    if not history:\n        return pd.DataFrame()\n        \n    try:\n        df = pd.DataFrame(history)\n        df = normalize_columns(df, \"ohlcv\")\n        \n        if 'datetime' not in df.columns and 't' in df.columns:\n            df['datetime'] = pd.to_datetime(df['t'], unit='s')\n        \n        # 필수 OHLCV 컬럼 확인\n        required_cols = ['open', 'high', 'low', 'close']\n        missing_cols = [col for col in required_cols if col not in df.columns]\n        \n        if missing_cols:\n            logger.warning(f\"OHLCV 대체 소스에서 필수 컬럼 누락: {missing_cols}\")\n            return pd.DataFrame()\n            \n        return df\n    except Exception as e:\n        logger.error(f\"OHLCV 대체 소스 데이터 처리 오류: {str(e)}\")\n        return pd.DataFrame()\n\ndef merge_dataframes(all_data_frames: Dict[str, pd.DataFrame], symbol: str) -> pd.DataFrame:\n    \"\"\"여러 데이터프레임 병합 (소수점 보존)\"\"\"\n    if not all_data_frames:\n        return pd.DataFrame()\n    \n    # 가능한 모든 기준 프레임 후보 (우선순위 순)\n    base_frame_candidates = [\"ohlcv\", \"long_short_ratio\", \"funding\", \"oi\"]\n    base_df = None\n    \n    # 우선순위에 따라 기준 프레임 선택\n    for candidate in base_frame_candidates:\n        if candidate in all_data_frames and not all_data_frames[candidate].empty:\n            base_df = all_data_frames[candidate].copy()\n            logger.info(f\"{symbol} 기준 프레임으로 {candidate} 선택 ({len(base_df)}행)\")\n            break\n    \n    # 기준 프레임이 없으면 빈 DataFrame 반환\n    if base_df is None:\n        logger.warning(f\"{symbol} 병합할 기준 프레임이 없습니다.\")\n        return pd.DataFrame()\n    \n    # 심볼 정보 확인 및 추가\n    if 'symbol' not in base_df.columns:\n        base_df['symbol'] = symbol\n        \n    # 기본 프레임 컬럼 이름 확인 (중복 방지 위해)\n    # 문제가 되는 t, o, h, l, c 등 짧은 컬럼명을 미리 표준화\n    if 't' in base_df.columns:\n        base_df.rename(columns={'t': 'timestamp'}, inplace=True)\n    for col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n        if col_old in base_df.columns:\n            base_df.rename(columns={col_old: col_new}, inplace=True)\n            \n    # 인덱스 리셋 (병합을 위한 준비)\n    base_df = base_df.reset_index(drop=True)\n    \n    # 표준 컬럼명 리스트\n    std_cols = {\n        'open', 'high', 'low', 'close', 'volume',\n        'open_interest', 'funding_rate', 'predicted_funding_rate',\n        'liquidation', 'long_percentage', 'short_percentage', 'long_short_ratio',\n        'datetime', 'symbol'\n    }\n    \n    # 각 데이터프레임을 병합 (왼쪽 조인 방식으로 변경)\n    for data_type, df in all_data_frames.items():\n        # 기준 프레임 또는 빈 프레임 건너뛰기\n        is_base_frame = data_type == list(base_frame_candidates)[0]\n        if is_base_frame or df.empty:\n            continue\n        \n        # 표준화된 컬럼명으로 변환\n        if 't' in df.columns:\n            df.rename(columns={'t': 'timestamp'}, inplace=True)\n        for col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\n            if col_old in df.columns:\n                df.rename(columns={col_old: col_new}, inplace=True)\n        \n        # 기준 프레임에 없는 컬럼만 추출\n        columns_to_merge = []\n        for col in df.columns:\n            # 표준 컬럼 외의 컬럼은 고유한 이름으로 변경\n            if col not in std_cols and col in base_df.columns:\n                new_col = f\"{col}_{data_type}\"\n                df.rename(columns={col: new_col}, inplace=True)\n                columns_to_merge.append(new_col)\n            elif col not in base_df.columns and col != 'datetime' and col != 'symbol':\n                columns_to_merge.append(col)\n        \n        # 병합할 컬럼이 있고 datetime 열이 있는 경우에만 병합 진행\n        if len(columns_to_merge) > 0 and 'datetime' in df.columns:\n            # 인덱스 리셋 (병합을 위한 준비)\n            df_to_merge = df.reset_index(drop=True)\n            merge_df = df_to_merge[['datetime'] + columns_to_merge].copy()\n            \n            # 병합 (왼쪽 조인 방식으로 변경)\n            try:\n                base_df = pd.merge(base_df, merge_df, on='datetime', how='left', suffixes=('', f'_{data_type}'))\n            except Exception as e:\n                logger.error(f\"데이터프레임 병합 오류 ({data_type}): {str(e)}\")\n                traceback.print_exc()  # 상세 오류 출력\n                # 병합 실패해도 계속 진행\n    \n    # 최종 정리\n    try:\n        base_df = base_df.sort_values('datetime').reset_index(drop=True)\n    except Exception as e:\n        logger.error(f\"데이터프레임 정렬 오류: {str(e)}\")\n    \n    return base_df\n\ndef load_existing_data(symbol: str) -> pd.DataFrame:\n    \"\"\"기존 데이터 로드 (소수점 보존)\"\"\"\n    csv_path = LIVE_DIR / f\"{symbol}.csv\"\n    \n    if not csv_path.exists():\n        return pd.DataFrame()\n        \n    try:\n        # 소수점 반올림 없이 데이터 로드\n        df = pd.read_csv(csv_path)\n        df['datetime'] = pd.to_datetime(df['datetime'])\n        logger.info(f\"{symbol} 기존 데이터 로드: {len(df)}행\")\n        return df\n    except Exception as e:\n        logger.error(f\"{symbol} 데이터 로드 실패: {str(e)}\")\n        traceback.print_exc()  # 상세 오류 출력\n        \n        # 손상된 파일 처리\n        try:\n            # 손상된 파일 백업\n            corrupted_dir = DATA_DIR / \"corrupted\"\n            corrupted_dir.mkdir(parents=True, exist_ok=True)\n            \n            backup_file = corrupted_dir / f\"{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n            os.rename(csv_path, backup_file)\n            logger.warning(f\"손상된 파일 이동: {csv_path} -> {backup_file}\")\n        except Exception as backup_err:\n            logger.error(f\"손상된 파일 백업 실패: {str(backup_err)}\")\n        \n        return pd.DataFrame()\n\ndef save_data(symbol: str, new_data: pd.DataFrame) -> bool:\n    \"\"\"데이터 저장 (기존 데이터와 병합, 소수점 보존)\"\"\"\n    if new_data.empty:\n        logger.warning(f\"{symbol}: 저장할 새 데이터가 없습니다\")\n        return False\n    \n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 저장이 중단되었습니다 (종료 중)\")\n        return False\n        \n    LIVE_DIR.mkdir(parents=True, exist_ok=True)\n    csv_path = LIVE_DIR / f\"{symbol}.csv\"\n    \n    try:\n        # 1. 기존 데이터 로드\n        existing_data = load_existing_data(symbol)\n        \n        # 2. 새 데이터와 기존 데이터 병합\n        if not existing_data.empty:\n            # 기존 데이터의 타임스탬프를 set으로 변환하여 비교 (성능 개선)\n            existing_timestamps = set(existing_data['datetime'].dt.to_pydatetime())\n            # set을 사용한 필터링으로 변경\n            mask = [dt not in existing_timestamps for dt in new_data['datetime'].dt.to_pydatetime()]\n            new_data_filtered = new_data.loc[mask] if any(mask) else pd.DataFrame()\n            \n            if not new_data_filtered.empty:\n                # 인덱스 리셋 후 병합하여 인덱스 충돌 방지\n                df = pd.concat([existing_data.reset_index(drop=True), \n                                new_data_filtered.reset_index(drop=True)], \n                               ignore_index=True)\n            else:\n                logger.info(f\"{symbol}: 새 데이터가 이미 기존 데이터에 포함되어 있습니다\")\n                return True\n        else:\n            df = new_data.copy()\n            \n        # 3. 중복 컬럼 제거\n        column_mappings = {\n            'price_open': 'open',\n            'price_high': 'high',\n            'price_low': 'low',\n            'price_close': 'close'\n        }\n        \n        for old_col, new_col in column_mappings.items():\n            if old_col in df.columns and new_col in df.columns:\n                df = df.drop(old_col, axis=1)\n            elif old_col in df.columns:\n                df = df.rename(columns={old_col: new_col})\n            \n        # 4. 불필요한 컬럼 제거\n        columns_to_keep = [\n            'datetime', 'symbol',\n            'open', 'high', 'low', 'close', 'volume',\n            'open_interest',\n            'funding_rate', 'predicted_funding_rate',\n            'liquidation',\n            'long_percentage', 'short_percentage', 'long_short_ratio'\n        ]\n        \n        available_columns = [col for col in columns_to_keep if col in df.columns]\n        \n        # 핵심 컬럼이 없어도 최소한의 메타데이터만이라도 저장\n        min_required_cols = ['datetime', 'symbol']\n        if all(col in available_columns for col in min_required_cols):\n            df = df[available_columns]\n            \n            # 5. 중복 제거 및 정렬\n            df = df.drop_duplicates(subset=['datetime']).reset_index(drop=True)\n            df = df.sort_values('datetime')\n            \n            # 6. 저장 (소수점 보존)\n            # 파일 손상 방지를 위한, 임시 파일에 저장 후 이동\n            temp_path = csv_path.with_name(f\"{symbol}_temp.csv\")\n            \n            # 소수점 반올림 없이 저장 (float_format 인자 사용하지 않음)\n            df.to_csv(temp_path, index=False)\n            \n            # 임시 파일이 성공적으로 작성되었으면 원본 파일로 이동\n            if temp_path.exists():\n                if csv_path.exists():\n                    csv_path.unlink()  # 기존 파일 삭제\n                temp_path.rename(csv_path)  # 임시 파일을 원본 파일로 이동\n            \n            # 핵심 데이터 누락 시 경고 (Series 직접 평가 수정)\n            has_open = 'open' in df.columns\n            missing_open_data = has_open and df['open'].isnull().all()\n            if not has_open or missing_open_data:\n                logger.warning(f\"{symbol} 가격 데이터(OHLCV)가 모두 누락되었습니다. 메타데이터만 저장됨.\")\n            \n            logger.info(f\"{symbol} 데이터 저장 완료: 총 {len(df)}행 (새 데이터: {len(new_data)}행)\")\n            \n            # 7. 데이터 검증\n            if len(df) > 1:  # 최소 2개 이상의 행이 있어야 diff 계산 가능\n                time_diffs = pd.Series(df['datetime'].diff().dt.total_seconds() / 60)\n                gaps = time_diffs[time_diffs > 10]  # 10분 이상의 간격이 있는 경우\n                \n                if not gaps.empty:\n                    logger.warning(f\"{symbol} 데이터 누락 발견:\")\n                    for idx in gaps.index:\n                        gap_start = df['datetime'].iloc[idx-1]\n                        gap_end = df['datetime'].iloc[idx]\n                        gap_minutes = gaps[idx]\n                        logger.warning(f\"  {gap_start} ~ {gap_end} ({gap_minutes:.0f}분 간격)\")\n            \n            return True\n        else:\n            logger.error(f\"{symbol} 필수 컬럼({min_required_cols})이 없어 저장할 수 없습니다.\")\n            return False\n            \n    except Exception as e:\n        logger.error(f\"{symbol} 데이터 저장 실패: {str(e)}\")\n        traceback.print_exc()  # 상세 오류 출력\n        return False\n\ndef fetch_historical_data(symbol: str, data_type: str, start_time: datetime, end_time: datetime, interval: str = \"5min\") -> pd.DataFrame:\n    \"\"\"특정 심볼의 역사적 데이터 수집 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} {data_type} 수집이 중단되었습니다 (종료 중)\")\n        return pd.DataFrame()\n    \n    endpoint_map = {\n        \"ohlcv\": \"ohlcv_history\",\n        \"oi\": \"open_interest_history\",\n        \"funding\": \"funding_rate_history\",\n        \"predicted_funding\": \"predicted_funding_rate_history\",\n        \"liquidation\": \"liquidation_history\",\n        \"long_short_ratio\": \"long_short_ratio_history\"\n    }\n    \n    if data_type not in endpoint_map:\n        logger.error(f\"지원되지 않는 데이터 타입: {data_type}\")\n        return pd.DataFrame()\n    \n    try:\n        # 1. Coinalyze API 요청\n        params = {\n            \"symbols\": f\"{symbol}USDT_PERP.A\",\n            \"interval\": interval,\n            \"from\": int(start_time.timestamp()),\n            \"to\": int(end_time.timestamp())\n        }\n        \n        tier = \"tier\" + str(random.randint(1, 4))  # 티어 무작위 선택\n        data = make_request(ENDPOINTS[endpoint_map[data_type]], params, tier)\n        \n        # API 응답 처리\n        if data is not None:\n            # 데이터 추출\n            history = extract_data_from_response(data, data_type)\n            \n            # DataFrame 생성 및 처리\n            try:\n                if not history:\n                    # 데이터 없음\n                    logger.warning(f\"{symbol} {data_type}: 데이터가 없습니다\")\n                else:\n                    df = pd.DataFrame(history)\n                    \n                    # 컬럼 정규화 (소수점 보존)\n                    if not df.empty:\n                        df = normalize_columns(df, data_type)\n                        \n                        # 심볼 정보 추가\n                        if 'symbol' not in df.columns:\n                            df['symbol'] = symbol\n                        \n                        logger.info(f\"Coinalyze {data_type} 데이터 수집 성공: {symbol}, {len(df)}행\")\n                        return df\n                        \n            except Exception as e:\n                logger.error(f\"{symbol} DataFrame 처리 중 오류: {str(e)}\")\n                traceback.print_exc()\n        \n        # 2. Coinalyze API 실패 시 CCXT 시도 (OHLCV 및 펀딩비만 지원)\n        if data_type == \"ohlcv\":\n            logger.warning(f\"{symbol} Coinalyze OHLCV 데이터 수집 실패, CCXT로 시도...\")\n            df = ccxt_manager.fetch_ohlcv(symbol, interval, start_time, 1000)\n            if not df.empty:\n                return df\n        \n        elif data_type == \"funding\":\n            logger.warning(f\"{symbol} Coinalyze 펀딩비 데이터 수집 실패, CCXT로 시도...\")\n            df = ccxt_manager.fetch_funding_rate(symbol, interval, start_time, 500)\n            if not df.empty:\n                return df\n    \n    except Exception as e:\n        logger.error(f\"{symbol} {data_type} 데이터 수집 중 오류 발생: {str(e)}\")\n        traceback.print_exc()\n    \n    # 데이터를 가져오지 못한 경우\n    logger.warning(f\"{symbol} {data_type} 데이터 수집 실패 (Coinalyze 및 CCXT)\")\n    return pd.DataFrame()\n\ndef check_data_gaps(symbol: str, interval: str = \"5min\", days: int = 30) -> List[Tuple[datetime, datetime]]:\n    \"\"\"데이터 누락 구간 확인\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 누락 검사가 중단되었습니다 (종료 중)\")\n        return []\n    \n    df = load_existing_data(symbol)\n    \n    if df.empty:\n        # 데이터가 없으면 전체 기간을 누락으로 간주\n        end_time = datetime.now()\n        start_time = end_time - timedelta(days=days)\n        return [(start_time, end_time)]\n    \n    # 데이터 정렬\n    df = df.sort_values('datetime')\n    \n    # 타임라인 예상 범위 생성 (5분 간격)\n    start_time = max(df['datetime'].min(), datetime.now() - timedelta(days=days))\n    end_time = datetime.now()\n    \n    expected_timeline = pd.date_range(start=start_time, end=end_time, freq=interval)\n    actual_timeline = df['datetime']\n    \n    # 누락된 시간대 찾기\n    missing_times = pd.DatetimeIndex(expected_timeline).difference(actual_timeline)\n    \n    if len(missing_times) == 0:\n        return []\n    \n    # 연속된 누락 구간 그룹화\n    gaps = []\n    if len(missing_times) > 0:\n        gap_start = missing_times[0]\n        for i in range(1, len(missing_times)):\n            curr_time = missing_times[i]\n            prev_time = missing_times[i-1]\n            # 다음 시간과 현재 시간의 차이가 간격보다 크면 새 그룹 시작\n            if (curr_time - prev_time).total_seconds() > pd.Timedelta(interval).total_seconds() * 2:\n                gaps.append((gap_start, prev_time))\n                gap_start = curr_time\n                \n        # 마지막 그룹 추가\n        gaps.append((gap_start, missing_times[-1]))\n    \n    return gaps\n\ndef fill_data_gaps(symbol: str, days: int = 30) -> bool:\n    \"\"\"데이터 누락 구간 채우기 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 누락 채우기가 중단되었습니다 (종료 중)\")\n        return False\n    \n    gaps = check_data_gaps(symbol, days=days)\n    \n    if not gaps:\n        logger.info(f\"{symbol}: 데이터 누락 없음\")\n        return True\n    \n    logger.info(f\"{symbol}: {len(gaps)}개의 데이터 누락 구간 발견\")\n    \n    data_types = [\"ohlcv\", \"oi\", \"funding\", \"predicted_funding\", \"liquidation\", \"long_short_ratio\"]\n    successful_fills = 0\n    \n    for start_time, end_time in gaps:\n        if shutdown_flag.is_set():\n            logger.warning(f\"{symbol} 데이터 누락 채우기가 중단되었습니다 (종료 중)\")\n            break\n            \n        logger.info(f\"{symbol} 누락 구간 채우기: {start_time} ~ {end_time}\")\n        \n        # 데이터 타입별로 수집\n        all_data_frames = {}\n        gap_filled = False\n        \n        # 각 데이터 타입별로 누락 구간 데이터 수집\n        for data_type in data_types:\n            if shutdown_flag.is_set():\n                break\n                \n            df = fetch_historical_data(symbol, data_type, start_time, end_time)\n            if not df.empty:\n                all_data_frames[data_type] = df\n                gap_filled = True\n        \n        # 수집된 데이터가 있으면 병합하여 저장\n        if all_data_frames and not shutdown_flag.is_set():\n            # 데이터프레임 병합 (소수점 보존)\n            merged_df = merge_dataframes(all_data_frames, symbol)\n            \n            if not merged_df.empty:\n                # 저장 (소수점 보존)\n                if save_data(symbol, merged_df):\n                    successful_fills += 1\n    \n    return successful_fills > 0\n\ndef process_symbol_data(symbol: str, interval: str = \"5min\", mode: str = \"realtime\") -> dict:\n    \"\"\"한 심볼의 데이터 처리 (Coinalyze 우선, 실패 시 CCXT 사용)\"\"\"\n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        return {\n            \"symbol\": symbol,\n            \"mode\": mode,\n            \"success\": False,\n            \"message\": \"종료 중\"\n        }\n    \n    result = {\n        \"symbol\": symbol,\n        \"mode\": mode,\n        \"success\": False,\n        \"start_time\": None,\n        \"end_time\": None,\n        \"rows_added\": 0\n    }\n    \n    try:\n        if mode == \"backfill\":\n            # 과거 데이터 누락 검사 및 채우기\n            gaps_filled = fill_data_gaps(symbol)\n            result[\"success\"] = gaps_filled\n            return result\n            \n        # 실시간 데이터 수집\n        current_time = datetime.now()\n        \n        # 일반적으로 30분 전 데이터부터 수집 (완전한 캔들을 위해)\n        start_time = current_time - timedelta(minutes=30)\n        \n        # 각 데이터 타입별로 최신 데이터 수집\n        data_types = [\"ohlcv\", \"oi\", \"funding\", \"predicted_funding\", \"liquidation\", \"long_short_ratio\"]\n        all_data_frames = {}\n        \n        for data_type in data_types:\n            if shutdown_flag.is_set():\n                result[\"message\"] = \"종료 중\"\n                return result\n                \n            df = fetch_historical_data(symbol, data_type, start_time, current_time, interval)\n            if not df.empty:\n                all_data_frames[data_type] = df\n        \n        # 수집된 데이터가 있으면 병합하여 저장\n        if all_data_frames and not shutdown_flag.is_set():\n            # 데이터프레임 병합 (소수점 보존)\n            merged_df = merge_dataframes(all_data_frames, symbol)\n            \n            if not merged_df.empty:\n                # 저장 (소수점 보존)\n                save_success = save_data(symbol, merged_df)\n                \n                result.update({\n                    \"success\": save_success,\n                    \"start_time\": merged_df['datetime'].min().isoformat() if not merged_df.empty else None,\n                    \"end_time\": merged_df['datetime'].max().isoformat() if not merged_df.empty else None,\n                    \"rows_added\": len(merged_df)\n                })\n            \n        return result\n        \n    except Exception as e:\n        logger.error(f\"{symbol} 처리 중 오류: {str(e)}\")\n        traceback.print_exc()\n        result[\"error\"] = str(e)\n        return result\n\ndef worker(task_queue, result_queue):\n    \"\"\"작업자 스레드 함수\"\"\"\n    while not shutdown_flag.is_set():\n        try:\n            # 종료 확인\n            if check_stop_file():\n                break\n                \n            # 1초 타임아웃으로 큐에서 작업 가져오기\n            try:\n                task = task_queue.get(timeout=1)\n            except queue.Empty:\n                continue\n                \n            # 종료 신호 확인\n            if task is None:\n                task_queue.task_done()\n                break\n                \n            symbol, interval, mode = task\n            result = process_symbol_data(symbol, interval, mode)\n            result_queue.put(result)\n            task_queue.task_done()\n            \n        except Exception as e:\n            logger.error(f\"작업자 스레드 오류: {str(e)}\")\n            traceback.print_exc()\n            time.sleep(1)  # 오류 시 잠시 대기\n    \n    logger.info(\"작업자 스레드 종료\")\n\ndef initialize_system():\n    \"\"\"시스템 초기화 및 심볼 매핑 생성\"\"\"\n    # 디렉토리 생성\n    for directory in [DATA_DIR, LIVE_DIR, BACKUP_DIR, ARCHIVE_DIR, LONG_TERM_DIR]:\n        directory.mkdir(parents=True, exist_ok=True)\n    \n    # CCXT 매니저 초기화\n    if not ccxt_manager.load_symbol_mappings():\n        # 저장된 매핑이 없으면 새로 생성\n        # 모든 심볼 목록 가져오기\n        symbols = get_all_symbols()\n        if symbols:\n            # 심볼 매핑 탐색\n            ccxt_manager.discover_symbol_mappings(symbols)\n    \n    logger.info(f\"시스템 초기화 완료: {len(ccxt_manager.symbol_mappings)}개 심볼 매핑됨\")\n\ndef continuous_data_collection(interval: str = \"5min\", num_threads: int = 8):\n    \"\"\"연속적인 데이터 수집 (멀티스레딩, Coinalyze 우선, CCXT 보완)\"\"\"\n    # 시스템 초기화\n    initialize_system()\n    \n    # 심볼 목록 로드\n    symbols = get_all_symbols()\n    if not symbols:\n        logger.error(\"심볼 목록을 가져올 수 없습니다\")\n        return\n    \n    logger.info(f\"데이터 수집 시작 ({interval} 간격): {len(symbols)}개 심볼\")\n    \n    # 작업 큐와 결과 큐 설정\n    task_queue = queue.Queue()\n    result_queue = queue.Queue()\n    \n    # 작업자 스레드 시작\n    threads = []\n    for _ in range(num_threads):\n        t = threading.Thread(target=worker, args=(task_queue, result_queue))\n        t.daemon = True  # 데몬 스레드로 설정\n        t.start()\n        threads.append(t)\n    \n    last_backup_check = datetime.now()\n    collection_start_time = time.time()\n    \n    while not shutdown_flag.is_set():\n        try:\n            # 종료 확인\n            if check_stop_file():\n                break\n            \n            # 모든 심볼에 대한 작업 생성\n            for symbol in symbols:\n                if shutdown_flag.is_set():\n                    break\n                task_queue.put((symbol, interval, \"realtime\"))\n            \n            # 모든 작업이 완료될 때까지 대기 (최대 대기 시간 설정)\n            wait_start = time.time()\n            while not task_queue.empty():\n                if shutdown_flag.is_set() or time.time() - wait_start > 600:  # 10분 초과 대기 시 타임아웃\n                    break\n                time.sleep(1)\n                check_stop_file()  # 정기적으로 종료 파일 확인\n            \n            # 작업 결과 처리\n            results_processed = 0\n            success_count = 0\n            fail_count = 0\n            \n            while not result_queue.empty() and results_processed < 1000:  # 최대 1000개 결과만 처리\n                result = result_queue.get()\n                if result[\"success\"]:\n                    logger.info(f\"{result['symbol']} 데이터 수집 성공: {result.get('rows_added', 0)}행 추가됨\")\n                    success_count += 1\n                else:\n                    logger.warning(f\"{result['symbol']} 데이터 수집 실패\")\n                    fail_count += 1\n                results_processed += 1\n            \n            logger.info(f\"데이터 수집 결과: 성공 {success_count}개, 실패 {fail_count}개\")\n            \n            # 수집 완료 시간 계산\n            collection_duration = time.time() - collection_start_time\n            \n            # 종료 확인\n            if shutdown_flag.is_set():\n                break\n            \n            # 다음 수집까지 대기\n            remaining_wait = max(0, COLLECTION_INTERVAL - collection_duration)\n            if remaining_wait > 0:\n                logger.info(f\"다음 데이터 수집까지 {remaining_wait:.1f}초 대기...\")\n                \n                # 대기 중에도 정기적으로 종료 확인\n                wait_start = time.time()\n                while time.time() - wait_start < remaining_wait:\n                    if shutdown_flag.is_set() or check_stop_file():\n                        break\n                    time.sleep(1)\n            \n            # 새로운 주기 시작\n            collection_start_time = time.time()\n            \n        except KeyboardInterrupt:\n            logger.info(\"사용자에 의해 프로그램이 중단되었습니다.\")\n            shutdown_flag.set()\n            break\n        except Exception as e:\n            logger.error(f\"데이터 수집 중 오류 발생: {str(e)}\")\n            traceback.print_exc()\n            time.sleep(60)  # 오류 발생 시 1분 대기 후 재시도\n    \n    # 안전한 종료 처리\n    logger.info(\"프로그램 종료 중...\")\n    \n    # 작업자 스레드 종료\n    for _ in threads:\n        task_queue.put(None)  # 종료 신호\n    \n    # 각 스레드가 안전하게 종료될 때까지 최대 30초 대기\n    for t in threads:\n        t.join(timeout=30)\n    \n    logger.info(\"데이터 수집이 완전히 종료되었습니다.\")\n\ndef main():\n    \"\"\"메인 함수\"\"\"\n    logger.info(\"암호화폐 데이터 통합 관리 시스템 시작\")\n    \n    try:\n        # 기존 종료 파일 확인 및 제거\n        if os.path.exists(\"stop.signal\"):\n            os.remove(\"stop.signal\")\n            logger.info(\"기존 종료 파일 제거됨\")\n        \n        # 시스템 초기화\n        initialize_system()\n        \n        # 실시간 데이터 수집 (5분 간격)\n        if not shutdown_flag.is_set():\n            logger.info(\"실시간 데이터 수집 시작...\")\n            continuous_data_collection(interval=\"5min\", num_threads=8)\n        \n    except KeyboardInterrupt:\n        logger.info(\"사용자에 의해 프로그램이 중단되었습니다.\")\n        shutdown_flag.set()\n    except Exception as e:\n        logger.error(f\"프로그램 실행 중 오류 발생: {str(e)}\")\n        traceback.print_exc()\n        shutdown_flag.set()\n    finally:\n        # 종료 마무리\n        if shutdown_flag.is_set():\n            logger.info(\"프로그램 종료 처리 완료\")\n\ndef create_stop_signal():\n    \"\"\"프로그램 종료 신호 파일 생성\"\"\"\n    try:\n        with open(\"stop.signal\", \"w\") as f:\n            f.write(f\"Stop requested at {datetime.now().isoformat()}\")\n        print(\"종료 신호가 생성되었습니다. 프로그램이 안전하게 종료됩니다.\")\n    except Exception as e:\n        print(f\"종료 신호 생성 실패: {str(e)}\")\n\nif __name__ == \"__main__\":\n    import sys\n    \n    # 명령행 인수 처리\n    if len(sys.argv) > 1 and sys.argv[1] == \"stop\":\n        create_stop_signal()\n    else:\n        main()\n\n이코드 적용해봐. ","commandType":4},{"text":"나눠서 저장해","commandType":4},{"text":"1 첫 실행 시 7일 데이터를 자동으로 가져오도록 설정\n2 데이터 누락 여부를 확인하고 누락된 부분만 채우기\n3 기본 시간 간격을 5분봉에서 1분봉으로 변경\n4 OHLCV, OI, 펀딩비 데이터에 집중하여 수집\n이렇게 수정","commandType":4},{"text":"구동해봐.","commandType":4},{"text":"종료가 잘안되고 400 에러뜬다","commandType":4},{"text":"로그를 살펴보니 심볼 데이터 처리 과정에서 오류가 발생했네요. 특히 \"API 요청 실패 (/open-interest-history): 400 Bad Request\" 에러와 \"symbols OHLCV 과거 데이터 수집 실패: 'NoneType' object is not iterable\" 에러가 보입니다.\n\n주요 문제점은 다음과 같습니다:\n\n1. 문자열 `\"symbols\"`를 심볼 이름으로 잘못 처리하고 있습니다. 이것은 심볼 목록 변수명을 실제 심볼로 혼동한 것으로 보입니다.\n2. 심볼 목록을 제대로 로드하지 못하고 있습니다.\n3. API 요청 형식이 잘못되었습니다 (`symbol=symbolsUSDT_PERP.A`와 같은 형태).\n\n이 문제를 해결하기 위해 `DataManager` 클래스의 `load_symbols` 메서드를 수정해야 합니다. 아래와 같이 수정하세요:\n\n```python\ndef load_symbols(self):\n    \"\"\"심볼 목록 로드\"\"\"\n    try:\n        symbols_file = DATA_DIR / 'symbols.json'\n        if symbols_file.exists():\n            with open(symbols_file, 'r') as f:\n                data = json.load(f)\n                self.symbols = data.get('symbols', [])\n            logger.info(f\"심볼 목록 로드 완료: {len(self.symbols)}개\")\n        else:\n            self.update_symbols()\n    except Exception as e:\n        logger.error(f\"심볼 목록 로드 실패: {str(e)}\")\n        self.symbols = ['BTC', 'ETH', 'SOL']  # 기본 심볼 목록 제공\n        logger.info(f\"기본 심볼 목록 사용: {len(self.symbols)}개\")\n```\n\n또한 데이터 수집 요청 부분에서도 확인해야 할 부분이 있습니다. 특히 `collect_historical_data` 메서드에서 OI 데이터 수집 부분을 다음과 같이 수정하세요:\n\n```python\n# OI 데이터 수집\ntry:\n    if end_time is None:\n        end_time = datetime.now()\n    \n    params = {\n        \"symbol\": f\"{symbol}USDT_PERP.A\",\n        \"from\": int(start_time.timestamp()),\n        \"to\": int(end_time.timestamp()),\n        \"interval\": \"1min\"\n    }\n    oi_data = make_request(ENDPOINTS[\"open_interest_history\"], params)\n    if oi_data:\n        history_data = extract_data_from_response(oi_data, \"oi\")\n        if history_data:\n            df_oi = pd.DataFrame(history_data)\n            if not df_oi.empty:\n                df_oi = normalize_columns(df_oi, \"oi\")\n                all_data_frames['oi'] = df_oi\nexcept Exception as e:\n    logger.error(f\"{symbol} OI 과거 데이터 수집 실패: {str(e)}\")\n    traceback.print_exc()\n```\n\n이 외에도 프로그램 시작 시 심볼 목록을 확실히 초기화하기 위해 `__init__` 메서드 마지막 부분을 수정하세요:\n\n```python\n# 심볼 목록 로드\nself.load_symbols()\n\n# 심볼 목록이 비어있으면 기본 심볼 설정\nif not self.symbols:\n    self.symbols = ['BTC', 'ETH', 'SOL']\n    logger.info(f\"기본 심볼 목록 사용: {len(self.symbols)}개\")\n```\n\n이러한 수정 사항을 적용하면 초기 에러를 해결하고 기본적인 데이터 수집이 가능해질 것입니다. 문제가 계속되면 로그를 더 확인하여 추가 문제를 식별하고 해결해야 합니다.\n\n이 내용을 체크한번해볼래? 심볼은 전체심볼로하는건 같은내용이야.","commandType":4},{"text":"ohlcv 값은 ccxt에서 펀딩비랑 oi값만 coinalyze에서 가져오는걸로 만들어.","commandType":4},{"text":"펀딩은 5분밖에 안되? 같이 1분으로 해야하는거아냐?","commandType":4},{"text":"구동해봐","commandType":4},{"text":"엔드포인트 문제인가? 의심을 해보자","commandType":4},{"text":"ccxt랑 심볼이 다르니까 정리 잘해서 하자?","commandType":4},{"text":"기존 코드에서 심볼 지정하지 않았어?","commandType":4},{"text":"뭐가 에러가 있는지 설명해봐","commandType":4},{"text":"가져올수있는 행들이 다른거보니까 신규 코인같은게 있나보네?","commandType":4},{"text":"그럼 적은데이터 기준에서 누적하는걸로? 만드는게 어때? 어떤건 길고 어떤건 짧으니 그기준맞추는게 문제되어보이는데?\n\nccxt에서 거래소마다 문제 생기는거면 거래소가 가장 오래된 기준으로만 잡으면 되는거아냐?","commandType":4},{"text":"데이터 저장은 어디서할건데?","commandType":4},{"text":"csv 파일 하나로 관리하자?  data/data 이렇게 만드는게 좋을것같아.\ncsv1개 파일에 ","commandType":4},{"text":"@1INCH.csv 이런 기준이 있잖아? 고점 저점을 생각해서 파라미터를 결정하라는 이야기야.. 이해 해? \n\n1. 정확도\n2. 반등율\n3. 알람횟수 이렇게 3개가 중요하거든?\n코드짜서 분석해봐 일단","commandType":4},{"text":"응 그기준으로 코드 돌려봐","commandType":4},{"text":"에러발생","commandType":4},{"text":"내용 확인해라","commandType":4},{"text":"에러가 발생하네?","commandType":4},{"text":"coinalyze 사이트 api 사이트 연동해야해?","commandType":4},{"text":"실제 키값 16개 줬잖아?? ccxt에 펀딩값이랑 OI값이 공유된다고?\n-코드 수정 하지말고 이야기나누자","commandType":4},{"text":"응 그렇게 진행해줘. 엔드포인트 자체가 문제인지 심볼자체의 문제인지 체크해봐. 근데 일부데이터를 가져오는거봐서는 심볼문제는 아닌것같은데","commandType":4},{"text":"체크해볼래?","commandType":4},{"text":"CryptoDataManager 클래스에 save_data 메서드를 추가하는 것이 가장 깔끔한 해결책입니다. 객체지향적으로 데이터 수집과 저장 기능을 하나의 클래스 내에 두면 유지보수와 확장이 더 쉬워집니다. 다음과 같이 수정하는 것을 추천합니다:\n\n```python\n# DataManager 클래스 내에 save_data 메서드 추가\ndef save_data(self, symbol: str, new_data: pd.DataFrame) -> bool:\n    \"\"\"데이터 저장 (기존 데이터와 병합, 소수점 보존)\"\"\"\n    if new_data.empty:\n        logger.warning(f\"{symbol}: 저장할 새 데이터가 없습니다\")\n        return False\n    \n    # 종료 플래그 확인\n    if shutdown_flag.is_set():\n        logger.warning(f\"{symbol} 데이터 저장이 중단되었습니다 (종료 중)\")\n        return False\n        \n    LIVE_DIR.mkdir(parents=True, exist_ok=True)\n    csv_path = LIVE_DIR / f\"{symbol}.csv\"\n    \n    try:\n        # 1. 기존 데이터 로드\n        existing_data = load_existing_data(symbol)\n        \n        # 2. 새 데이터와 기존 데이터 병합\n        if not existing_data.empty:\n            # 기존 데이터의 타임스탬프를 set으로 변환하여 비교 (성능 개선)\n            existing_timestamps = set(existing_data['datetime'].dt.to_pydatetime())\n            # set을 사용한 필터링으로 변경\n            mask = [dt not in existing_timestamps for dt in new_data['datetime'].dt.to_pydatetime()]\n            new_data_filtered = new_data.loc[mask] if any(mask) else pd.DataFrame()\n            \n            if not new_data_filtered.empty:\n                # 인덱스 리셋 후 병합하여 인덱스 충돌 방지\n                df = pd.concat([existing_data.reset_index(drop=True), \n                                new_data_filtered.reset_index(drop=True)], \n                               ignore_index=True)\n                logger.info(f\"{symbol}: {len(new_data_filtered)}행의 새 데이터 추가\")\n            else:\n                logger.info(f\"{symbol}: 새 데이터가 이미 기존 데이터에 포함되어 있습니다\")\n                return True\n        else:\n            df = new_data.copy()\n            logger.info(f\"{symbol}: 새 데이터 {len(new_data)}행 저장 (기존 데이터 없음)\")\n            \n        # 3. 중복 컬럼 제거\n        column_mappings = {\n            'price_open': 'open',\n            'price_high': 'high',\n            'price_low': 'low',\n            'price_close': 'close'\n        }\n        \n        for old_col, new_col in column_mappings.items():\n            if old_col in df.columns and new_col in df.columns:\n                df = df.drop(old_col, axis=1)\n            elif old_col in df.columns:\n                df = df.rename(columns={old_col: new_col})\n            \n        # 4. 불필요한 컬럼 제거\n        columns_to_keep = [\n            'datetime', 'symbol',\n            'open', 'high', 'low', 'close', 'volume',\n            'open_interest',\n            'funding_rate', 'predicted_funding_rate',\n            'liquidation',\n            'long_percentage', 'short_percentage', 'long_short_ratio'\n        ]\n        \n        available_columns = [col for col in columns_to_keep if col in df.columns]\n        \n        # 핵심 컬럼이 없어도 최소한의 메타데이터만이라도 저장\n        min_required_cols = ['datetime', 'symbol']\n        if all(col in available_columns for col in min_required_cols):\n            df = df[available_columns]\n            \n            # 5. 중복 제거 및 정렬\n            df = df.drop_duplicates(subset=['datetime']).reset_index(drop=True)\n            df = df.sort_values('datetime')\n            \n            # 6. 저장 (소수점 보존)\n            # 파일 손상 방지를 위한, 임시 파일에 저장 후 이동\n            temp_path = csv_path.with_name(f\"{symbol}_temp.csv\")\n            \n            # 소수점 반올림 없이 저장 (float_format 인자 사용하지 않음)\n            df.to_csv(temp_path, index=False)\n            \n            # 임시 파일이 성공적으로 작성되었으면 원본 파일로 이동\n            if temp_path.exists():\n                if csv_path.exists():\n                    csv_path.unlink()  # 기존 파일 삭제\n                temp_path.rename(csv_path)  # 임시 파일을 원본 파일로 이동\n            \n            # 핵심 데이터 누락 시 경고 (Series 직접 평가 수정)\n            has_open = 'open' in df.columns\n            missing_open_data = has_open and df['open'].isnull().all()\n            if not has_open or missing_open_data:\n                logger.warning(f\"{symbol} 가격 데이터(OHLCV)가 모두 누락되었습니다. 메타데이터만 저장됨.\")\n            \n            logger.info(f\"{symbol} 데이터 저장 완료: 총 {len(df)}행 (새 데이터: {len(new_data)}행)\")\n            \n            # 7. 데이터 검증\n            if len(df) > 1:  # 최소 2개 이상의 행이 있어야 diff 계산 가능\n                time_diffs = pd.Series(df['datetime'].diff().dt.total_seconds() / 60)\n                gaps = time_diffs[time_diffs > 10]  # 10분 이상의 간격이 있는 경우\n                \n                if not gaps.empty:\n                    logger.warning(f\"{symbol} 데이터 누락 발견:\")\n                    for idx in gaps.index:\n                        gap_start = df['datetime'].iloc[idx-1]\n                        gap_end = df['datetime'].iloc[idx]\n                        gap_minutes = gaps[idx]\n                        logger.warning(f\"  {gap_start} ~ {gap_end} ({gap_minutes:.0f}분 간격)\")\n            \n            return True\n        else:\n            logger.error(f\"{symbol} 필수 컬럼({min_required_cols})이 없어 저장할 수 없습니다.\")\n            return False\n            \n    except Exception as e:\n        logger.error(f\"{symbol} 데이터 저장 실패: {str(e)}\")\n        traceback.print_exc()  # 상세 오류 출력\n        return False\n```\n\n그리고 `collect_historical_data` 메서드 내부에서 다음과 같이 변경합니다:\n\n```python\n# 데이터 병합 및 저장 (전역 함수 save_data 대신 클래스 메서드 사용)\nif all_data_frames:\n    merged_df = merge_dataframes(all_data_frames, symbol)\n    if not merged_df.empty:\n        self.save_data(symbol, merged_df)  # self. 추가\n        logger.info(f\"{symbol} 데이터 병합 및 저장 완료: {len(merged_df)}행\")\n```\n\n동일한 변경을 `collect_data` 메서드에도 적용합니다:\n\n```python\n# 데이터프레임 병합 및 저장\nif all_data_frames:\n    merged_df = merge_dataframes(all_data_frames, symbol)\n    if not merged_df.empty:\n        self.save_data(symbol, merged_df)  # self. 추가\n```\n\n이렇게 수정하면 객체 내에서 일관된 방식으로 데이터를 관리할 수 있으며, 저장 관련 로직을 클래스 내부로 캡슐화하여 유지보수성이 향상됩니다. 또한 에러가 해결되고 데이터 수집 및 저장 과정이 정상적으로 작동할 것입니다.\n\n너의 의견은?","commandType":4},{"text":"에러발생","commandType":4},{"text":"이 경고 메시지는 Pandas의 향후 버전 변경에 관한 내용입니다. 간단히 설명하자면:\n\n### 경고 내용 해석\n\n1. `DatetimeProperties.to_pydatetime` 메서드의 동작 방식이 변경될 예정입니다.\n   - 현재 버전: `datetime.dt.to_pydatetime()`은 NumPy ndarray 객체를 반환합니다.\n   - 미래 버전: Python datetime 객체의 Series를 반환할 예정입니다.\n\n2. 경고 해결 방법:\n   - 현재 동작을 유지하려면 `np.array()`로 감싸야 합니다.\n   - 예: `np.array(existing_data['datetime'].dt.to_pydatetime())`\n\n### 코드 수정 방법\n\n해당 경고를 제거하려면 다음과 같이 수정하면 됩니다:\n\n```python\n# 수정 전\nexisting_timestamps = set(existing_data['datetime'].dt.to_pydatetime())\nmask = [dt not in existing_timestamps for dt in new_data['datetime'].dt.to_pydatetime()]\n\n# 수정 후 (np.array 사용)\nimport numpy as np\nexisting_timestamps = set(np.array(existing_data['datetime'].dt.to_pydatetime()))\nmask = [dt not in existing_timestamps for dt in np.array(new_data['datetime'].dt.to_pydatetime())]\n```\n\n또는 Pandas에서 권장하는 미래 호환 방식으로 변경:\n\n```python\n# 더 간단한 미래 호환 방식\nexisting_timestamps = set(existing_data['datetime'].astype(str))\nmask = [dt.astype(str) not in existing_timestamps for dt in new_data['datetime']]\n```\n\n### 중요 포인트\n\n- 이 경고는 프로그램 기능에 영향을 주지 않는 **미래 호환성 경고**입니다.\n- 현재는 코드가 정상 작동하지만, 향후 Pandas 업데이트 시 문제가 발생할 수 있습니다.\n- 지금 수정하거나 Pandas 업데이트 전에 수정하면 됩니다.\n\n이런 경고는 라이브러리가 더 나은 일관성과 성능을 위해 API를 개선할 때 발생하며, 개발자들이 미리 코드를 준비할 수 있도록 알려주는 역할을 합니다.\n\n해결하고 넘기자 아니면 너무 어렵다","commandType":4},{"text":"구동해보자","commandType":4},{"text":"이거 어디저장되?","commandType":4},{"text":"저장폴더 확실히하고 api키 16개로 번갈아가면서 받아라 ccxt api가 문제야?","commandType":4},{"text":"data/data에 저장하자니까?","commandType":4},{"text":"그냥 csv 파일 data/data 저장 다른거 말고 딱 저장만 변경하자고","commandType":4},{"text":"그냥 csv 파일  data/data 저장 다른거 말고 딱 저장만 변경하자고","commandType":4},{"text":"구동해보자","commandType":4},{"text":"@1000000MOG.csv OI값없지않아?","commandType":4},{"text":"실행해봐","commandType":4},{"text":"@AGLD.csv @BIGTIME.csv @CYBER.csv @MAV.csv @NMR.csv @POWR.csv 여전히 OI 값이 없네? 1분봉에서 해결안되는문제야?","commandType":4},{"text":"실행해봐","commandType":4},{"text":"이 에러 체킹","commandType":4},{"text":"에러가있지? ","commandType":4},{"text":"데이터를 가져올 수 있는 최대치인거야 저게?","commandType":4},{"text":"리미트가 1000개인데 어떻게 바꾸냐 그걸 그럼 오히려 더 꼬이잖아? 근데 과거 ohlcv 보면 7일치 데이터 가져오던데 ? 1회 1000번이야? 그럼 누적치로 가져오면 더가져올 수 있는거야?(질문이야)","commandType":4},{"text":"oi랑 펀딩비는?","commandType":4},{"text":"최대치가 어디까지인지 체크해라","commandType":4},{"text":"아니 제한이 없다고? 1번에 안가져와도되 여러번 누적으로 가져와서 데이터 누적만 된다면 상관없어 ","commandType":4},{"text":"응 그렇게 수정하면 좋을것 같고 데이터 정렬은 타임프레임으로 지속 정리 하는거 가능하지?","commandType":4},{"text":"1분봉 기준으로 받아오면 다른타임프레임이 필요해?","commandType":4},{"text":"데이터 공간은 얼마나 필요할까?","commandType":4},{"text":"크립토 데이터 매니저 어디갔니?","commandType":4},{"text":"아무렇게나 만들어도 되니까 기존에 의견 내던데로 말하자","commandType":4},{"text":"데이터 최적화 하고 최대 1년데이터까지 가져오게 \n프로그램짜 보자 근데 기존 대화를 통해서 가져온 데이터가 저게 다야? API키도 있었고 내용 많았는데?","commandType":4},{"text":"적용해봐","commandType":4},{"text":"\"\"\"\n암호화폐 데이터 통합 관리 시스템 (CCXT 데이터 보완 및 장기 보관 기능 추가)\n- 과거 데이터 누락 검사 및 복구 (CCXT로 보완)\n- 실시간 데이터 수집 및 저장\n- 단일 CSV 파일에 시간순 정렬 저장\n- 자동 심볼 추가 및 관리\n- 안전한 종료 메커니즘\n- 데이터 장기 보관\n- 소수점 정확도 유지\n\"\"\"\n\nimport os\nimport logging\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nimport time\nimport base64\nimport signal\nimport traceback\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Union\nimport random\nimport threading\nimport queue\nimport ccxt\nimport shutil\nimport zipfile\n\n# 안전한 종료를 위한 전역 플래그\nshutdown_flag = threading.Event()\n\n# 기본 설정\nBASE_URL = \"https://api.coinalyze.net/v1\"\nDATA_DIR = Path(\"data\")\nLIVE_DIR = DATA_DIR / \"live\"\nBACKUP_DIR = DATA_DIR / \"backup\"\nARCHIVE_DIR = DATA_DIR / \"archive\"\nLONG_TERM_DIR = DATA_DIR / \"longterm\"\n\n# API 요청 간격 설정 (초)\nAPI_REQUEST_INTERVAL = 0.5 # API 요청 사이 대기 시간 (초)\nCOLLECTION_INTERVAL = 300 # 데이터 수집 간격 (초) - 5분\n\n# API 엔드포인트\nENDPOINTS = {\n\"exchanges\": \"/exchanges\",\n\"future_markets\": \"/future-markets\",\n\"spot_markets\": \"/spot-markets\",\n\"open_interest\": \"/open-interest\",\n\"funding_rate\": \"/funding-rate\",\n\"predicted_funding_rate\": \"/predicted-funding-rate\",\n\"open_interest_history\": \"/open-interest-history\",\n\"funding_rate_history\": \"/funding-rate-history\",\n\"predicted_funding_rate_history\": \"/predicted-funding-rate-history\",\n\"liquidation_history\": \"/liquidation-history\",\n\"long_short_ratio_history\": \"/long-short-ratio-history\",\n\"ohlcv_history\": \"/ohlcv-history\"\n}\n\n# API 키 설정 (16개 키를 4개 티어로 구분)\nAPI_KEYS = {\n\"tier1\": [\n\"0d0ba171-4185-4a58-bc02-8c8627cd1f54\",\n\"e629efa9-68b0-4b6b-b794-fa2d9e379b79\",\n\"c9779cfd-a85c-4e58-a378-9474174a075e\",\n\"58a39d7b-84e5-4852-8375-90de055cba18\"\n],\n\"tier2\": [\n\"7b80a59d-f0f9-4a83-81f7-1314dbdd9dc7\",\n\"2b951698-64e7-4a86-9930-c503d4e29e54\",\n\"10b24c79-ddd1-4046-94b6-0af0d23b241e\",\n\"fac040b5-5043-4351-b8f3-09c1d8cfd78f\"\n],\n\"tier3\": [\n\"fe1caf7e-5e27-4f0c-9d14-2a4611db625f\",\n\"fc9e5080-9607-46e8-b48c-deca57591990\",\n\"a82fd8a4-873f-4619-9b9f-d8a29373b5b8\",\n\"6431161c-8815-4d18-846d-55e00863682e\"\n],\n\"tier4\": [\n\"07cc6de0-0d02-41b8-acdc-dd1284bf5730\",\n\"f2842bed-d43e-4152-a0b5-68d9d9ed30b0\",\n\"013d0da7-76ea-4699-97d7-6e0f66857939\",\n\"7906d7bf-b493-42ad-a578-0cd13c6a070c\"\n]\n}\n\n# CCXT 설정 - 공개 데이터만 사용하므로 API 키 불필요\nCCXT_EXCHANGES = [\n\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"\n]\n\n# 로깅 설정\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[\nlogging.StreamHandler(),\nlogging.FileHandler(\"crypto_data_manager.log\", encoding='utf-8')\n]\n)\nlogger = logging.getLogger(__name__)\n\ndef signal_handler(sig, frame):\n\"\"\"시스템 종료 신호를 처리하는 핸들러\"\"\"\nlogger.warning(\"종료 신호를 받았습니다. 안전하게 종료합니다...\")\nshutdown_flag.set()\n\n# 시그널 핸들러 등록\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\n\ndef check_stop_file():\n\"\"\"종료 파일 확인\"\"\"\nif os.path.exists(\"stop.signal\"):\nlogger.warning(\"종료 파일이 감지되었습니다. 안전하게 종료합니다...\")\ntry:\nos.remove(\"stop.signal\") # 종료 파일 삭제\nexcept:\npass\nshutdown_flag.set()\nreturn True\nreturn False\n\nclass APIKeyManager:\n\"\"\"API 키 관리 클래스\"\"\"\ndef __init__(self):\nself.tier_keys = API_KEYS\nself.current_key_index = {tier: 0 for tier in API_KEYS.keys()}\nself.key_locks = {tier: threading.Lock() for tier in API_KEYS.keys()}\nself.key_rate_limits = {key: {'last_used': 0, 'count': 0} for tier in API_KEYS.values() for key in tier}\nself.rate_limit_reset_time = time.time() + 60 # 1분 후 리셋\n\ndef get_next_key(self, tier: str) -> str:\n\"\"\"라운드 로빈 방식으로 다음 API 키 반환\"\"\"\nif tier not in self.tier_keys or not self.tier_keys[tier]:\navailable_tiers = [t for t in self.tier_keys if self.tier_keys[t]]\nif not available_tiers:\nraise ValueError(\"사용 가능한 API 키가 없습니다\")\ntier = random.choice(available_tiers)\n\nwith self.key_locks[tier]:\nkeys = self.tier_keys[tier]\ncurrent_index = self.current_key_index[tier]\n\n# 현재 시간이 리셋 시간을 초과했는지 확인\ncurrent_time = time.time()\nif current_time > self.rate_limit_reset_time:\n# 사용량 카운터 리셋\nfor key_info in self.key_rate_limits.values():\nkey_info['count'] = 0\nself.rate_limit_reset_time = current_time + 60 # 다음 1분 후 리셋\n\n# 사용량이 가장 적은 키를 찾아 반환\nmin_count = float('inf')\nselected_key = None\n\nfor i in range(len(keys)):\nidx = (current_index + i) % len(keys)\nkey = keys[idx]\n\nif self.key_rate_limits[key]['count'] < min_count:\nmin_count = self.key_rate_limits[key]['count']\nselected_key = key\nself.current_key_index[tier] = (idx + 1) % len(keys)\n\n# 키 사용량 기록\nself.key_rate_limits[selected_key]['last_used'] = current_time\nself.key_rate_limits[selected_key]['count'] += 1\n\n# API 요청 간격 조절을 위한 대기\nlast_used = self.key_rate_limits[selected_key]['last_used']\nif current_time - last_used < API_REQUEST_INTERVAL:\ntime.sleep(API_REQUEST_INTERVAL - (current_time - last_used))\n\nreturn selected_key\n\nclass CcxtManager:\n\"\"\"CCXT 거래소 관리 및 심볼 매핑 클래스\"\"\"\ndef __init__(self):\nself.exchanges = {}\nself.symbol_mappings = {} # base_symbol -> {exchange_id -> ccxt_symbol}\nself.exchange_priority = [\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"]\nself.init_exchanges()\nself.supported_symbols = set() # CCXT에서 지원하는 심볼 목록\nself.timeframe_map = {\n\"1min\": \"1m\", \"5min\": \"5m\", \"15min\": \"15m\", \"30min\": \"30m\",\n\"1h\": \"1h\", \"4h\": \"4h\", \"1d\": \"1d\", \"1w\": \"1w\"\n}\n\ndef init_exchanges(self):\n\"\"\"CCXT 거래소 초기화 (API 키 없이)\"\"\"\nfor exchange_id in CCXT_EXCHANGES:\ntry:\nif exchange_id in ccxt.exchanges:\nexchange_class = getattr(ccxt, exchange_id)\nself.exchanges[exchange_id] = exchange_class({\n'enableRateLimit': True,\n'options': {\n'defaultType': 'future' # 선물 거래 기본 설정\n}\n})\nlogger.info(f\"CCXT {exchange_id} 거래소 초기화 성공\")\nexcept Exception as e:\nlogger.error(f\"CCXT {exchange_id} 거래소 초기화 실패: {str(e)}\")\n\ndef load_all_markets(self):\n\"\"\"모든 거래소의 시장 정보 로드\"\"\"\nfor exchange_id, exchange in self.exchanges.items():\ntry:\nexchange.load_markets()\nlogger.info(f\"{exchange_id} 시장 정보 로드 성공: {len(exchange.symbols)}개 심볼\")\nexcept Exception as e:\nlogger.error(f\"{exchange_id} 시장 정보 로드 실패: {str(e)}\")\n\ndef discover_symbol_mappings(self, base_symbols):\n\"\"\"여러 기본 심볼에 대한 CCXT 매핑 자동 탐색\"\"\"\nlogger.info(f\"CCXT 심볼 매핑 탐색 시작: {len(base_symbols)}개 심볼\")\n\n# 거래소 시장 정보 로드\nself.load_all_markets()\n\n# 각 심볼별 매핑 탐색\nfor base_symbol in base_symbols:\nself.discover_symbol_mapping(base_symbol)\n\n# 매핑 정보 저장\nself.save_symbol_mappings()\n\nlogger.info(f\"CCXT 심볼 매핑 탐색 완료: {len(self.symbol_mappings)}개 심볼\")\n\ndef discover_symbol_mapping(self, base_symbol):\n\"\"\"단일 기본 심볼에 대한 CCXT 매핑 탐색\"\"\"\nif base_symbol in self.symbol_mappings:\nreturn\n\nself.symbol_mappings[base_symbol] = {}\n\n# 거래소별 검색 패턴\nsearch_patterns = {\n\"binance\": [\nf\"{base_symbol}/USDT:USDT\", # 영구 선물\nf\"{base_symbol}USDT_PERP\",\nf\"{base_symbol}/USDT\" # 현물\n],\n\"bybit\": [\nf\"{base_symbol}USDT\", # 영구 선물\nf\"{base_symbol}/USDT\" # 현물\n],\n\"okx\": [\nf\"{base_symbol}-USDT-SWAP\", # 영구 선물\nf\"{base_symbol}/USDT:USDT\",\nf\"{base_symbol}/USDT\" # 현물\n],\n\"kucoin\": [\nf\"{base_symbol}USDTM\", # 영구 선물\nf\"{base_symbol}-USDT\", # 영구 선물 (다른 형식)\nf\"{base_symbol}/USDT\" # 현물\n],\n\"bitget\": [\nf\"{base_symbol}USDT_UMCBL\", # 영구 선물\nf\"{base_symbol}/USDT\" # 현물\n]\n}\n\n# 거래소 우선순위대로 검색\nfor exchange_id in self.exchange_priority:\nif exchange_id not in self.exchanges:\ncontinue\n\nexchange = self.exchanges[exchange_id]\npatterns = search_patterns.get(exchange_id, [f\"{base_symbol}/USDT\"])\n\nfor pattern in patterns:\nfound = False\nfor symbol in exchange.symbols:\nnormalized_symbol = symbol.upper()\nnormalized_pattern = pattern.upper()\n\n# 정확한 일치 또는 포함 관계 확인\nif normalized_symbol == normalized_pattern or normalized_pattern in normalized_symbol:\nself.symbol_mappings[base_symbol][exchange_id] = symbol\nlogger.info(f\"심볼 매핑 발견: {base_symbol} -> {exchange_id}:{symbol}\")\nself.supported_symbols.add(base_symbol)\nfound = True\nbreak\n\nif found:\nbreak\n\ndef save_symbol_mappings(self):\n\"\"\"심볼 매핑 정보 파일로 저장\"\"\"\ntry:\nfilepath = DATA_DIR / \"symbol_mappings.json\"\nwith open(filepath, 'w') as f:\njson.dump({\n\"mappings\": self.symbol_mappings,\n\"supported_symbols\": list(self.supported_symbols),\n\"timestamp\": datetime.now().isoformat()\n}, f, indent=2)\nlogger.info(f\"심볼 매핑 정보 저장 완료: {filepath}\")\nexcept Exception as e:\nlogger.error(f\"심볼 매핑 정보 저장 실패: {str(e)}\")\n\ndef load_symbol_mappings(self):\n\"\"\"저장된 심볼 매핑 정보 로드\"\"\"\ntry:\nfilepath = DATA_DIR / \"symbol_mappings.json\"\nif filepath.exists():\nwith open(filepath, 'r') as f:\ndata = json.load(f)\nself.symbol_mappings = data.get(\"mappings\", {})\nself.supported_symbols = set(data.get(\"supported_symbols\", []))\nlogger.info(f\"심볼 매핑 정보 로드 완료: {len(self.symbol_mappings)}개 심볼\")\nreturn True\nreturn False\nexcept Exception as e:\nlogger.error(f\"심볼 매핑 정보 로드 실패: {str(e)}\")\nreturn False\n\ndef get_ccxt_symbol(self, base_symbol, exchange_id=None):\n\"\"\"CCXT 심볼 가져오기 (특정 거래소 또는 우선순위 기반)\"\"\"\n# 매핑 정보 확인\nif base_symbol not in self.symbol_mappings:\nself.discover_symbol_mapping(base_symbol)\n\n# 특정 거래소 지정한 경우\nif exchange_id:\nreturn self.symbol_mappings.get(base_symbol, {}).get(exchange_id)\n\n# 거래소 우선순위 기준 탐색\nfor ex_id in self.exchange_priority:\nif ex_id in self.symbol_mappings.get(base_symbol, {}):\nreturn self.symbol_mappings[base_symbol][ex_id], ex_id\n\nreturn None, None\n\ndef fetch_ohlcv(self, base_symbol, timeframe='5m', since=None, limit=1000):\n\"\"\"CCXT를 통해 OHLCV 데이터 가져오기\"\"\"\nif shutdown_flag.is_set():\nreturn pd.DataFrame()\n\n# 심볼 매핑 확인 및 가져오기\nccxt_symbol, exchange_id = self.get_ccxt_symbol(base_symbol)\n\nif not ccxt_symbol or not exchange_id:\nlogger.warning(f\"CCXT 심볼 매핑을 찾을 수 없음: {base_symbol}\")\nreturn pd.DataFrame()\n\n# 타임프레임 변환\nccxt_timeframe = self.timeframe_map.get(timeframe, timeframe)\n\n# since 변환 (timestamp to milliseconds)\nsince_ms = int(since.timestamp() * 1000) if since else None\n\n# 데이터 가져오기 시도\ntry:\nexchange = self.exchanges[exchange_id]\n\n# OHLCV 데이터 가져오기\nohlcv = exchange.fetch_ohlcv(ccxt_symbol, ccxt_timeframe, since_ms, limit)\n\nif not ohlcv:\nlogger.warning(f\"CCXT에서 데이터 없음: {exchange_id}:{ccxt_symbol}\")\nreturn pd.DataFrame()\n\n# DataFrame 변환\ndf = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\ndf['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\ndf['symbol'] = base_symbol\ndf['exchange'] = exchange_id\n\nlogger.info(f\"CCXT 데이터 수집 성공: {exchange_id}:{ccxt_symbol}, {len(df)}행\")\nreturn df\n\nexcept Exception as e:\nlogger.error(f\"CCXT 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\nreturn pd.DataFrame()\n\ndef fetch_funding_rate(self, base_symbol, timeframe='1h', since=None, limit=500):\n\"\"\"CCXT를 통해 펀딩비 데이터 가져오기 (지원하는 거래소만)\"\"\"\nif shutdown_flag.is_set():\nreturn pd.DataFrame()\n\n# 펀딩비를 지원하는 거래소들\nfunding_supported = [\"binance\", \"bybit\", \"okx\"]\nfunding_data = []\n\n# 지원하는 거래소에서 데이터 수집 시도\nfor exchange_id in funding_supported:\nif exchange_id not in self.exchanges:\ncontinue\n\nccxt_symbol = self.get_ccxt_symbol(base_symbol, exchange_id)\nif not ccxt_symbol:\ncontinue\n\ntry:\nexchange = self.exchanges[exchange_id]\n\n# 1. fetchFundingRateHistory 사용 (최신 데이터)\nif hasattr(exchange, 'fetchFundingRateHistory') and callable(getattr(exchange, 'fetchFundingRateHistory')):\nsince_ms = int(since.timestamp() * 1000) if since else None\nfunding_history = exchange.fetch_funding_rate_history(ccxt_symbol, since_ms, limit)\n\nif funding_history:\nfor entry in funding_history:\nfunding_data.append({\n'datetime': pd.to_datetime(entry['timestamp'], unit='ms'),\n'symbol': base_symbol,\n'exchange': exchange_id,\n'funding_rate': entry['fundingRate']\n})\n\n# 2. fetchFundingRate 사용 (현재 펀딩비)\nelif hasattr(exchange, 'fetchFundingRate') and callable(getattr(exchange, 'fetchFundingRate')):\nfunding_info = exchange.fetch_funding_rate(ccxt_symbol)\nif funding_info:\nfunding_data.append({\n'datetime': pd.to_datetime(funding_info['timestamp'], unit='ms'),\n'symbol': base_symbol,\n'exchange': exchange_id,\n'funding_rate': funding_info['fundingRate']\n})\n\nexcept Exception as e:\nlogger.error(f\"CCXT 펀딩비 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n\n# 수집된 데이터를 DataFrame으로 변환\nif funding_data:\ndf = pd.DataFrame(funding_data)\nlogger.info(f\"CCXT 펀딩비 데이터 수집 성공: {base_symbol}, {len(df)}행\")\nreturn df\nelse:\nreturn pd.DataFrame()\n\n# API 키 매니저 인스턴스 생성\napi_key_manager = APIKeyManager()\n\n# CCXT 매니저 인스턴스 생성\nccxt_manager = CcxtManager()\n\ndef get_simple_symbol(symbol: str) -> str:\n\"\"\"심볼명을 단순화 (예: BTCUSDT_PERP.A -> BTC)\"\"\"\nif 'USDT_PERP.A' in symbol:\nreturn symbol.split('USDT_')[0]\nreturn symbol.replace('.csv', '')\n\ndef exponential_backoff(attempt, base_delay=2, max_delay=120):\n\"\"\"지수 백오프 알고리즘\"\"\"\ndelay = min(max_delay, base_delay * (2 ** attempt))\njitter = random.uniform(0, 0.1 * delay) # 10% 지터 추가\nreturn delay + jitter\n\ndef make_request(endpoint: str, params: dict = None, tier: str = \"tier1\", max_retries: int = 5) -> Optional[dict]:\n\"\"\"API 요청 수행 (지수 백오프 적용)\"\"\"\nurl = f\"{BASE_URL}{endpoint}\"\n\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nlogger.warning(\"종료 중에는 API 요청을 수행하지 않습니다\")\nreturn None\n\nfor attempt in range(max_retries):\ntry:\napi_key = api_key_manager.get_next_key(tier)\nheaders = {\n\"Accept\": \"application/json\",\n\"Api-Key\": api_key\n}\n\nresponse = requests.get(url, headers=headers, params=params, timeout=60)\n\nif response.status_code == 429: # Rate limit exceeded\nsleep_time = exponential_backoff(attempt)\nlogger.warning(f\"API 속도 제한 초과. {sleep_time:.2f}초 후 재시도... (시도 {attempt+1}/{max_retries})\")\ntime.sleep(sleep_time)\ncontinue\n\nif response.status_code == 401: # 인증 실패\nlogger.error(f\"API 키 인증 실패: {api_key}\")\ncontinue\n\nresponse.raise_for_status()\nreturn response.json()\n\nexcept requests.exceptions.Timeout:\nsleep_time = exponential_backoff(attempt)\nlogger.error(f\"API 요청 타임아웃 ({endpoint}) - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\ntime.sleep(sleep_time)\nexcept requests.exceptions.RequestException as e:\nsleep_time = exponential_backoff(attempt)\nlogger.error(f\"API 요청 실패 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\ntime.sleep(sleep_time)\nexcept Exception as e:\nsleep_time = exponential_backoff(attempt)\nlogger.error(f\"예상치 못한 오류 발생 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\ntime.sleep(sleep_time)\n\nreturn None\n\ndef get_all_symbols() -> List[str]:\n\"\"\"사용 가능한 모든 심볼 목록 조회\"\"\"\ndata = make_request(ENDPOINTS[\"future_markets\"])\n\nif not data:\nlogger.error(\"심볼 목록을 가져올 수 없습니다\")\nreturn []\n\nsymbols = []\nif isinstance(data, dict):\nmarkets = data.get('markets', [])\nelse:\nmarkets = data\n\nfor market in markets:\nsymbol = market.get('symbol', '')\nif symbol.endswith('USDT_PERP.A'):\nbase_symbol = get_simple_symbol(symbol)\nsymbols.append(base_symbol)\n\nlogger.info(f\"총 {len(symbols)}개의 심볼을 찾았습니다\")\n\n# 심볼 목록 파일로 저장\nwith open(DATA_DIR / 'symbols.json', 'w') as f:\njson.dump({\n'symbols': symbols,\n'count': len(symbols),\n'timestamp': datetime.now().isoformat()\n}, f, indent=2)\n\nreturn sorted(symbols)\n\ndef extract_data_from_response(data, data_type: str):\n\"\"\"API 응답에서 데이터 추출 개선\"\"\"\nhistory = []\n\ntry:\n# 응답 데이터 구조 확인\nif isinstance(data, list):\n# 리스트 응답 처리\nif len(data) > 0:\nif isinstance(data[0], dict):\nif 'history' in data[0]:\n# 기존 구조: [{\"history\": [...]}]\nhistory = data[0]['history']\nelse:\n# 새 구조: 리스트가 직접 데이터인 경우\nhistory = data\nelse:\n# 직접 데이터 리스트인 경우\nhistory = data\nelif isinstance(data, dict):\n# 딕셔너리 응답 처리\nif 'history' in data:\nhistory = data['history']\nelif 'data' in data:\nhistory = data['data']\nelif 'result' in data:\nhistory = data['result']\nelif 'items' in data:\nhistory = data['items']\nelse:\n# 다른 키가 없으면 응답 전체를 사용\npotential_data = []\nfor key, value in data.items():\nif isinstance(value, list) and len(value) > 0:\npotential_data.extend(value)\nif potential_data:\nhistory = potential_data\nexcept Exception as e:\nlogger.error(f\"{data_type} 데이터 파싱 중 오류: {str(e)}\")\n\nreturn history\n\ndef normalize_columns(df, data_type: str) -> pd.DataFrame:\n\"\"\"데이터 프레임 컬럼 정규화 (소수점 보존)\"\"\"\n# 타임스탬프 변환\nif 't' in df.columns:\ndf['datetime'] = pd.to_datetime(df['t'], unit='s')\nelif 'timestamp' in df.columns:\ndf['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\nelif 'time' in df.columns:\ndf['datetime'] = pd.to_datetime(df['time'], unit='s')\n\n# 데이터 타입별 컬럼 처리\nif data_type == 'ohlcv':\ncolumn_mappings = {\n'o': 'open', 'open': 'open', 'price_open': 'open',\n'h': 'high', 'high': 'high', 'price_high': 'high',\n'l': 'low', 'low': 'low', 'price_low': 'low',\n'c': 'close', 'close': 'close', 'price_close': 'close',\n'v': 'volume', 'volume': 'volume', 'vol': 'volume'\n}\nelif data_type == 'oi':\ncolumn_mappings = {\n'c': 'open_interest', 'value': 'open_interest',\n'openInterest': 'open_interest', 'oi': 'open_interest'\n}\nelif data_type == 'funding':\ncolumn_mappings = {\n'c': 'funding_rate', 'rate': 'funding_rate',\n'fundingRate': 'funding_rate', 'funding': 'funding_rate'\n}\nelif data_type == 'predicted_funding':\ncolumn_mappings = {\n'c': 'predicted_funding_rate', 'rate': 'predicted_funding_rate',\n'predictedRate': 'predicted_funding_rate', 'predicted': 'predicted_funding_rate'\n}\nelif data_type == 'liquidation':\ncolumn_mappings = {\n'c': 'liquidation', 'value': 'liquidation',\n'liq': 'liquidation', 'liquidations': 'liquidation'\n}\nelif data_type == 'long_short_ratio':\ncolumn_mappings = {\n'c': 'long_short_ratio', 'ratio': 'long_short_ratio',\n'l': 'long_percentage', 'long': 'long_percentage', 'longPercentage': 'long_percentage',\n's': 'short_percentage', 'short': 'short_percentage', 'shortPercentage': 'short_percentage'\n}\nelse:\ncolumn_mappings = {}\n\n# 컬럼 매핑 적용\nfor old_col, new_col in column_mappings.items():\nif old_col in df.columns and old_col != new_col:\ndf[new_col] = df[old_col]\n\nreturn df\n\ndef validate_data(df, data_type: str) -> pd.DataFrame:\n\"\"\"데이터 검증 (소수점 보존)\"\"\"\nif df.empty:\nreturn df\n\n# 타임스탬프가 없는 행 제거\nif 'datetime' in df.columns:\ndf = df[df['datetime'].notna()]\n\n# 데이터 타입별 검증\nif data_type == 'ohlcv':\n# 가격 데이터 검증 (음수 값 제거, 소수점은 유지)\nfor col in ['open', 'high', 'low', 'close']:\nif col in df.columns:\n# 0 이하 값 확인 후 경고만 발생 (제거하지 않음)\nzero_or_neg = df[df[col] <= 0]\nif not zero_or_neg.empty:\nlogger.warning(f\"{data_type}: {len(zero_or_neg)}개 행에서 {col} 값이 0 이하입니다. 데이터 유지됨.\")\n\n# 볼륨 검증 (음수 제거)\nif 'volume' in df.columns:\nneg_volume = df[df['volume'] < 0]\nif not neg_volume.empty:\nlogger.warning(f\"{data_type}: {len(neg_volume)}개 행에서 volume 값이 음수입니다. 해당 값을 0으로 설정합니다.\")\ndf.loc[df['volume'] < 0, 'volume'] = 0\n\nelif data_type == 'funding' or data_type == 'predicted_funding':\n# 극단적인 펀딩비 값 확인 (소수점은 유지)\nfunding_col = 'funding_rate' if data_type == 'funding' else 'predicted_funding_rate'\nif funding_col in df.columns:\nextreme_funding = df[(df[funding_col] < -0.15) | (df[funding_col] > 0.15)]\nif not extreme_funding.empty:\nlogger.warning(f\"{data_type}: {len(extreme_funding)}개 행에서 {funding_col} 값이 극단적입니다 (-0.15 미만 또는 0.15 초과). 데이터 유지됨.\")\n\nelif data_type == 'long_short_ratio':\n# 롱/숏 퍼센티지 합이 100%인지 확인 (오차 범위 ±2%)\nif 'long_percentage' in df.columns and 'short_percentage' in df.columns:\ntotal = df['long_percentage'] + df['short_percentage']\ninvalid_rows = df[(total < 98) | (total > 102)]\nif not invalid_rows.empty:\nlogger.warning(f\"{data_type}: {len(invalid_rows)}개 행에서 long_percentage와 short_percentage 합이 100%와 크게 다릅니다. 데이터 유지됨.\")\n\nreturn df\n\ndef process_ohlcv_data(data) -> pd.DataFrame:\n\"\"\"OHLCV 데이터 처리 (대체 소스용)\"\"\"\nhistory = extract_data_from_response(data, \"ohlcv\")\n\nif not history:\nreturn pd.DataFrame()\n\ntry:\ndf = pd.DataFrame(history)\ndf = normalize_columns(df, \"ohlcv\")\n\nif 'datetime' not in df.columns and 't' in df.columns:\ndf['datetime'] = pd.to_datetime(df['t'], unit='s')\n\n# 필수 OHLCV 컬럼 확인\nrequired_cols = ['open', 'high', 'low', 'close']\nmissing_cols = [col for col in required_cols if col not in df.columns]\n\nif missing_cols:\nlogger.warning(f\"OHLCV 대체 소스에서 필수 컬럼 누락: {missing_cols}\")\nreturn pd.DataFrame()\n\nreturn df\nexcept Exception as e:\nlogger.error(f\"OHLCV 대체 소스 데이터 처리 오류: {str(e)}\")\nreturn pd.DataFrame()\n\ndef merge_dataframes(all_data_frames: Dict[str, pd.DataFrame], symbol: str) -> pd.DataFrame:\n\"\"\"여러 데이터프레임 병합 (소수점 보존)\"\"\"\nif not all_data_frames:\nreturn pd.DataFrame()\n\n# 가능한 모든 기준 프레임 후보 (우선순위 순)\nbase_frame_candidates = [\"ohlcv\", \"long_short_ratio\", \"funding\", \"oi\"]\nbase_df = None\n\n# 우선순위에 따라 기준 프레임 선택\nfor candidate in base_frame_candidates:\nif candidate in all_data_frames and not all_data_frames[candidate].empty:\nbase_df = all_data_frames[candidate].copy()\nlogger.info(f\"{symbol} 기준 프레임으로 {candidate} 선택 ({len(base_df)}행)\")\nbreak\n\n# 기준 프레임이 없으면 빈 DataFrame 반환\nif base_df is None:\nlogger.warning(f\"{symbol} 병합할 기준 프레임이 없습니다.\")\nreturn pd.DataFrame()\n\n# 심볼 정보 확인 및 추가\nif 'symbol' not in base_df.columns:\nbase_df['symbol'] = symbol\n\n# 기본 프레임 컬럼 이름 확인 (중복 방지 위해)\n# 문제가 되는 t, o, h, l, c 등 짧은 컬럼명을 미리 표준화\nif 't' in base_df.columns:\nbase_df.rename(columns={'t': 'timestamp'}, inplace=True)\nfor col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\nif col_old in base_df.columns:\nbase_df.rename(columns={col_old: col_new}, inplace=True)\n\n# 인덱스 리셋 (병합을 위한 준비)\nbase_df = base_df.reset_index(drop=True)\n\n# 표준 컬럼명 리스트\nstd_cols = {\n'open', 'high', 'low', 'close', 'volume',\n'open_interest', 'funding_rate', 'predicted_funding_rate',\n'liquidation', 'long_percentage', 'short_percentage', 'long_short_ratio',\n'datetime', 'symbol'\n}\n\n# 각 데이터프레임을 병합 (왼쪽 조인 방식으로 변경)\nfor data_type, df in all_data_frames.items():\n# 기준 프레임 또는 빈 프레임 건너뛰기\nis_base_frame = data_type == list(base_frame_candidates)[0]\nif is_base_frame or df.empty:\ncontinue\n\n# 표준화된 컬럼명으로 변환\nif 't' in df.columns:\ndf.rename(columns={'t': 'timestamp'}, inplace=True)\nfor col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\nif col_old in df.columns:\ndf.rename(columns={col_old: col_new}, inplace=True)\n\n# 기준 프레임에 없는 컬럼만 추출\ncolumns_to_merge = []\nfor col in df.columns:\n# 표준 컬럼 외의 컬럼은 고유한 이름으로 변경\nif col not in std_cols and col in base_df.columns:\nnew_col = f\"{col}_{data_type}\"\ndf.rename(columns={col: new_col}, inplace=True)\ncolumns_to_merge.append(new_col)\nelif col not in base_df.columns and col != 'datetime' and col != 'symbol':\ncolumns_to_merge.append(col)\n\n# 병합할 컬럼이 있고 datetime 열이 있는 경우에만 병합 진행\nif len(columns_to_merge) > 0 and 'datetime' in df.columns:\n# 인덱스 리셋 (병합을 위한 준비)\ndf_to_merge = df.reset_index(drop=True)\nmerge_df = df_to_merge[['datetime'] + columns_to_merge].copy()\n\n# 병합 (왼쪽 조인 방식으로 변경)\ntry:\nbase_df = pd.merge(base_df, merge_df, on='datetime', how='left', suffixes=('', f'_{data_type}'))\nexcept Exception as e:\nlogger.error(f\"데이터프레임 병합 오류 ({data_type}): {str(e)}\")\ntraceback.print_exc() # 상세 오류 출력\n# 병합 실패해도 계속 진행\n\n# 최종 정리\ntry:\nbase_df = base_df.sort_values('datetime').reset_index(drop=True)\nexcept Exception as e:\nlogger.error(f\"데이터프레임 정렬 오류: {str(e)}\")\n\nreturn base_df\n\ndef load_existing_data(symbol: str) -> pd.DataFrame:\n\"\"\"기존 데이터 로드 (소수점 보존)\"\"\"\ncsv_path = LIVE_DIR / f\"{symbol}.csv\"\n\nif not csv_path.exists():\nreturn pd.DataFrame()\n\ntry:\n# 소수점 반올림 없이 데이터 로드\ndf = pd.read_csv(csv_path)\ndf['datetime'] = pd.to_datetime(df['datetime'])\nlogger.info(f\"{symbol} 기존 데이터 로드: {len(df)}행\")\nreturn df\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 로드 실패: {str(e)}\")\ntraceback.print_exc() # 상세 오류 출력\n\n# 손상된 파일 처리\ntry:\n# 손상된 파일 백업\ncorrupted_dir = DATA_DIR / \"corrupted\"\ncorrupted_dir.mkdir(parents=True, exist_ok=True)\n\nbackup_file = corrupted_dir / f\"{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\nos.rename(csv_path, backup_file)\nlogger.warning(f\"손상된 파일 이동: {csv_path} -> {backup_file}\")\nexcept Exception as backup_err:\nlogger.error(f\"손상된 파일 백업 실패: {str(backup_err)}\")\n\nreturn pd.DataFrame()\n\ndef save_data(symbol: str, new_data: pd.DataFrame) -> bool:\n\"\"\"데이터 저장 (기존 데이터와 병합, 소수점 보존)\"\"\"\nif new_data.empty:\nlogger.warning(f\"{symbol}: 저장할 새 데이터가 없습니다\")\nreturn False\n\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nlogger.warning(f\"{symbol} 데이터 저장이 중단되었습니다 (종료 중)\")\nreturn False\n\nLIVE_DIR.mkdir(parents=True, exist_ok=True)\ncsv_path = LIVE_DIR / f\"{symbol}.csv\"\n\ntry:\n# 1. 기존 데이터 로드\nexisting_data = load_existing_data(symbol)\n\n# 2. 새 데이터와 기존 데이터 병합\nif not existing_data.empty:\n# 기존 데이터의 타임스탬프를 set으로 변환하여 비교 (성능 개선)\nexisting_timestamps = set(existing_data['datetime'].dt.to_pydatetime())\n# set을 사용한 필터링으로 변경\nmask = [dt not in existing_timestamps for dt in new_data['datetime'].dt.to_pydatetime()]\nnew_data_filtered = new_data.loc[mask] if any(mask) else pd.DataFrame()\n\nif not new_data_filtered.empty:\n# 인덱스 리셋 후 병합하여 인덱스 충돌 방지\ndf = pd.concat([existing_data.reset_index(drop=True),\nnew_data_filtered.reset_index(drop=True)],\nignore_index=True)\nelse:\nlogger.info(f\"{symbol}: 새 데이터가 이미 기존 데이터에 포함되어 있습니다\")\nreturn True\nelse:\ndf = new_data.copy()\n\n# 3. 중복 컬럼 제거\ncolumn_mappings = {\n'price_open': 'open',\n'price_high': 'high',\n'price_low': 'low',\n'price_close': 'close'\n}\n\nfor old_col, new_col in column_mappings.items():\nif old_col in df.columns and new_col in df.columns:\ndf = df.drop(old_col, axis=1)\nelif old_col in df.columns:\ndf = df.rename(columns={old_col: new_col})\n\n# 4. 불필요한 컬럼 제거\ncolumns_to_keep = [\n'datetime', 'symbol',\n'open', 'high', 'low', 'close', 'volume',\n'open_interest',\n'funding_rate', 'predicted_funding_rate',\n'liquidation',\n'long_percentage', 'short_percentage', 'long_short_ratio'\n]\n\navailable_columns = [col for col in columns_to_keep if col in df.columns]\n\n# 핵심 컬럼이 없어도 최소한의 메타데이터만이라도 저장\nmin_required_cols = ['datetime', 'symbol']\nif all(col in available_columns for col in min_required_cols):\ndf = df[available_columns]\n\n# 5. 중복 제거 및 정렬\ndf = df.drop_duplicates(subset=['datetime']).reset_index(drop=True)\ndf = df.sort_values('datetime')\n\n# 6. 저장 (소수점 보존)\n# 파일 손상 방지를 위한, 임시 파일에 저장 후 이동\ntemp_path = csv_path.with_name(f\"{symbol}_temp.csv\")\n\n# 소수점 반올림 없이 저장 (float_format 인자 사용하지 않음)\ndf.to_csv(temp_path, index=False)\n\n# 임시 파일이 성공적으로 작성되었으면 원본 파일로 이동\nif temp_path.exists():\nif csv_path.exists():\ncsv_path.unlink() # 기존 파일 삭제\ntemp_path.rename(csv_path) # 임시 파일을 원본 파일로 이동\n\n# 핵심 데이터 누락 시 경고 (Series 직접 평가 수정)\nhas_open = 'open' in df.columns\nmissing_open_data = has_open and df['open'].isnull().all()\nif not has_open or missing_open_data:\nlogger.warning(f\"{symbol} 가격 데이터(OHLCV)가 모두 누락되었습니다. 메타데이터만 저장됨.\")\n\nlogger.info(f\"{symbol} 데이터 저장 완료: 총 {len(df)}행 (새 데이터: {len(new_data)}행)\")\n\n# 7. 데이터 검증\nif len(df) > 1: # 최소 2개 이상의 행이 있어야 diff 계산 가능\ntime_diffs = pd.Series(df['datetime'].diff().dt.total_seconds() / 60)\ngaps = time_diffs[time_diffs > 10] # 10분 이상의 간격이 있는 경우\n\nif not gaps.empty:\nlogger.warning(f\"{symbol} 데이터 누락 발견:\")\nfor idx in gaps.index:\ngap_start = df['datetime'].iloc[idx-1]\ngap_end = df['datetime'].iloc[idx]\ngap_minutes = gaps[idx]\nlogger.warning(f\" {gap_start} ~ {gap_end} ({gap_minutes:.0f}분 간격)\")\n\nreturn True\nelse:\nlogger.error(f\"{symbol} 필수 컬럼({min_required_cols})이 없어 저장할 수 없습니다.\")\nreturn False\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 저장 실패: {str(e)}\")\ntraceback.print_exc() # 상세 오류 출력\nreturn False\n\nclass DataManager:\n\"\"\"암호화폐 데이터 수집 및 관리 클래스\"\"\"\n\ndef __init__(self):\nself.api_key_manager = APIKeyManager()\nself.ccxt_manager = CcxtManager()\nself.symbols = []\nself.last_update = {}\nself.update_intervals = {\n'ohlcv': timedelta(minutes=1), # 1분봉으로 변경\n'funding': timedelta(minutes=5),\n'oi': timedelta(minutes=1) # OI도 1분 간격으로 변경\n}\n\n# 데이터 디렉토리 생성\nfor dir_path in [DATA_DIR, LIVE_DIR, BACKUP_DIR, ARCHIVE_DIR, LONG_TERM_DIR]:\ndir_path.mkdir(parents=True, exist_ok=True)\n\n# 심볼 목록 로드\nself.load_symbols()\n\ndef load_symbols(self):\n\"\"\"심볼 목록 로드\"\"\"\ntry:\nsymbols_file = DATA_DIR / 'symbols.json'\nif symbols_file.exists():\nwith open(symbols_file, 'r') as f:\nself.symbols = json.load(f)\nlogger.info(f\"심볼 목록 로드 완료: {len(self.symbols)}개\")\nelse:\nself.update_symbols()\nexcept Exception as e:\nlogger.error(f\"심볼 목록 로드 실패: {str(e)}\")\nself.symbols = []\n\ndef update_symbols(self):\n\"\"\"심볼 목록 업데이트\"\"\"\ntry:\nself.symbols = get_all_symbols()\nlogger.info(f\"심볼 목록 업데이트 완료: {len(self.symbols)}개\")\nexcept Exception as e:\nlogger.error(f\"심볼 목록 업데이트 실패: {str(e)}\")\n\ndef should_update(self, symbol: str, data_type: str) -> bool:\n\"\"\"업데이트가 필요한지 확인\"\"\"\nif symbol not in self.last_update:\nself.last_update[symbol] = {}\nreturn True\n\nif data_type not in self.last_update[symbol]:\nreturn True\n\nlast_update = self.last_update[symbol].get(data_type)\nif not last_update:\nreturn True\n\ninterval = self.update_intervals.get(data_type, timedelta(minutes=5))\nreturn datetime.now() - last_update > interval\n\ndef update_last_update(self, symbol: str, data_type: str):\n\"\"\"마지막 업데이트 시간 갱신\"\"\"\nif symbol not in self.last_update:\nself.last_update[symbol] = {}\nself.last_update[symbol][data_type] = datetime.now()\n\ndef check_data_gaps(self, df: pd.DataFrame, expected_interval: int = 1) -> List[Tuple[datetime, datetime]]:\n\"\"\"데이터 누락 구간 확인 (expected_interval: 분 단위)\"\"\"\nif df.empty or len(df) < 2:\nreturn []\n\ndf = df.sort_values('datetime')\ntime_diffs = df['datetime'].diff()\ngaps = []\n\nfor i in range(1, len(df)):\ndiff_minutes = time_diffs.iloc[i].total_seconds() / 60\nif diff_minutes > expected_interval * 1.5: # 1.5배 이상 차이나는 경우를 누락으로 간주\ngaps.append((df['datetime'].iloc[i-1], df['datetime'].iloc[i]))\n\nreturn gaps\n\ndef fill_missing_data(self, symbol: str, start_time: datetime = None):\n\"\"\"누락된 데이터 채우기\"\"\"\ntry:\n# 기존 데이터 로드\ndf = load_existing_data(symbol)\n\n# 시작 시간이 지정되지 않은 경우, 7일 전부터 시작\nif start_time is None:\nstart_time = datetime.now() - timedelta(days=7)\n\nif df.empty:\n# 데이터가 없는 경우, 전체 기간 수집\nlogger.info(f\"{symbol}: 전체 기간 데이터 수집 시작 ({start_time}부터)\")\nself.collect_historical_data(symbol, start_time)\nreturn\n\n# 누락 구간 확인\ngaps = self.check_data_gaps(df)\n\n# 오래된 데이터 시작 시점부터 현재까지 누락 구간 추가\noldest_time = df['datetime'].min()\nif oldest_time > start_time:\ngaps.insert(0, (start_time, oldest_time))\n\n# 최신 데이터부터 현재까지 누락 구간 추가\nlatest_time = df['datetime'].max()\ncurrent_time = datetime.now()\nif (current_time - latest_time).total_seconds() / 60 > 1:\ngaps.append((latest_time, current_time))\n\n# 누락 구간 데이터 수집\nfor gap_start, gap_end in gaps:\nlogger.info(f\"{symbol}: 누락 구간 데이터 수집 {gap_start} ~ {gap_end}\")\nself.collect_historical_data(symbol, gap_start, gap_end)\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 누락 데이터 채우기 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef collect_historical_data(self, symbol: str, start_time: datetime, end_time: datetime = None):\n\"\"\"과거 데이터 수집\"\"\"\ntry:\nall_data_frames = {}\n\n# OHLCV 데이터 수집\ntry:\nohlcv_data = self.ccxt_manager.fetch_ohlcv(symbol, timeframe='1m', since=start_time, limit=1000)\nif not ohlcv_data.empty:\nall_data_frames['ohlcv'] = ohlcv_data\nexcept Exception as e:\nlogger.error(f\"{symbol} OHLCV 과거 데이터 수집 실패: {str(e)}\")\n\n# OI 데이터 수집\ntry:\nparams = {\n\"symbol\": f\"{symbol}USDT_PERP.A\",\n\"from\": int(start_time.timestamp()),\n\"to\": int(end_time.timestamp()) if end_time else int(datetime.now().timestamp()),\n\"interval\": \"1min\"\n}\noi_data = make_request(ENDPOINTS[\"open_interest_history\"], params)\nif oi_data:\ndf_oi = pd.DataFrame(extract_data_from_response(oi_data, \"oi\"))\nif not df_oi.empty:\ndf_oi = normalize_columns(df_oi, \"oi\")\nall_data_frames['oi'] = df_oi\nexcept Exception as e:\nlogger.error(f\"{symbol} OI 과거 데이터 수집 실패: {str(e)}\")\n\n# 펀딩레이트 데이터 수집\ntry:\nfunding_data = self.ccxt_manager.fetch_funding_rate(symbol)\nif not funding_data.empty:\nall_data_frames['funding'] = funding_data\nexcept Exception as e:\nlogger.error(f\"{symbol} 펀딩레이트 과거 데이터 수집 실패: {str(e)}\")\n\n# 데이터 병합 및 저장\nif all_data_frames:\nmerged_df = merge_dataframes(all_data_frames, symbol)\nif not merged_df.empty:\nsave_data(symbol, merged_df)\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 과거 데이터 수집 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef collect_data(self, symbol: str):\n\"\"\"단일 심볼 데이터 수집\"\"\"\nif shutdown_flag.is_set():\nreturn\n\ntry:\nall_data_frames = {}\n\n# OHLCV 데이터 수집 (1분봉)\nif self.should_update(symbol, 'ohlcv'):\ntry:\nohlcv_data = self.ccxt_manager.fetch_ohlcv(symbol, timeframe='1m')\nif not ohlcv_data.empty:\nall_data_frames['ohlcv'] = ohlcv_data\nself.update_last_update(symbol, 'ohlcv')\nexcept Exception as e:\nlogger.error(f\"{symbol} OHLCV 데이터 수집 실패: {str(e)}\")\n\n# OI 데이터 수집 (1분봉)\nif self.should_update(symbol, 'oi'):\ntry:\nparams = {\n\"symbol\": f\"{symbol}USDT_PERP.A\",\n\"interval\": \"1min\"\n}\noi_data = make_request(ENDPOINTS[\"open_interest\"], params)\nif oi_data:\ndf_oi = pd.DataFrame(extract_data_from_response(oi_data, \"oi\"))\nif not df_oi.empty:\ndf_oi = normalize_columns(df_oi, \"oi\")\nall_data_frames['oi'] = df_oi\nself.update_last_update(symbol, 'oi')\nexcept Exception as e:\nlogger.error(f\"{symbol} OI 데이터 수집 실패: {str(e)}\")\n\n# 펀딩레이트 데이터 수집\nif self.should_update(symbol, 'funding'):\ntry:\nfunding_data = self.ccxt_manager.fetch_funding_rate(symbol)\nif not funding_data.empty:\nall_data_frames['funding'] = funding_data\nself.update_last_update(symbol, 'funding')\nexcept Exception as e:\nlogger.error(f\"{symbol} 펀딩레이트 데이터 수집 실패: {str(e)}\")\n\n# 데이터프레임 병합 및 저장\nif all_data_frames:\nmerged_df = merge_dataframes(all_data_frames, symbol)\nif not merged_df.empty:\nsave_data(symbol, merged_df)\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 수집 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef collect_all_data(self):\n\"\"\"모든 심볼 데이터 수집\"\"\"\nlogger.info(\"전체 데이터 수집 시작\")\n\ntry:\n# 심볼 목록이 비어있으면 업데이트\nif not self.symbols:\nself.update_symbols()\n\n# 각 심볼에 대해 데이터 수집\nfor symbol in self.symbols:\nif shutdown_flag.is_set():\nlogger.warning(\"종료 신호 감지: 데이터 수집 중단\")\nbreak\n\ntry:\n# 첫 실행 시 7일치 데이터 수집 및 누락 데이터 채우기\nif symbol not in self.last_update:\nstart_time = datetime.now() - timedelta(days=7)\nself.fill_missing_data(symbol, start_time)\n\n# 실시간 데이터 수집\nself.collect_data(symbol)\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 수집 실패: {str(e)}\")\ncontinue\n\nexcept Exception as e:\nlogger.error(f\"전체 데이터 수집 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef backup_data(self):\n\"\"\"데이터 백업\"\"\"\nlogger.info(\"데이터 백업 시작\")\n\ntry:\n# 백업 디렉토리 생성\nbackup_time = datetime.now().strftime('%Y%m%d_%H%M%S')\nbackup_dir = BACKUP_DIR / backup_time\nbackup_dir.mkdir(parents=True, exist_ok=True)\n\n# 라이브 데이터 백업\nfor file in LIVE_DIR.glob('*.csv'):\ntry:\nshutil.copy2(file, backup_dir / file.name)\nexcept Exception as e:\nlogger.error(f\"파일 백업 실패 ({file.name}): {str(e)}\")\n\nlogger.info(f\"데이터 백업 완료: {backup_dir}\")\n\n# 오래된 백업 정리 (30일 이상)\nself.cleanup_old_backups()\n\nexcept Exception as e:\nlogger.error(f\"데이터 백업 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef cleanup_old_backups(self, days: int = 30):\n\"\"\"오래된 백업 정리\"\"\"\ntry:\ncutoff_time = datetime.now() - timedelta(days=days)\n\nfor backup_dir in BACKUP_DIR.iterdir():\nif not backup_dir.is_dir():\ncontinue\n\ntry:\n# 디렉토리 이름에서 시간 추출\ndir_time = datetime.strptime(backup_dir.name, '%Y%m%d_%H%M%S')\n\nif dir_time < cutoff_time:\nshutil.rmtree(backup_dir)\nlogger.info(f\"오래된 백업 삭제: {backup_dir}\")\nexcept ValueError:\n# 잘못된 형식의 디렉토리 이름은 무시\ncontinue\nexcept Exception as e:\nlogger.error(f\"백업 정리 실패 ({backup_dir}): {str(e)}\")\n\nexcept Exception as e:\nlogger.error(f\"백업 정리 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef archive_data(self, symbol: str = None):\n\"\"\"데이터 아카이브\"\"\"\nlogger.info(\"데이터 아카이브 시작\")\n\ntry:\n# 아카이브 디렉토리 생성\narchive_time = datetime.now().strftime('%Y%m')\narchive_dir = ARCHIVE_DIR / archive_time\narchive_dir.mkdir(parents=True, exist_ok=True)\n\n# 특정 심볼 또는 모든 심볼에 대해 아카이브\nfiles_to_archive = []\nif symbol:\nfile_path = LIVE_DIR / f\"{symbol}.csv\"\nif file_path.exists():\nfiles_to_archive.append(file_path)\nelse:\nfiles_to_archive.extend(LIVE_DIR.glob('*.csv'))\n\nfor file in files_to_archive:\ntry:\n# 데이터 로드\ndf = pd.read_csv(file)\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\n# 최근 데이터만 라이브에 유지 (7일)\ncutoff_date = datetime.now() - timedelta(days=7)\nmask = df['datetime'] < cutoff_date\n\nif mask.any():\n# 오래된 데이터는 아카이브로 이동\nold_data = df[mask].copy()\nnew_data = df[~mask].copy()\n\n# 아카이브 파일에 추가\narchive_file = archive_dir / file.name\nif archive_file.exists():\nexisting_data = pd.read_csv(archive_file)\nexisting_data['datetime'] = pd.to_datetime(existing_data['datetime'])\nold_data = pd.concat([existing_data, old_data])\n\n# 중복 제거 및 정렬\nold_data = old_data.drop_duplicates(subset=['datetime']).sort_values('datetime')\nold_data.to_csv(archive_file, index=False)\n\n# 최근 데이터만 라이브 파일에 저장\nif len(new_data) > 0:\nnew_data.to_csv(file, index=False)\nelse:\nfile.unlink() # 모든 데이터가 아카이브되면 라이브 파일 삭제\n\nlogger.info(f\"데이터 아카이브 완료 ({file.name}): {len(old_data)}행 아카이브, {len(new_data)}행 유지\")\n\nexcept Exception as e:\nlogger.error(f\"파일 아카이브 실패 ({file.name}): {str(e)}\")\ncontinue\n\nexcept Exception as e:\nlogger.error(f\"데이터 아카이브 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef compress_archive(self, archive_dir: Path = None):\n\"\"\"아카이브 압축\"\"\"\ntry:\nif archive_dir is None:\n# 가장 오래된 아카이브 디렉토리 찾기\narchive_dirs = sorted(ARCHIVE_DIR.glob('*'))\nif not archive_dirs:\nreturn\narchive_dir = archive_dirs[0]\n\nif not archive_dir.is_dir():\nreturn\n\n# 압축 파일 생성\narchive_name = archive_dir.name\nzip_file = LONG_TERM_DIR / f\"{archive_name}.zip\"\n\nwith zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\nfor file in archive_dir.glob('*.csv'):\nzf.write(file, file.name)\n\n# 압축 성공 시 원본 디렉토리 삭제\nshutil.rmtree(archive_dir)\nlogger.info(f\"아카이브 압축 완료: {zip_file}\")\n\nexcept Exception as e:\nlogger.error(f\"아카이브 압축 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef run(self):\n\"\"\"데이터 수집 실행\"\"\"\nlogger.info(\"데이터 수집 시작\")\n\ntry:\nwhile not shutdown_flag.is_set():\ntry:\n# 데이터 수집\nself.collect_all_data()\n\n# 매일 자정에 백업 및 아카이브\nnow = datetime.now()\nif now.hour == 0 and now.minute < 5:\nself.backup_data()\nself.archive_data()\n\n# 3개월 이상 된 아카이브는 압축\nthree_months_ago = (now - timedelta(days=90)).strftime('%Y%m')\nold_archives = [d for d in ARCHIVE_DIR.glob('*') if d.name <= three_months_ago]\nfor archive_dir in old_archives:\nself.compress_archive(archive_dir)\n\n# 1분 대기\ntime.sleep(60)\n\nexcept Exception as e:\nlogger.error(f\"데이터 수집 중 오류 발생: {str(e)}\")\ntraceback.print_exc()\ntime.sleep(60) # 오류 발생 시 1분 대기\n\nexcept KeyboardInterrupt:\nlogger.info(\"사용자에 의한 종료\")\nexcept Exception as e:\nlogger.error(f\"예상치 못한 오류 발생: {str(e)}\")\ntraceback.print_exc()\nfinally:\nlogger.info(\"데이터 수집 종료\")\n\nif __name__ == \"__main__\":\ntry:\n# 종료 시그널 핸들러 등록\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\n\n# 데이터 매니저 실행\nmanager = DataManager()\nmanager.run()\n\nexcept Exception as e:\nlogger.error(f\"프로그램 실행 실패: {str(e)}\")\ntraceback.print_exc()\nfinally:\nlogger.info(\"프로그램 종료\")\n\n이 코드에서 변경하던거잖아 우리? 이코드에서 변경된부분만 새로 적용하자 ","commandType":4},{"text":"응 다적용해서 한번에 코드화 못하니까 나눠서 하고 주석처리 잘하고 ","commandType":4},{"text":"어차피 파일 자체에 문제가 있으니까 신규 파일로 새로만들어라 오히려 더 꼬이는 느낌이다.","commandType":4},{"text":"\"\"\"\n암호화폐 데이터 통합 관리 시스템 (CCXT 데이터 보완 및 장기 보관 기능 추가)\n- 과거 데이터 누락 검사 및 복구 (CCXT로 보완)\n- 실시간 데이터 수집 및 저장\n- 단일 CSV 파일에 시간순 정렬 저장\n- 자동 심볼 추가 및 관리\n- 안전한 종료 메커니즘\n- 데이터 장기 보관\n- 소수점 정확도 유지\n\"\"\"\n\nimport os\nimport logging\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nimport time\nimport base64\nimport signal\nimport traceback\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Union\nimport random\nimport threading\nimport queue\nimport ccxt\nimport shutil\nimport zipfile\n\n# 안전한 종료를 위한 전역 플래그\nshutdown_flag = threading.Event()\n\n# 기본 설정\nBASE_URL = \"https://api.coinalyze.net/v1\"\nDATA_DIR = Path(\"data\")\nLIVE_DIR = DATA_DIR / \"live\"\nBACKUP_DIR = DATA_DIR / \"backup\"\nARCHIVE_DIR = DATA_DIR / \"archive\"\nLONG_TERM_DIR = DATA_DIR / \"longterm\"\n\n# API 요청 간격 설정 (초)\nAPI_REQUEST_INTERVAL = 0.5 # API 요청 사이 대기 시간 (초)\nCOLLECTION_INTERVAL = 300 # 데이터 수집 간격 (초) - 5분\n\n# API 엔드포인트\nENDPOINTS = {\n\"exchanges\": \"/exchanges\",\n\"future_markets\": \"/future-markets\",\n\"spot_markets\": \"/spot-markets\",\n\"open_interest\": \"/open-interest\",\n\"funding_rate\": \"/funding-rate\",\n\"predicted_funding_rate\": \"/predicted-funding-rate\",\n\"open_interest_history\": \"/open-interest-history\",\n\"funding_rate_history\": \"/funding-rate-history\",\n\"predicted_funding_rate_history\": \"/predicted-funding-rate-history\",\n\"liquidation_history\": \"/liquidation-history\",\n\"long_short_ratio_history\": \"/long-short-ratio-history\",\n\"ohlcv_history\": \"/ohlcv-history\"\n}\n\n# API 키 설정 (16개 키를 4개 티어로 구분)\nAPI_KEYS = {\n\"tier1\": [\n\"0d0ba171-4185-4a58-bc02-8c8627cd1f54\",\n\"e629efa9-68b0-4b6b-b794-fa2d9e379b79\",\n\"c9779cfd-a85c-4e58-a378-9474174a075e\",\n\"58a39d7b-84e5-4852-8375-90de055cba18\"\n],\n\"tier2\": [\n\"7b80a59d-f0f9-4a83-81f7-1314dbdd9dc7\",\n\"2b951698-64e7-4a86-9930-c503d4e29e54\",\n\"10b24c79-ddd1-4046-94b6-0af0d23b241e\",\n\"fac040b5-5043-4351-b8f3-09c1d8cfd78f\"\n],\n\"tier3\": [\n\"fe1caf7e-5e27-4f0c-9d14-2a4611db625f\",\n\"fc9e5080-9607-46e8-b48c-deca57591990\",\n\"a82fd8a4-873f-4619-9b9f-d8a29373b5b8\",\n\"6431161c-8815-4d18-846d-55e00863682e\"\n],\n\"tier4\": [\n\"07cc6de0-0d02-41b8-acdc-dd1284bf5730\",\n\"f2842bed-d43e-4152-a0b5-68d9d9ed30b0\",\n\"013d0da7-76ea-4699-97d7-6e0f66857939\",\n\"7906d7bf-b493-42ad-a578-0cd13c6a070c\"\n]\n}\n\n# CCXT 설정 - 공개 데이터만 사용하므로 API 키 불필요\nCCXT_EXCHANGES = [\n\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"\n]\n\n# 로깅 설정\nlogging.basicConfig(\nlevel=logging.INFO,\nformat='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\nhandlers=[\nlogging.StreamHandler(),\nlogging.FileHandler(\"crypto_data_manager.log\", encoding='utf-8')\n]\n)\nlogger = logging.getLogger(__name__)\n\ndef signal_handler(sig, frame):\n\"\"\"시스템 종료 신호를 처리하는 핸들러\"\"\"\nlogger.warning(\"종료 신호를 받았습니다. 안전하게 종료합니다...\")\nshutdown_flag.set()\n\n# 시그널 핸들러 등록\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\n\ndef check_stop_file():\n\"\"\"종료 파일 확인\"\"\"\nif os.path.exists(\"stop.signal\"):\nlogger.warning(\"종료 파일이 감지되었습니다. 안전하게 종료합니다...\")\ntry:\nos.remove(\"stop.signal\") # 종료 파일 삭제\nexcept:\npass\nshutdown_flag.set()\nreturn True\nreturn False\n\nclass APIKeyManager:\n\"\"\"API 키 관리 클래스\"\"\"\ndef __init__(self):\nself.tier_keys = API_KEYS\nself.current_key_index = {tier: 0 for tier in API_KEYS.keys()}\nself.key_locks = {tier: threading.Lock() for tier in API_KEYS.keys()}\nself.key_rate_limits = {key: {'last_used': 0, 'count': 0} for tier in API_KEYS.values() for key in tier}\nself.rate_limit_reset_time = time.time() + 60 # 1분 후 리셋\n\ndef get_next_key(self, tier: str) -> str:\n\"\"\"라운드 로빈 방식으로 다음 API 키 반환\"\"\"\nif tier not in self.tier_keys or not self.tier_keys[tier]:\navailable_tiers = [t for t in self.tier_keys if self.tier_keys[t]]\nif not available_tiers:\nraise ValueError(\"사용 가능한 API 키가 없습니다\")\ntier = random.choice(available_tiers)\n\nwith self.key_locks[tier]:\nkeys = self.tier_keys[tier]\ncurrent_index = self.current_key_index[tier]\n\n# 현재 시간이 리셋 시간을 초과했는지 확인\ncurrent_time = time.time()\nif current_time > self.rate_limit_reset_time:\n# 사용량 카운터 리셋\nfor key_info in self.key_rate_limits.values():\nkey_info['count'] = 0\nself.rate_limit_reset_time = current_time + 60 # 다음 1분 후 리셋\n\n# 사용량이 가장 적은 키를 찾아 반환\nmin_count = float('inf')\nselected_key = None\n\nfor i in range(len(keys)):\nidx = (current_index + i) % len(keys)\nkey = keys[idx]\n\nif self.key_rate_limits[key]['count'] < min_count:\nmin_count = self.key_rate_limits[key]['count']\nselected_key = key\nself.current_key_index[tier] = (idx + 1) % len(keys)\n\n# 키 사용량 기록\nself.key_rate_limits[selected_key]['last_used'] = current_time\nself.key_rate_limits[selected_key]['count'] += 1\n\n# API 요청 간격 조절을 위한 대기\nlast_used = self.key_rate_limits[selected_key]['last_used']\nif current_time - last_used < API_REQUEST_INTERVAL:\ntime.sleep(API_REQUEST_INTERVAL - (current_time - last_used))\n\nreturn selected_key\n\nclass CcxtManager:\n\"\"\"CCXT 거래소 관리 및 심볼 매핑 클래스\"\"\"\ndef __init__(self):\nself.exchanges = {}\nself.symbol_mappings = {} # base_symbol -> {exchange_id -> ccxt_symbol}\nself.exchange_priority = [\"binance\", \"bybit\", \"okx\", \"kucoin\", \"bitget\"]\nself.init_exchanges()\nself.supported_symbols = set() # CCXT에서 지원하는 심볼 목록\nself.timeframe_map = {\n\"1min\": \"1m\", \"5min\": \"5m\", \"15min\": \"15m\", \"30min\": \"30m\",\n\"1h\": \"1h\", \"4h\": \"4h\", \"1d\": \"1d\", \"1w\": \"1w\"\n}\n\ndef init_exchanges(self):\n\"\"\"CCXT 거래소 초기화 (API 키 없이)\"\"\"\nfor exchange_id in CCXT_EXCHANGES:\ntry:\nif exchange_id in ccxt.exchanges:\nexchange_class = getattr(ccxt, exchange_id)\nself.exchanges[exchange_id] = exchange_class({\n'enableRateLimit': True,\n'options': {\n'defaultType': 'future' # 선물 거래 기본 설정\n}\n})\nlogger.info(f\"CCXT {exchange_id} 거래소 초기화 성공\")\nexcept Exception as e:\nlogger.error(f\"CCXT {exchange_id} 거래소 초기화 실패: {str(e)}\")\n\ndef load_all_markets(self):\n\"\"\"모든 거래소의 시장 정보 로드\"\"\"\nfor exchange_id, exchange in self.exchanges.items():\ntry:\nexchange.load_markets()\nlogger.info(f\"{exchange_id} 시장 정보 로드 성공: {len(exchange.symbols)}개 심볼\")\nexcept Exception as e:\nlogger.error(f\"{exchange_id} 시장 정보 로드 실패: {str(e)}\")\n\ndef discover_symbol_mappings(self, base_symbols):\n\"\"\"여러 기본 심볼에 대한 CCXT 매핑 자동 탐색\"\"\"\nlogger.info(f\"CCXT 심볼 매핑 탐색 시작: {len(base_symbols)}개 심볼\")\n\n# 거래소 시장 정보 로드\nself.load_all_markets()\n\n# 각 심볼별 매핑 탐색\nfor base_symbol in base_symbols:\nself.discover_symbol_mapping(base_symbol)\n\n# 매핑 정보 저장\nself.save_symbol_mappings()\n\nlogger.info(f\"CCXT 심볼 매핑 탐색 완료: {len(self.symbol_mappings)}개 심볼\")\n\ndef discover_symbol_mapping(self, base_symbol):\n\"\"\"단일 기본 심볼에 대한 CCXT 매핑 탐색\"\"\"\nif base_symbol in self.symbol_mappings:\nreturn\n\nself.symbol_mappings[base_symbol] = {}\n\n# 거래소별 검색 패턴\nsearch_patterns = {\n\"binance\": [\nf\"{base_symbol}/USDT:USDT\", # 영구 선물\nf\"{base_symbol}USDT_PERP\",\nf\"{base_symbol}/USDT\" # 현물\n],\n\"bybit\": [\nf\"{base_symbol}USDT\", # 영구 선물\nf\"{base_symbol}/USDT\" # 현물\n],\n\"okx\": [\nf\"{base_symbol}-USDT-SWAP\", # 영구 선물\nf\"{base_symbol}/USDT:USDT\",\nf\"{base_symbol}/USDT\" # 현물\n],\n\"kucoin\": [\nf\"{base_symbol}USDTM\", # 영구 선물\nf\"{base_symbol}-USDT\", # 영구 선물 (다른 형식)\nf\"{base_symbol}/USDT\" # 현물\n],\n\"bitget\": [\nf\"{base_symbol}USDT_UMCBL\", # 영구 선물\nf\"{base_symbol}/USDT\" # 현물\n]\n}\n\n# 거래소 우선순위대로 검색\nfor exchange_id in self.exchange_priority:\nif exchange_id not in self.exchanges:\ncontinue\n\nexchange = self.exchanges[exchange_id]\npatterns = search_patterns.get(exchange_id, [f\"{base_symbol}/USDT\"])\n\nfor pattern in patterns:\nfound = False\nfor symbol in exchange.symbols:\nnormalized_symbol = symbol.upper()\nnormalized_pattern = pattern.upper()\n\n# 정확한 일치 또는 포함 관계 확인\nif normalized_symbol == normalized_pattern or normalized_pattern in normalized_symbol:\nself.symbol_mappings[base_symbol][exchange_id] = symbol\nlogger.info(f\"심볼 매핑 발견: {base_symbol} -> {exchange_id}:{symbol}\")\nself.supported_symbols.add(base_symbol)\nfound = True\nbreak\n\nif found:\nbreak\n\ndef save_symbol_mappings(self):\n\"\"\"심볼 매핑 정보 파일로 저장\"\"\"\ntry:\nfilepath = DATA_DIR / \"symbol_mappings.json\"\nwith open(filepath, 'w') as f:\njson.dump({\n\"mappings\": self.symbol_mappings,\n\"supported_symbols\": list(self.supported_symbols),\n\"timestamp\": datetime.now().isoformat()\n}, f, indent=2)\nlogger.info(f\"심볼 매핑 정보 저장 완료: {filepath}\")\nexcept Exception as e:\nlogger.error(f\"심볼 매핑 정보 저장 실패: {str(e)}\")\n\ndef load_symbol_mappings(self):\n\"\"\"저장된 심볼 매핑 정보 로드\"\"\"\ntry:\nfilepath = DATA_DIR / \"symbol_mappings.json\"\nif filepath.exists():\nwith open(filepath, 'r') as f:\ndata = json.load(f)\nself.symbol_mappings = data.get(\"mappings\", {})\nself.supported_symbols = set(data.get(\"supported_symbols\", []))\nlogger.info(f\"심볼 매핑 정보 로드 완료: {len(self.symbol_mappings)}개 심볼\")\nreturn True\nreturn False\nexcept Exception as e:\nlogger.error(f\"심볼 매핑 정보 로드 실패: {str(e)}\")\nreturn False\n\ndef get_ccxt_symbol(self, base_symbol, exchange_id=None):\n\"\"\"CCXT 심볼 가져오기 (특정 거래소 또는 우선순위 기반)\"\"\"\n# 매핑 정보 확인\nif base_symbol not in self.symbol_mappings:\nself.discover_symbol_mapping(base_symbol)\n\n# 특정 거래소 지정한 경우\nif exchange_id:\nreturn self.symbol_mappings.get(base_symbol, {}).get(exchange_id)\n\n# 거래소 우선순위 기준 탐색\nfor ex_id in self.exchange_priority:\nif ex_id in self.symbol_mappings.get(base_symbol, {}):\nreturn self.symbol_mappings[base_symbol][ex_id], ex_id\n\nreturn None, None\n\ndef fetch_ohlcv(self, base_symbol, timeframe='5m', since=None, limit=1000):\n\"\"\"CCXT를 통해 OHLCV 데이터 가져오기\"\"\"\nif shutdown_flag.is_set():\nreturn pd.DataFrame()\n\n# 심볼 매핑 확인 및 가져오기\nccxt_symbol, exchange_id = self.get_ccxt_symbol(base_symbol)\n\nif not ccxt_symbol or not exchange_id:\nlogger.warning(f\"CCXT 심볼 매핑을 찾을 수 없음: {base_symbol}\")\nreturn pd.DataFrame()\n\n# 타임프레임 변환\nccxt_timeframe = self.timeframe_map.get(timeframe, timeframe)\n\n# since 변환 (timestamp to milliseconds)\nsince_ms = int(since.timestamp() * 1000) if since else None\n\n# 데이터 가져오기 시도\ntry:\nexchange = self.exchanges[exchange_id]\n\n# OHLCV 데이터 가져오기\nohlcv = exchange.fetch_ohlcv(ccxt_symbol, ccxt_timeframe, since_ms, limit)\n\nif not ohlcv:\nlogger.warning(f\"CCXT에서 데이터 없음: {exchange_id}:{ccxt_symbol}\")\nreturn pd.DataFrame()\n\n# DataFrame 변환\ndf = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\ndf['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\ndf['symbol'] = base_symbol\ndf['exchange'] = exchange_id\n\nlogger.info(f\"CCXT 데이터 수집 성공: {exchange_id}:{ccxt_symbol}, {len(df)}행\")\nreturn df\n\nexcept Exception as e:\nlogger.error(f\"CCXT 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\nreturn pd.DataFrame()\n\ndef fetch_funding_rate(self, base_symbol, timeframe='1h', since=None, limit=500):\n\"\"\"CCXT를 통해 펀딩비 데이터 가져오기 (지원하는 거래소만)\"\"\"\nif shutdown_flag.is_set():\nreturn pd.DataFrame()\n\n# 펀딩비를 지원하는 거래소들\nfunding_supported = [\"binance\", \"bybit\", \"okx\"]\nfunding_data = []\n\n# 지원하는 거래소에서 데이터 수집 시도\nfor exchange_id in funding_supported:\nif exchange_id not in self.exchanges:\ncontinue\n\nccxt_symbol = self.get_ccxt_symbol(base_symbol, exchange_id)\nif not ccxt_symbol:\ncontinue\n\ntry:\nexchange = self.exchanges[exchange_id]\n\n# 1. fetchFundingRateHistory 사용 (최신 데이터)\nif hasattr(exchange, 'fetchFundingRateHistory') and callable(getattr(exchange, 'fetchFundingRateHistory')):\nsince_ms = int(since.timestamp() * 1000) if since else None\nfunding_history = exchange.fetch_funding_rate_history(ccxt_symbol, since_ms, limit)\n\nif funding_history:\nfor entry in funding_history:\nfunding_data.append({\n'datetime': pd.to_datetime(entry['timestamp'], unit='ms'),\n'symbol': base_symbol,\n'exchange': exchange_id,\n'funding_rate': entry['fundingRate']\n})\n\n# 2. fetchFundingRate 사용 (현재 펀딩비)\nelif hasattr(exchange, 'fetchFundingRate') and callable(getattr(exchange, 'fetchFundingRate')):\nfunding_info = exchange.fetch_funding_rate(ccxt_symbol)\nif funding_info:\nfunding_data.append({\n'datetime': pd.to_datetime(funding_info['timestamp'], unit='ms'),\n'symbol': base_symbol,\n'exchange': exchange_id,\n'funding_rate': funding_info['fundingRate']\n})\n\nexcept Exception as e:\nlogger.error(f\"CCXT 펀딩비 데이터 수집 실패: {exchange_id}:{ccxt_symbol}, {str(e)}\")\n\n# 수집된 데이터를 DataFrame으로 변환\nif funding_data:\ndf = pd.DataFrame(funding_data)\nlogger.info(f\"CCXT 펀딩비 데이터 수집 성공: {base_symbol}, {len(df)}행\")\nreturn df\nelse:\nreturn pd.DataFrame()\n\n# API 키 매니저 인스턴스 생성\napi_key_manager = APIKeyManager()\n\n# CCXT 매니저 인스턴스 생성\nccxt_manager = CcxtManager()\n\ndef get_simple_symbol(symbol: str) -> str:\n\"\"\"심볼명을 단순화 (예: BTCUSDT_PERP.A -> BTC)\"\"\"\nif 'USDT_PERP.A' in symbol:\nreturn symbol.split('USDT_')[0]\nreturn symbol.replace('.csv', '')\n\ndef exponential_backoff(attempt, base_delay=2, max_delay=120):\n\"\"\"지수 백오프 알고리즘\"\"\"\ndelay = min(max_delay, base_delay * (2 ** attempt))\njitter = random.uniform(0, 0.1 * delay) # 10% 지터 추가\nreturn delay + jitter\n\ndef make_request(endpoint: str, params: dict = None, tier: str = \"tier1\", max_retries: int = 5) -> Optional[dict]:\n\"\"\"API 요청 수행 (지수 백오프 적용)\"\"\"\nurl = f\"{BASE_URL}{endpoint}\"\n\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nlogger.warning(\"종료 중에는 API 요청을 수행하지 않습니다\")\nreturn None\n\nfor attempt in range(max_retries):\ntry:\napi_key = api_key_manager.get_next_key(tier)\nheaders = {\n\"Accept\": \"application/json\",\n\"Api-Key\": api_key\n}\n\nresponse = requests.get(url, headers=headers, params=params, timeout=60)\n\nif response.status_code == 429: # Rate limit exceeded\nsleep_time = exponential_backoff(attempt)\nlogger.warning(f\"API 속도 제한 초과. {sleep_time:.2f}초 후 재시도... (시도 {attempt+1}/{max_retries})\")\ntime.sleep(sleep_time)\ncontinue\n\nif response.status_code == 401: # 인증 실패\nlogger.error(f\"API 키 인증 실패: {api_key}\")\ncontinue\n\nresponse.raise_for_status()\nreturn response.json()\n\nexcept requests.exceptions.Timeout:\nsleep_time = exponential_backoff(attempt)\nlogger.error(f\"API 요청 타임아웃 ({endpoint}) - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\ntime.sleep(sleep_time)\nexcept requests.exceptions.RequestException as e:\nsleep_time = exponential_backoff(attempt)\nlogger.error(f\"API 요청 실패 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\ntime.sleep(sleep_time)\nexcept Exception as e:\nsleep_time = exponential_backoff(attempt)\nlogger.error(f\"예상치 못한 오류 발생 ({endpoint}): {str(e)} - {sleep_time:.2f}초 후 재시도 {attempt+1}/{max_retries}\")\ntime.sleep(sleep_time)\n\nreturn None\n\ndef get_all_symbols() -> List[str]:\n\"\"\"사용 가능한 모든 심볼 목록 조회\"\"\"\ndata = make_request(ENDPOINTS[\"future_markets\"])\n\nif not data:\nlogger.error(\"심볼 목록을 가져올 수 없습니다\")\nreturn []\n\nsymbols = []\nif isinstance(data, dict):\nmarkets = data.get('markets', [])\nelse:\nmarkets = data\n\nfor market in markets:\nsymbol = market.get('symbol', '')\nif symbol.endswith('USDT_PERP.A'):\nbase_symbol = get_simple_symbol(symbol)\nsymbols.append(base_symbol)\n\nlogger.info(f\"총 {len(symbols)}개의 심볼을 찾았습니다\")\n\n# 심볼 목록 파일로 저장\nwith open(DATA_DIR / 'symbols.json', 'w') as f:\njson.dump({\n'symbols': symbols,\n'count': len(symbols),\n'timestamp': datetime.now().isoformat()\n}, f, indent=2)\n\nreturn sorted(symbols)\n\ndef extract_data_from_response(data, data_type: str):\n\"\"\"API 응답에서 데이터 추출 개선\"\"\"\nhistory = []\n\ntry:\n# 응답 데이터 구조 확인\nif isinstance(data, list):\n# 리스트 응답 처리\nif len(data) > 0:\nif isinstance(data[0], dict):\nif 'history' in data[0]:\n# 기존 구조: [{\"history\": [...]}]\nhistory = data[0]['history']\nelse:\n# 새 구조: 리스트가 직접 데이터인 경우\nhistory = data\nelse:\n# 직접 데이터 리스트인 경우\nhistory = data\nelif isinstance(data, dict):\n# 딕셔너리 응답 처리\nif 'history' in data:\nhistory = data['history']\nelif 'data' in data:\nhistory = data['data']\nelif 'result' in data:\nhistory = data['result']\nelif 'items' in data:\nhistory = data['items']\nelse:\n# 다른 키가 없으면 응답 전체를 사용\npotential_data = []\nfor key, value in data.items():\nif isinstance(value, list) and len(value) > 0:\npotential_data.extend(value)\nif potential_data:\nhistory = potential_data\nexcept Exception as e:\nlogger.error(f\"{data_type} 데이터 파싱 중 오류: {str(e)}\")\n\nreturn history\n\ndef normalize_columns(df, data_type: str) -> pd.DataFrame:\n\"\"\"데이터 프레임 컬럼 정규화 (소수점 보존)\"\"\"\n# 타임스탬프 변환\nif 't' in df.columns:\ndf['datetime'] = pd.to_datetime(df['t'], unit='s')\nelif 'timestamp' in df.columns:\ndf['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\nelif 'time' in df.columns:\ndf['datetime'] = pd.to_datetime(df['time'], unit='s')\n\n# 데이터 타입별 컬럼 처리\nif data_type == 'ohlcv':\ncolumn_mappings = {\n'o': 'open', 'open': 'open', 'price_open': 'open',\n'h': 'high', 'high': 'high', 'price_high': 'high',\n'l': 'low', 'low': 'low', 'price_low': 'low',\n'c': 'close', 'close': 'close', 'price_close': 'close',\n'v': 'volume', 'volume': 'volume', 'vol': 'volume'\n}\nelif data_type == 'oi':\ncolumn_mappings = {\n'c': 'open_interest', 'value': 'open_interest',\n'openInterest': 'open_interest', 'oi': 'open_interest'\n}\nelif data_type == 'funding':\ncolumn_mappings = {\n'c': 'funding_rate', 'rate': 'funding_rate',\n'fundingRate': 'funding_rate', 'funding': 'funding_rate'\n}\nelif data_type == 'predicted_funding':\ncolumn_mappings = {\n'c': 'predicted_funding_rate', 'rate': 'predicted_funding_rate',\n'predictedRate': 'predicted_funding_rate', 'predicted': 'predicted_funding_rate'\n}\nelif data_type == 'liquidation':\ncolumn_mappings = {\n'c': 'liquidation', 'value': 'liquidation',\n'liq': 'liquidation', 'liquidations': 'liquidation'\n}\nelif data_type == 'long_short_ratio':\ncolumn_mappings = {\n'c': 'long_short_ratio', 'ratio': 'long_short_ratio',\n'l': 'long_percentage', 'long': 'long_percentage', 'longPercentage': 'long_percentage',\n's': 'short_percentage', 'short': 'short_percentage', 'shortPercentage': 'short_percentage'\n}\nelse:\ncolumn_mappings = {}\n\n# 컬럼 매핑 적용\nfor old_col, new_col in column_mappings.items():\nif old_col in df.columns and old_col != new_col:\ndf[new_col] = df[old_col]\n\nreturn df\n\ndef validate_data(df, data_type: str) -> pd.DataFrame:\n\"\"\"데이터 검증 (소수점 보존)\"\"\"\nif df.empty:\nreturn df\n\n# 타임스탬프가 없는 행 제거\nif 'datetime' in df.columns:\ndf = df[df['datetime'].notna()]\n\n# 데이터 타입별 검증\nif data_type == 'ohlcv':\n# 가격 데이터 검증 (음수 값 제거, 소수점은 유지)\nfor col in ['open', 'high', 'low', 'close']:\nif col in df.columns:\n# 0 이하 값 확인 후 경고만 발생 (제거하지 않음)\nzero_or_neg = df[df[col] <= 0]\nif not zero_or_neg.empty:\nlogger.warning(f\"{data_type}: {len(zero_or_neg)}개 행에서 {col} 값이 0 이하입니다. 데이터 유지됨.\")\n\n# 볼륨 검증 (음수 제거)\nif 'volume' in df.columns:\nneg_volume = df[df['volume'] < 0]\nif not neg_volume.empty:\nlogger.warning(f\"{data_type}: {len(neg_volume)}개 행에서 volume 값이 음수입니다. 해당 값을 0으로 설정합니다.\")\ndf.loc[df['volume'] < 0, 'volume'] = 0\n\nelif data_type == 'funding' or data_type == 'predicted_funding':\n# 극단적인 펀딩비 값 확인 (소수점은 유지)\nfunding_col = 'funding_rate' if data_type == 'funding' else 'predicted_funding_rate'\nif funding_col in df.columns:\nextreme_funding = df[(df[funding_col] < -0.15) | (df[funding_col] > 0.15)]\nif not extreme_funding.empty:\nlogger.warning(f\"{data_type}: {len(extreme_funding)}개 행에서 {funding_col} 값이 극단적입니다 (-0.15 미만 또는 0.15 초과). 데이터 유지됨.\")\n\nelif data_type == 'long_short_ratio':\n# 롱/숏 퍼센티지 합이 100%인지 확인 (오차 범위 ±2%)\nif 'long_percentage' in df.columns and 'short_percentage' in df.columns:\ntotal = df['long_percentage'] + df['short_percentage']\ninvalid_rows = df[(total < 98) | (total > 102)]\nif not invalid_rows.empty:\nlogger.warning(f\"{data_type}: {len(invalid_rows)}개 행에서 long_percentage와 short_percentage 합이 100%와 크게 다릅니다. 데이터 유지됨.\")\n\nreturn df\n\ndef process_ohlcv_data(data) -> pd.DataFrame:\n\"\"\"OHLCV 데이터 처리 (대체 소스용)\"\"\"\nhistory = extract_data_from_response(data, \"ohlcv\")\n\nif not history:\nreturn pd.DataFrame()\n\ntry:\ndf = pd.DataFrame(history)\ndf = normalize_columns(df, \"ohlcv\")\n\nif 'datetime' not in df.columns and 't' in df.columns:\ndf['datetime'] = pd.to_datetime(df['t'], unit='s')\n\n# 필수 OHLCV 컬럼 확인\nrequired_cols = ['open', 'high', 'low', 'close']\nmissing_cols = [col for col in required_cols if col not in df.columns]\n\nif missing_cols:\nlogger.warning(f\"OHLCV 대체 소스에서 필수 컬럼 누락: {missing_cols}\")\nreturn pd.DataFrame()\n\nreturn df\nexcept Exception as e:\nlogger.error(f\"OHLCV 대체 소스 데이터 처리 오류: {str(e)}\")\nreturn pd.DataFrame()\n\ndef merge_dataframes(all_data_frames: Dict[str, pd.DataFrame], symbol: str) -> pd.DataFrame:\n\"\"\"여러 데이터프레임 병합 (소수점 보존)\"\"\"\nif not all_data_frames:\nreturn pd.DataFrame()\n\n# 가능한 모든 기준 프레임 후보 (우선순위 순)\nbase_frame_candidates = [\"ohlcv\", \"long_short_ratio\", \"funding\", \"oi\"]\nbase_df = None\n\n# 우선순위에 따라 기준 프레임 선택\nfor candidate in base_frame_candidates:\nif candidate in all_data_frames and not all_data_frames[candidate].empty:\nbase_df = all_data_frames[candidate].copy()\nlogger.info(f\"{symbol} 기준 프레임으로 {candidate} 선택 ({len(base_df)}행)\")\nbreak\n\n# 기준 프레임이 없으면 빈 DataFrame 반환\nif base_df is None:\nlogger.warning(f\"{symbol} 병합할 기준 프레임이 없습니다.\")\nreturn pd.DataFrame()\n\n# 심볼 정보 확인 및 추가\nif 'symbol' not in base_df.columns:\nbase_df['symbol'] = symbol\n\n# 기본 프레임 컬럼 이름 확인 (중복 방지 위해)\n# 문제가 되는 t, o, h, l, c 등 짧은 컬럼명을 미리 표준화\nif 't' in base_df.columns:\nbase_df.rename(columns={'t': 'timestamp'}, inplace=True)\nfor col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\nif col_old in base_df.columns:\nbase_df.rename(columns={col_old: col_new}, inplace=True)\n\n# 인덱스 리셋 (병합을 위한 준비)\nbase_df = base_df.reset_index(drop=True)\n\n# 표준 컬럼명 리스트\nstd_cols = {\n'open', 'high', 'low', 'close', 'volume',\n'open_interest', 'funding_rate', 'predicted_funding_rate',\n'liquidation', 'long_percentage', 'short_percentage', 'long_short_ratio',\n'datetime', 'symbol'\n}\n\n# 각 데이터프레임을 병합 (왼쪽 조인 방식으로 변경)\nfor data_type, df in all_data_frames.items():\n# 기준 프레임 또는 빈 프레임 건너뛰기\nis_base_frame = data_type == list(base_frame_candidates)[0]\nif is_base_frame or df.empty:\ncontinue\n\n# 표준화된 컬럼명으로 변환\nif 't' in df.columns:\ndf.rename(columns={'t': 'timestamp'}, inplace=True)\nfor col_old, col_new in {'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume'}.items():\nif col_old in df.columns:\ndf.rename(columns={col_old: col_new}, inplace=True)\n\n# 기준 프레임에 없는 컬럼만 추출\ncolumns_to_merge = []\nfor col in df.columns:\n# 표준 컬럼 외의 컬럼은 고유한 이름으로 변경\nif col not in std_cols and col in base_df.columns:\nnew_col = f\"{col}_{data_type}\"\ndf.rename(columns={col: new_col}, inplace=True)\ncolumns_to_merge.append(new_col)\nelif col not in base_df.columns and col != 'datetime' and col != 'symbol':\ncolumns_to_merge.append(col)\n\n# 병합할 컬럼이 있고 datetime 열이 있는 경우에만 병합 진행\nif len(columns_to_merge) > 0 and 'datetime' in df.columns:\n# 인덱스 리셋 (병합을 위한 준비)\ndf_to_merge = df.reset_index(drop=True)\nmerge_df = df_to_merge[['datetime'] + columns_to_merge].copy()\n\n# 병합 (왼쪽 조인 방식으로 변경)\ntry:\nbase_df = pd.merge(base_df, merge_df, on='datetime', how='left', suffixes=('', f'_{data_type}'))\nexcept Exception as e:\nlogger.error(f\"데이터프레임 병합 오류 ({data_type}): {str(e)}\")\ntraceback.print_exc() # 상세 오류 출력\n# 병합 실패해도 계속 진행\n\n# 최종 정리\ntry:\nbase_df = base_df.sort_values('datetime').reset_index(drop=True)\nexcept Exception as e:\nlogger.error(f\"데이터프레임 정렬 오류: {str(e)}\")\n\nreturn base_df\n\ndef load_existing_data(symbol: str) -> pd.DataFrame:\n\"\"\"기존 데이터 로드 (소수점 보존)\"\"\"\ncsv_path = LIVE_DIR / f\"{symbol}.csv\"\n\nif not csv_path.exists():\nreturn pd.DataFrame()\n\ntry:\n# 소수점 반올림 없이 데이터 로드\ndf = pd.read_csv(csv_path)\ndf['datetime'] = pd.to_datetime(df['datetime'])\nlogger.info(f\"{symbol} 기존 데이터 로드: {len(df)}행\")\nreturn df\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 로드 실패: {str(e)}\")\ntraceback.print_exc() # 상세 오류 출력\n\n# 손상된 파일 처리\ntry:\n# 손상된 파일 백업\ncorrupted_dir = DATA_DIR / \"corrupted\"\ncorrupted_dir.mkdir(parents=True, exist_ok=True)\n\nbackup_file = corrupted_dir / f\"{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\nos.rename(csv_path, backup_file)\nlogger.warning(f\"손상된 파일 이동: {csv_path} -> {backup_file}\")\nexcept Exception as backup_err:\nlogger.error(f\"손상된 파일 백업 실패: {str(backup_err)}\")\n\nreturn pd.DataFrame()\n\ndef save_data(symbol: str, new_data: pd.DataFrame) -> bool:\n\"\"\"데이터 저장 (기존 데이터와 병합, 소수점 보존)\"\"\"\nif new_data.empty:\nlogger.warning(f\"{symbol}: 저장할 새 데이터가 없습니다\")\nreturn False\n\n# 종료 플래그 확인\nif shutdown_flag.is_set():\nlogger.warning(f\"{symbol} 데이터 저장이 중단되었습니다 (종료 중)\")\nreturn False\n\nLIVE_DIR.mkdir(parents=True, exist_ok=True)\ncsv_path = LIVE_DIR / f\"{symbol}.csv\"\n\ntry:\n# 1. 기존 데이터 로드\nexisting_data = load_existing_data(symbol)\n\n# 2. 새 데이터와 기존 데이터 병합\nif not existing_data.empty:\n# 기존 데이터의 타임스탬프를 set으로 변환하여 비교 (성능 개선)\nexisting_timestamps = set(existing_data['datetime'].dt.to_pydatetime())\n# set을 사용한 필터링으로 변경\nmask = [dt not in existing_timestamps for dt in new_data['datetime'].dt.to_pydatetime()]\nnew_data_filtered = new_data.loc[mask] if any(mask) else pd.DataFrame()\n\nif not new_data_filtered.empty:\n# 인덱스 리셋 후 병합하여 인덱스 충돌 방지\ndf = pd.concat([existing_data.reset_index(drop=True),\nnew_data_filtered.reset_index(drop=True)],\nignore_index=True)\nelse:\nlogger.info(f\"{symbol}: 새 데이터가 이미 기존 데이터에 포함되어 있습니다\")\nreturn True\nelse:\ndf = new_data.copy()\n\n# 3. 중복 컬럼 제거\ncolumn_mappings = {\n'price_open': 'open',\n'price_high': 'high',\n'price_low': 'low',\n'price_close': 'close'\n}\n\nfor old_col, new_col in column_mappings.items():\nif old_col in df.columns and new_col in df.columns:\ndf = df.drop(old_col, axis=1)\nelif old_col in df.columns:\ndf = df.rename(columns={old_col: new_col})\n\n# 4. 불필요한 컬럼 제거\ncolumns_to_keep = [\n'datetime', 'symbol',\n'open', 'high', 'low', 'close', 'volume',\n'open_interest',\n'funding_rate', 'predicted_funding_rate',\n'liquidation',\n'long_percentage', 'short_percentage', 'long_short_ratio'\n]\n\navailable_columns = [col for col in columns_to_keep if col in df.columns]\n\n# 핵심 컬럼이 없어도 최소한의 메타데이터만이라도 저장\nmin_required_cols = ['datetime', 'symbol']\nif all(col in available_columns for col in min_required_cols):\ndf = df[available_columns]\n\n# 5. 중복 제거 및 정렬\ndf = df.drop_duplicates(subset=['datetime']).reset_index(drop=True)\ndf = df.sort_values('datetime')\n\n# 6. 저장 (소수점 보존)\n# 파일 손상 방지를 위한, 임시 파일에 저장 후 이동\ntemp_path = csv_path.with_name(f\"{symbol}_temp.csv\")\n\n# 소수점 반올림 없이 저장 (float_format 인자 사용하지 않음)\ndf.to_csv(temp_path, index=False)\n\n# 임시 파일이 성공적으로 작성되었으면 원본 파일로 이동\nif temp_path.exists():\nif csv_path.exists():\ncsv_path.unlink() # 기존 파일 삭제\ntemp_path.rename(csv_path) # 임시 파일을 원본 파일로 이동\n\n# 핵심 데이터 누락 시 경고 (Series 직접 평가 수정)\nhas_open = 'open' in df.columns\nmissing_open_data = has_open and df['open'].isnull().all()\nif not has_open or missing_open_data:\nlogger.warning(f\"{symbol} 가격 데이터(OHLCV)가 모두 누락되었습니다. 메타데이터만 저장됨.\")\n\nlogger.info(f\"{symbol} 데이터 저장 완료: 총 {len(df)}행 (새 데이터: {len(new_data)}행)\")\n\n# 7. 데이터 검증\nif len(df) > 1: # 최소 2개 이상의 행이 있어야 diff 계산 가능\ntime_diffs = pd.Series(df['datetime'].diff().dt.total_seconds() / 60)\ngaps = time_diffs[time_diffs > 10] # 10분 이상의 간격이 있는 경우\n\nif not gaps.empty:\nlogger.warning(f\"{symbol} 데이터 누락 발견:\")\nfor idx in gaps.index:\ngap_start = df['datetime'].iloc[idx-1]\ngap_end = df['datetime'].iloc[idx]\ngap_minutes = gaps[idx]\nlogger.warning(f\" {gap_start} ~ {gap_end} ({gap_minutes:.0f}분 간격)\")\n\nreturn True\nelse:\nlogger.error(f\"{symbol} 필수 컬럼({min_required_cols})이 없어 저장할 수 없습니다.\")\nreturn False\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 저장 실패: {str(e)}\")\ntraceback.print_exc() # 상세 오류 출력\nreturn False\n\nclass DataManager:\n\"\"\"암호화폐 데이터 수집 및 관리 클래스\"\"\"\n\ndef __init__(self):\nself.api_key_manager = APIKeyManager()\nself.ccxt_manager = CcxtManager()\nself.symbols = []\nself.last_update = {}\nself.update_intervals = {\n'ohlcv': timedelta(minutes=1), # 1분봉으로 변경\n'funding': timedelta(minutes=5),\n'oi': timedelta(minutes=1) # OI도 1분 간격으로 변경\n}\n\n# 데이터 디렉토리 생성\nfor dir_path in [DATA_DIR, LIVE_DIR, BACKUP_DIR, ARCHIVE_DIR, LONG_TERM_DIR]:\ndir_path.mkdir(parents=True, exist_ok=True)\n\n# 심볼 목록 로드\nself.load_symbols()\n\ndef load_symbols(self):\n\"\"\"심볼 목록 로드\"\"\"\ntry:\nsymbols_file = DATA_DIR / 'symbols.json'\nif symbols_file.exists():\nwith open(symbols_file, 'r') as f:\nself.symbols = json.load(f)\nlogger.info(f\"심볼 목록 로드 완료: {len(self.symbols)}개\")\nelse:\nself.update_symbols()\nexcept Exception as e:\nlogger.error(f\"심볼 목록 로드 실패: {str(e)}\")\nself.symbols = []\n\ndef update_symbols(self):\n\"\"\"심볼 목록 업데이트\"\"\"\ntry:\nself.symbols = get_all_symbols()\nlogger.info(f\"심볼 목록 업데이트 완료: {len(self.symbols)}개\")\nexcept Exception as e:\nlogger.error(f\"심볼 목록 업데이트 실패: {str(e)}\")\n\ndef should_update(self, symbol: str, data_type: str) -> bool:\n\"\"\"업데이트가 필요한지 확인\"\"\"\nif symbol not in self.last_update:\nself.last_update[symbol] = {}\nreturn True\n\nif data_type not in self.last_update[symbol]:\nreturn True\n\nlast_update = self.last_update[symbol].get(data_type)\nif not last_update:\nreturn True\n\ninterval = self.update_intervals.get(data_type, timedelta(minutes=5))\nreturn datetime.now() - last_update > interval\n\ndef update_last_update(self, symbol: str, data_type: str):\n\"\"\"마지막 업데이트 시간 갱신\"\"\"\nif symbol not in self.last_update:\nself.last_update[symbol] = {}\nself.last_update[symbol][data_type] = datetime.now()\n\ndef check_data_gaps(self, df: pd.DataFrame, expected_interval: int = 1) -> List[Tuple[datetime, datetime]]:\n\"\"\"데이터 누락 구간 확인 (expected_interval: 분 단위)\"\"\"\nif df.empty or len(df) < 2:\nreturn []\n\ndf = df.sort_values('datetime')\ntime_diffs = df['datetime'].diff()\ngaps = []\n\nfor i in range(1, len(df)):\ndiff_minutes = time_diffs.iloc[i].total_seconds() / 60\nif diff_minutes > expected_interval * 1.5: # 1.5배 이상 차이나는 경우를 누락으로 간주\ngaps.append((df['datetime'].iloc[i-1], df['datetime'].iloc[i]))\n\nreturn gaps\n\ndef fill_missing_data(self, symbol: str, start_time: datetime = None):\n\"\"\"누락된 데이터 채우기\"\"\"\ntry:\n# 기존 데이터 로드\ndf = load_existing_data(symbol)\n\n# 시작 시간이 지정되지 않은 경우, 7일 전부터 시작\nif start_time is None:\nstart_time = datetime.now() - timedelta(days=7)\n\nif df.empty:\n# 데이터가 없는 경우, 전체 기간 수집\nlogger.info(f\"{symbol}: 전체 기간 데이터 수집 시작 ({start_time}부터)\")\nself.collect_historical_data(symbol, start_time)\nreturn\n\n# 누락 구간 확인\ngaps = self.check_data_gaps(df)\n\n# 오래된 데이터 시작 시점부터 현재까지 누락 구간 추가\noldest_time = df['datetime'].min()\nif oldest_time > start_time:\ngaps.insert(0, (start_time, oldest_time))\n\n# 최신 데이터부터 현재까지 누락 구간 추가\nlatest_time = df['datetime'].max()\ncurrent_time = datetime.now()\nif (current_time - latest_time).total_seconds() / 60 > 1:\ngaps.append((latest_time, current_time))\n\n# 누락 구간 데이터 수집\nfor gap_start, gap_end in gaps:\nlogger.info(f\"{symbol}: 누락 구간 데이터 수집 {gap_start} ~ {gap_end}\")\nself.collect_historical_data(symbol, gap_start, gap_end)\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 누락 데이터 채우기 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef collect_historical_data(self, symbol: str, start_time: datetime, end_time: datetime = None):\n\"\"\"과거 데이터 수집\"\"\"\ntry:\nall_data_frames = {}\n\n# OHLCV 데이터 수집\ntry:\nohlcv_data = self.ccxt_manager.fetch_ohlcv(symbol, timeframe='1m', since=start_time, limit=1000)\nif not ohlcv_data.empty:\nall_data_frames['ohlcv'] = ohlcv_data\nexcept Exception as e:\nlogger.error(f\"{symbol} OHLCV 과거 데이터 수집 실패: {str(e)}\")\n\n# OI 데이터 수집\ntry:\nparams = {\n\"symbol\": f\"{symbol}USDT_PERP.A\",\n\"from\": int(start_time.timestamp()),\n\"to\": int(end_time.timestamp()) if end_time else int(datetime.now().timestamp()),\n\"interval\": \"1min\"\n}\noi_data = make_request(ENDPOINTS[\"open_interest_history\"], params)\nif oi_data:\ndf_oi = pd.DataFrame(extract_data_from_response(oi_data, \"oi\"))\nif not df_oi.empty:\ndf_oi = normalize_columns(df_oi, \"oi\")\nall_data_frames['oi'] = df_oi\nexcept Exception as e:\nlogger.error(f\"{symbol} OI 과거 데이터 수집 실패: {str(e)}\")\n\n# 펀딩레이트 데이터 수집\ntry:\nfunding_data = self.ccxt_manager.fetch_funding_rate(symbol)\nif not funding_data.empty:\nall_data_frames['funding'] = funding_data\nexcept Exception as e:\nlogger.error(f\"{symbol} 펀딩레이트 과거 데이터 수집 실패: {str(e)}\")\n\n# 데이터 병합 및 저장\nif all_data_frames:\nmerged_df = merge_dataframes(all_data_frames, symbol)\nif not merged_df.empty:\nsave_data(symbol, merged_df)\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 과거 데이터 수집 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef collect_data(self, symbol: str):\n\"\"\"단일 심볼 데이터 수집\"\"\"\nif shutdown_flag.is_set():\nreturn\n\ntry:\nall_data_frames = {}\n\n# OHLCV 데이터 수집 (1분봉)\nif self.should_update(symbol, 'ohlcv'):\ntry:\nohlcv_data = self.ccxt_manager.fetch_ohlcv(symbol, timeframe='1m')\nif not ohlcv_data.empty:\nall_data_frames['ohlcv'] = ohlcv_data\nself.update_last_update(symbol, 'ohlcv')\nexcept Exception as e:\nlogger.error(f\"{symbol} OHLCV 데이터 수집 실패: {str(e)}\")\n\n# OI 데이터 수집 (1분봉)\nif self.should_update(symbol, 'oi'):\ntry:\nparams = {\n\"symbol\": f\"{symbol}USDT_PERP.A\",\n\"interval\": \"1min\"\n}\noi_data = make_request(ENDPOINTS[\"open_interest\"], params)\nif oi_data:\ndf_oi = pd.DataFrame(extract_data_from_response(oi_data, \"oi\"))\nif not df_oi.empty:\ndf_oi = normalize_columns(df_oi, \"oi\")\nall_data_frames['oi'] = df_oi\nself.update_last_update(symbol, 'oi')\nexcept Exception as e:\nlogger.error(f\"{symbol} OI 데이터 수집 실패: {str(e)}\")\n\n# 펀딩레이트 데이터 수집\nif self.should_update(symbol, 'funding'):\ntry:\nfunding_data = self.ccxt_manager.fetch_funding_rate(symbol)\nif not funding_data.empty:\nall_data_frames['funding'] = funding_data\nself.update_last_update(symbol, 'funding')\nexcept Exception as e:\nlogger.error(f\"{symbol} 펀딩레이트 데이터 수집 실패: {str(e)}\")\n\n# 데이터프레임 병합 및 저장\nif all_data_frames:\nmerged_df = merge_dataframes(all_data_frames, symbol)\nif not merged_df.empty:\nsave_data(symbol, merged_df)\n\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 수집 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef collect_all_data(self):\n\"\"\"모든 심볼 데이터 수집\"\"\"\nlogger.info(\"전체 데이터 수집 시작\")\n\ntry:\n# 심볼 목록이 비어있으면 업데이트\nif not self.symbols:\nself.update_symbols()\n\n# 각 심볼에 대해 데이터 수집\nfor symbol in self.symbols:\nif shutdown_flag.is_set():\nlogger.warning(\"종료 신호 감지: 데이터 수집 중단\")\nbreak\n\ntry:\n# 첫 실행 시 7일치 데이터 수집 및 누락 데이터 채우기\nif symbol not in self.last_update:\nstart_time = datetime.now() - timedelta(days=7)\nself.fill_missing_data(symbol, start_time)\n\n# 실시간 데이터 수집\nself.collect_data(symbol)\nexcept Exception as e:\nlogger.error(f\"{symbol} 데이터 수집 실패: {str(e)}\")\ncontinue\n\nexcept Exception as e:\nlogger.error(f\"전체 데이터 수집 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef backup_data(self):\n\"\"\"데이터 백업\"\"\"\nlogger.info(\"데이터 백업 시작\")\n\ntry:\n# 백업 디렉토리 생성\nbackup_time = datetime.now().strftime('%Y%m%d_%H%M%S')\nbackup_dir = BACKUP_DIR / backup_time\nbackup_dir.mkdir(parents=True, exist_ok=True)\n\n# 라이브 데이터 백업\nfor file in LIVE_DIR.glob('*.csv'):\ntry:\nshutil.copy2(file, backup_dir / file.name)\nexcept Exception as e:\nlogger.error(f\"파일 백업 실패 ({file.name}): {str(e)}\")\n\nlogger.info(f\"데이터 백업 완료: {backup_dir}\")\n\n# 오래된 백업 정리 (30일 이상)\nself.cleanup_old_backups()\n\nexcept Exception as e:\nlogger.error(f\"데이터 백업 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef cleanup_old_backups(self, days: int = 30):\n\"\"\"오래된 백업 정리\"\"\"\ntry:\ncutoff_time = datetime.now() - timedelta(days=days)\n\nfor backup_dir in BACKUP_DIR.iterdir():\nif not backup_dir.is_dir():\ncontinue\n\ntry:\n# 디렉토리 이름에서 시간 추출\ndir_time = datetime.strptime(backup_dir.name, '%Y%m%d_%H%M%S')\n\nif dir_time < cutoff_time:\nshutil.rmtree(backup_dir)\nlogger.info(f\"오래된 백업 삭제: {backup_dir}\")\nexcept ValueError:\n# 잘못된 형식의 디렉토리 이름은 무시\ncontinue\nexcept Exception as e:\nlogger.error(f\"백업 정리 실패 ({backup_dir}): {str(e)}\")\n\nexcept Exception as e:\nlogger.error(f\"백업 정리 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef archive_data(self, symbol: str = None):\n\"\"\"데이터 아카이브\"\"\"\nlogger.info(\"데이터 아카이브 시작\")\n\ntry:\n# 아카이브 디렉토리 생성\narchive_time = datetime.now().strftime('%Y%m')\narchive_dir = ARCHIVE_DIR / archive_time\narchive_dir.mkdir(parents=True, exist_ok=True)\n\n# 특정 심볼 또는 모든 심볼에 대해 아카이브\nfiles_to_archive = []\nif symbol:\nfile_path = LIVE_DIR / f\"{symbol}.csv\"\nif file_path.exists():\nfiles_to_archive.append(file_path)\nelse:\nfiles_to_archive.extend(LIVE_DIR.glob('*.csv'))\n\nfor file in files_to_archive:\ntry:\n# 데이터 로드\ndf = pd.read_csv(file)\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\n# 최근 데이터만 라이브에 유지 (7일)\ncutoff_date = datetime.now() - timedelta(days=7)\nmask = df['datetime'] < cutoff_date\n\nif mask.any():\n# 오래된 데이터는 아카이브로 이동\nold_data = df[mask].copy()\nnew_data = df[~mask].copy()\n\n# 아카이브 파일에 추가\narchive_file = archive_dir / file.name\nif archive_file.exists():\nexisting_data = pd.read_csv(archive_file)\nexisting_data['datetime'] = pd.to_datetime(existing_data['datetime'])\nold_data = pd.concat([existing_data, old_data])\n\n# 중복 제거 및 정렬\nold_data = old_data.drop_duplicates(subset=['datetime']).sort_values('datetime')\nold_data.to_csv(archive_file, index=False)\n\n# 최근 데이터만 라이브 파일에 저장\nif len(new_data) > 0:\nnew_data.to_csv(file, index=False)\nelse:\nfile.unlink() # 모든 데이터가 아카이브되면 라이브 파일 삭제\n\nlogger.info(f\"데이터 아카이브 완료 ({file.name}): {len(old_data)}행 아카이브, {len(new_data)}행 유지\")\n\nexcept Exception as e:\nlogger.error(f\"파일 아카이브 실패 ({file.name}): {str(e)}\")\ncontinue\n\nexcept Exception as e:\nlogger.error(f\"데이터 아카이브 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef compress_archive(self, archive_dir: Path = None):\n\"\"\"아카이브 압축\"\"\"\ntry:\nif archive_dir is None:\n# 가장 오래된 아카이브 디렉토리 찾기\narchive_dirs = sorted(ARCHIVE_DIR.glob('*'))\nif not archive_dirs:\nreturn\narchive_dir = archive_dirs[0]\n\nif not archive_dir.is_dir():\nreturn\n\n# 압축 파일 생성\narchive_name = archive_dir.name\nzip_file = LONG_TERM_DIR / f\"{archive_name}.zip\"\n\nwith zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\nfor file in archive_dir.glob('*.csv'):\nzf.write(file, file.name)\n\n# 압축 성공 시 원본 디렉토리 삭제\nshutil.rmtree(archive_dir)\nlogger.info(f\"아카이브 압축 완료: {zip_file}\")\n\nexcept Exception as e:\nlogger.error(f\"아카이브 압축 실패: {str(e)}\")\ntraceback.print_exc()\n\ndef run(self):\n\"\"\"데이터 수집 실행\"\"\"\nlogger.info(\"데이터 수집 시작\")\n\ntry:\nwhile not shutdown_flag.is_set():\ntry:\n# 데이터 수집\nself.collect_all_data()\n\n# 매일 자정에 백업 및 아카이브\nnow = datetime.now()\nif now.hour == 0 and now.minute < 5:\nself.backup_data()\nself.archive_data()\n\n# 3개월 이상 된 아카이브는 압축\nthree_months_ago = (now - timedelta(days=90)).strftime('%Y%m')\nold_archives = [d for d in ARCHIVE_DIR.glob('*') if d.name <= three_months_ago]\nfor archive_dir in old_archives:\nself.compress_archive(archive_dir)\n\n# 1분 대기\ntime.sleep(60)\n\nexcept Exception as e:\nlogger.error(f\"데이터 수집 중 오류 발생: {str(e)}\")\ntraceback.print_exc()\ntime.sleep(60) # 오류 발생 시 1분 대기\n\nexcept KeyboardInterrupt:\nlogger.info(\"사용자에 의한 종료\")\nexcept Exception as e:\nlogger.error(f\"예상치 못한 오류 발생: {str(e)}\")\ntraceback.print_exc()\nfinally:\nlogger.info(\"데이터 수집 종료\")\n\nif __name__ == \"__main__\":\ntry:\n# 종료 시그널 핸들러 등록\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\n\n# 데이터 매니저 실행\nmanager = DataManager()\nmanager.run()\n\nexcept Exception as e:\nlogger.error(f\"프로그램 실행 실패: {str(e)}\")\ntraceback.print_exc()\nfinally:\nlogger.info(\"프로그램 종료\")\n\n이걸 적용한거맞음?","commandType":4},{"text":"단계별로해 근데 아까 OI값 못가져오더라고 그거 해결한거 적용하고 해","commandType":4},{"text":"시작해","commandType":4},{"text":"다음","commandType":4},{"text":"알아서 계속 추가해","commandType":4},{"text":"진행해","commandType":4},{"text":"압축보관하면 분석할때 문제 생기는거아냐?","commandType":4},{"text":"응 구동해봐","commandType":4},{"text":"종료문제 심볼 문제 해결 해봐.","commandType":4},{"text":"문제점 파악하라","commandType":4},{"text":"해결 하고싶어 차근차근 구조 도와줘","commandType":4},{"text":"읽고 지금 에러가 생기는 부분이 어딘지 체크해라","commandType":4},{"text":"data로 수정","commandType":4},{"text":"제공하신 코드와 에러 로그를 분석한 결과, 주요 문제는 타임스탬프 범위 오류 발생으로 보입니다. \"Out of bounds nanosecond timestamp: 57166-12-19\" 형태의 에러가 발생하고 있습니다.\n\n## 주요 문제점:\n\n1. **타임스탬프 범위 초과 오류**: \n   - API 응답에서 받은 타임스탬프가 pandas가 처리할 수 있는 범위(1678~2262년)를 초과하는 값을 포함하고 있습니다.\n   - 57166년은 명백히 잘못된 타임스탬프입니다.\n\n2. **FutureWarning 발생**:\n   - `datetime.dt.to_pydatetime()`의 동작이 향후 변경될 예정이라는 경고가 표시됩니다.\n\n## 해결 방안:\n\n1. **타임스탬프 범위 검증**:\n   ```python\n   def sanitize_timestamps(df, column='datetime'):\n       \"\"\"범위를 벗어나는 타임스탬프 제거\"\"\"\n       if column in df.columns:\n           min_timestamp = pd.Timestamp.min.timestamp()\n           max_timestamp = pd.Timestamp.max.timestamp()\n           \n           # Unix timestamp로 변환하여 범위 검사\n           if df[column].dtype != 'datetime64[ns]':\n               df[column] = pd.to_datetime(df[column], errors='coerce')\n           \n           # 범위 밖의 값은 NaT로 변환\n           valid_mask = df[column].notna()\n           timestamps = df.loc[valid_mask, column].view('int64') // 10**9  # nanoseconds to seconds\n           valid_range = (timestamps >= min_timestamp) & (timestamps <= max_timestamp)\n           \n           # 범위 밖의 값을 NaT로 설정\n           invalid_indices = df.loc[valid_mask].index[~valid_range]\n           if len(invalid_indices) > 0:\n               df.loc[invalid_indices, column] = pd.NaT\n               logger.warning(f\"{len(invalid_indices)}개의 유효하지 않은 타임스탬프 제거됨\")\n           \n           # NaT 행 제거\n           df = df.dropna(subset=[column])\n       \n       return df\n   ```\n\n2. **API 응답 처리 개선**:\n   `fetch_oi` 및 `fetch_funding_rate` 함수에서 타임스탬프 처리 부분을 수정:\n\n   ```python\n   def fetch_oi(self, params: Dict) -> pd.DataFrame:\n       # 기존 코드...\n       \n       try:\n           df = pd.DataFrame(data)\n           if 'update' in df.columns and 'value' in df.columns:\n               # 타임스탬프 유효성 검사\n               df['update'] = pd.to_numeric(df['update'], errors='coerce')\n               # 명백히 잘못된 타임스탬프 제외 (현재 기준 +/- 100년)\n               current_ts = time.time()\n               mask = (df['update'] > current_ts - 3650*24*60*60) & (df['update'] < current_ts + 3650*24*60*60)\n               df = df[mask]\n               \n               # 변환\n               df['datetime'] = pd.to_datetime(df['update'], unit='s', errors='coerce')\n               df['oi'] = pd.to_numeric(df['value'], errors='coerce')\n           # 이하 동일...\n   ```\n\n3. **FutureWarning 해결**:\n   `save_data` 메서드에서 타임스탬프 비교 부분을 수정:\n\n   ```python\n   # 수정 전\n   existing_timestamps = set(existing_df['datetime'].dt.to_pydatetime())\n   mask = [dt not in existing_timestamps for dt in df['datetime'].dt.to_pydatetime()]\n   \n   # 수정 후\n   existing_timestamps = set(pd.Series(existing_df['datetime']).astype(str))\n   mask = [str(dt) not in existing_timestamps for dt in df['datetime']]\n   ```\n\n4. **CCXT 데이터와 Coinalyze 데이터 통합 관련 오류**:\n   두 데이터 소스에서 얻은 타임스탬프의 형식이 다를 수 있습니다. 병합 전에 통일된 형식으로 변환하세요.\n\n이 해결책들을 적용하면 타임스탬프 관련 오류를 방지하고 데이터를 안정적으로 처리할 수 있을 것입니다. 특히 `sanitize_timestamps` 함수를 모든 데이터프레임 처리 과정에 추가하여 잘못된 타임스탬프를 일관되게 처리하는 것이 중요합니다.\n\n내용 체크 해볼래?","commandType":4},{"text":"내용 확인해볼래?? ","commandType":4},{"text":"에러부분 먼저체[크하자","commandType":4},{"text":"1. 과거데이터를 가져와서. 신규데이터를 넣는 방식이잖아?\n그러니까 히스토리랑 둘다 있어야겠지 않겠니?","commandType":4},{"text":"에러생기는데???","commandType":4},{"text":"문제점을 파악해봐 현실적으로 가자?","commandType":4},{"text":"과거 데이터를 가져올 수 있는 심볼수를 먼저 체크 할 방법있지 않을까?","commandType":4},{"text":"타임프레임 에러 안생기게 해서 적용해봐","commandType":4},{"text":"timestamp가 왜 57167로 잡히는지 설명하라.","commandType":4},{"text":"oi값이랑 펀딩비는? nmr.csv.gz 파일 내용 문제가 좀 있지 않을까?","commandType":4},{"text":"종료 안되는 내용 문제 그리고 파일 열어볼수있는 방식을 모르겠네?","commandType":4},{"text":"csv.gz가 문제가 있다면 해결해야할것같은데?","commandType":4},{"text":"종료도 바로안되는것도 해결했어?","commandType":4},{"text":"그냥 csv로 저장하자 그리고 api 16개로 돌려써","commandType":4},{"text":"실행해보자","commandType":4},{"text":"@AGLD.csv 데이터를 가져왔는데 과거 전체 데이터를 못가져오는데 방법은 없어?","commandType":4},{"text":"내 생각은 api키가 1~16개가 있으니까 1~16번 과거데이터 초기부터 가져오는거야. 한바퀴 돌면 다시 1~16 16000개 데이터를 가져오는 방식으로 모든 심볼을 하고싶어 너의 생각은 어떻게 생각해?\n\n모든 과거데이터를 다가져오면 한번 더 돌려서 최신화 시킨다음에 실시간 데이터랑 타임프레임중에 빠진데이터만 넣는거야. 가능해?","commandType":4},{"text":"에러발생","commandType":4},{"text":"에러","commandType":4},{"text":"에러발생","commandType":4},{"text":"갑자기 안되는 이유는?","commandType":4},{"text":"체크해봐","commandType":4},{"text":"이거 코드 다돌면 실시간 데이터 가져오는거야??(질문)","commandType":4},{"text":"\n\n이거 무한루프 빠진것같은데??","commandType":4},{"text":"있는 내용은 빼고 가져와야할것같은데? 계속 덮어쓰기 하는것은 비효율적이야..","commandType":4},{"text":"먼저 검사해야하는게 타임프레임상 빠진게 있는지 체크해야한다.\n그뒤에 빠진부분을 채워야한다.\n다 채운이후는 1분마다 실시간 데이터를 가져오는 로직구조이다.\n\napi 키 16개를 순차적으로 돌릴 생각이니. 그런 개념으로 구조를 변경가능한지 확인만 먼저하자.","commandType":4},{"text":"다음","commandType":4},{"text":"실행해보자","commandType":4},{"text":"보니까 1개 코인을 1분씩 돌리는데 그러지 않고 모든코인을 순차적으로 돌리신거","commandType":4},{"text":"응 구동시켜봐","commandType":4},{"text":"누락구간 찾고 저장하고 누락구간찾고 저장하고 이런걸 api16개 키를 돌아가면서 진행하자는거야. 현재 상황은 너무 비효율적이야. ","commandType":4},{"text":"\n이거 exe화 시키는게 좋겠다","commandType":4},{"text":"exe파일을 실행시키면 어디에 데이터를 저장하는가??","commandType":4},{"text":"야 이거 코드 또 사라졌다?","commandType":4},{"text":"@crypto_data_manager_v3.py 이거보다 데이터가 더있었는데??","commandType":4},{"text":"1분마다 csv파일 다가져오는거 맞아?","commandType":4},{"text":"이걸로 ohlcv 랑 OI 펀딩비 를 1분마다 검사하고 가져올 수 있는거야??","commandType":4},{"text":"무슨변경사항이냐 원래 만들던거잖아\n\ndata/data 안에 있는 csv 파일 보고 저걸 실시간으로 가져오려면 어떻게 코드짜야하는지 생각하라","commandType":4},{"text":"아니면 exe 파일 py 파일로 변환하던지. 근데 왜 자꾸 너랑 코딩짜다가 저장된 py파일이 삭제되는경우가 발생하는지 설명해봐","commandType":4},{"text":"임시저장하지말고 그냥 저장하면안되?","commandType":4},{"text":"아니 그거 커서에서 설정변경으로 안되냐?","commandType":4},{"text":"exe 파일을 py 파일로 가져오는법","commandType":4},{"text":"그럼 임시저장파일로 커서에서 날라간 내용은 어떻게 복구할래??","commandType":4},{"text":"어디폴더인데 이건","commandType":4},{"text":"??","commandType":4}]